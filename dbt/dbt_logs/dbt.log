

============================== 2022-07-27 21:07:37.125502 | c753ce90-4226-408f-8e2c-36a54a109654 ==============================
[0m21:07:37.125612 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:07:37.133440 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:07:37.134558 [debug] [MainThread]: Tracking: tracking
[0m21:07:37.321875 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b694f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b695e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b695b0>]}
[0m21:07:37.460974 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m21:07:37.462984 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b7e760>]}
[0m21:07:37.527585 [debug] [MainThread]: Parsing macros/relations.sql
[0m21:07:37.543261 [debug] [MainThread]: Parsing macros/adapters.sql
[0m21:07:37.695602 [debug] [MainThread]: Parsing macros/catalog.sql
[0m21:07:37.704743 [debug] [MainThread]: Parsing macros/materializations/snapshot_merge.sql
[0m21:07:37.712354 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:07:37.714921 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:07:37.720765 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:07:37.723980 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:07:37.729198 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:07:37.755901 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:07:37.762606 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m21:07:37.773383 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m21:07:37.791589 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m21:07:37.799439 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m21:07:37.853495 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m21:07:38.016788 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m21:07:38.118960 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m21:07:38.207479 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m21:07:38.381032 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m21:07:38.459703 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m21:07:38.503415 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m21:07:38.643316 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m21:07:38.747925 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m21:07:38.754874 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m21:07:38.769674 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m21:07:38.815611 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m21:07:38.831205 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m21:07:38.841413 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m21:07:38.905499 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m21:07:38.950159 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m21:07:38.982008 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m21:07:39.065959 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m21:07:39.109511 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m21:07:39.118655 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m21:07:39.134203 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m21:07:39.155926 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m21:07:39.179953 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m21:07:39.302678 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m21:07:39.370175 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m21:07:39.430497 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m21:07:39.473669 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m21:07:39.598146 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m21:07:39.622897 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m21:07:39.635905 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m21:07:39.657251 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m21:07:39.665171 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m21:07:39.676900 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m21:07:39.688048 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m21:07:39.695171 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m21:07:39.712345 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m21:07:39.726774 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m21:07:39.745963 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m21:07:39.781097 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m21:07:39.785554 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m21:07:39.811219 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m21:07:39.817874 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m21:07:39.832006 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m21:07:39.846837 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m21:07:39.937438 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m21:07:39.953695 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m21:07:39.973536 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m21:07:39.989986 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m21:07:40.020142 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m21:07:40.047142 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m21:07:40.065701 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m21:07:40.074993 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m21:07:40.140629 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m21:07:40.249404 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m21:07:40.269520 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m21:07:40.298706 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m21:07:40.316054 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m21:07:42.789841 [debug] [MainThread]: 1699: static parser successfully parsed example/my_first_dbt_model.sql
[0m21:07:42.945704 [debug] [MainThread]: 1699: static parser successfully parsed example/my_second_dbt_model.sql
[0m21:07:42.972925 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:07:43.001028 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:07:43.017967 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:07:43.488917 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a9eee0>]}
[0m21:07:43.516397 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6619a30>]}
[0m21:07:43.517794 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:07:43.519899 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a26fd0>]}
[0m21:07:43.526597 [info ] [MainThread]: 
[0m21:07:43.529093 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:07:43.534439 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:07:43.596812 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:07:43.597741 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:07:43.598476 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:07:43.620927 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:07:43.626854 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:07:43.649010 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:07:43.711653 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:07:43.712799 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:07:43.713697 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:07:43.726782 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:07:43.729289 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:07:43.730189 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:07:43.757106 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.03 seconds
[0m21:07:43.765747 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:07:43.772362 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:07:43.790453 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.791379 [debug] [MainThread]: On master: BEGIN
[0m21:07:43.797666 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:07:43.813454 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:07:43.815368 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.816748 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:07:43.853589 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:07:43.867461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6a9ed00>]}
[0m21:07:43.872833 [debug] [MainThread]: On master: ROLLBACK
[0m21:07:43.874456 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.875381 [debug] [MainThread]: On master: BEGIN
[0m21:07:43.884905 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:07:43.886641 [debug] [MainThread]: On master: COMMIT
[0m21:07:43.892079 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:43.893849 [debug] [MainThread]: On master: COMMIT
[0m21:07:43.899499 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:07:43.906314 [debug] [MainThread]: On master: Close
[0m21:07:43.917659 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:07:43.919299 [info ] [MainThread]: 
[0m21:07:43.968515 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:07:43.969906 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:07:43.974976 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:07:43.975920 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:07:43.976853 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:07:43.989039 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:07:43.990884 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:43.991899 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:07:44.360302 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:07:44.366988 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:07:44.367928 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:07:44.368834 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:44.383186 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:07:44.384649 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:07:44.385768 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:07:44.408225 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."warehouse"."source"
                          ^

[0m21:07:44.409251 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:07:44.411395 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:44.412458 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:07:44.414413 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:07:44.415711 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ce910>]}
[0m21:07:44.423606 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.44s]
[0m21:07:44.434923 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:07:44.436070 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:07:44.444036 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:07:44.450951 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.451891 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:07:44.461019 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:07:44.486059 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:07:44.487577 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:44.497355 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:07:44.543609 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:07:44.561434 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.566882 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:07:44.567656 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:44.587775 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:07:44.588842 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.589588 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:07:44.595609 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
[0m21:07:44.679904 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.692861 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:07:44.695142 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:07:44.762432 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:44.763329 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:07:44.781549 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.02 seconds
[0m21:07:45.211528 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:07:45.218537 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:45.228422 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:07:45.245909 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:07:45.280322 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:07:45.281410 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:07:45.302072 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:07:45.305997 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.307053 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:07:45.320343 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff65928b0>]}
[0m21:07:45.321984 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.87s]
[0m21:07:45.336723 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:07:45.337996 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:07:45.345320 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:07:45.353047 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:07:45.358689 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:07:45.372331 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:07:45.375525 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:07:45.380528 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:07:45.385990 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:07:45.390165 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.391161 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:07:45.397286 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:07:45.438242 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:07:45.445511 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.446640 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:07:45.671627 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:07:45.677851 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.678848 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:07:45.681322 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:07:45.696420 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:07:45.697384 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.698175 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:07:45.706028 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.0 seconds
[0m21:07:45.718274 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.719178 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:07:45.731888 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:07:45.755824 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:07:45.756807 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.757658 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:07:45.778728 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:07:45.799449 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:07:45.800625 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:07:45.809268 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:07:45.818271 [debug] [Thread-1  ]: finished collecting timing info
[0m21:07:45.824318 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:07:45.829411 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c753ce90-4226-408f-8e2c-36a54a109654', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ced60>]}
[0m21:07:45.836666 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.44s]
[0m21:07:45.841517 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:07:45.863284 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:07:45.864955 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:45.866194 [debug] [MainThread]: On master: BEGIN
[0m21:07:45.867069 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:07:45.882766 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:07:45.884097 [debug] [MainThread]: On master: COMMIT
[0m21:07:45.888119 [debug] [MainThread]: Using postgres connection "master"
[0m21:07:45.892204 [debug] [MainThread]: On master: COMMIT
[0m21:07:45.901053 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:07:45.908327 [debug] [MainThread]: On master: Close
[0m21:07:45.916110 [info ] [MainThread]: 
[0m21:07:45.917876 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.39 seconds (2.39s).
[0m21:07:45.924605 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:07:45.931959 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:07:45.932887 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:07:46.022111 [info ] [MainThread]: 
[0m21:07:46.026239 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:07:46.053084 [info ] [MainThread]: 
[0m21:07:46.059244 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:07:46.069292 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:07:46.078576 [error] [MainThread]:   LINE 8:     select * from "warehouse"."warehouse"."source"
[0m21:07:46.086814 [error] [MainThread]:                             ^
[0m21:07:46.090253 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:07:46.102003 [info ] [MainThread]: 
[0m21:07:46.145309 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:07:46.162754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff658bf40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff6b7e6d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ff44ced60>]}


============================== 2022-07-27 21:12:17.533748 | c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c ==============================
[0m21:12:17.533882 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:12:17.552806 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:12:17.560519 [debug] [MainThread]: Tracking: tracking
[0m21:12:17.801662 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb0520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb0610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016cb05e0>]}
[0m21:12:18.011181 [debug] [MainThread]: Partial parsing enabled: 1 files deleted, 1 files added, 0 files changed.
[0m21:12:18.013120 [debug] [MainThread]: Partial parsing: added file: dbt_://models/traffic_models/schema.yml
[0m21:12:18.024887 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:12:18.095307 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:12:18.157689 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:12:18.198725 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:12:18.334661 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016722eb0>]}
[0m21:12:18.366246 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016c29b50>]}
[0m21:12:18.367794 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:12:18.370384 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b9a100>]}
[0m21:12:18.378231 [info ] [MainThread]: 
[0m21:12:18.382145 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:12:18.389848 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:12:18.445581 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:12:18.446700 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:12:18.448366 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:12:18.474407 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
[0m21:12:18.479491 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:12:18.503716 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:12:18.609026 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:12:18.616062 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:12:18.628125 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:12:18.670636 [debug] [ThreadPool]: SQL status: BEGIN in 0.04 seconds
[0m21:12:18.671591 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:12:18.672660 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:12:18.683510 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:12:18.706182 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:12:18.713017 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:12:18.743676 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.750703 [debug] [MainThread]: On master: BEGIN
[0m21:12:18.751450 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:12:18.782191 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
[0m21:12:18.785241 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.789834 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:12:18.835736 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:12:18.840005 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b67e80>]}
[0m21:12:18.841331 [debug] [MainThread]: On master: ROLLBACK
[0m21:12:18.850754 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.851702 [debug] [MainThread]: On master: BEGIN
[0m21:12:18.867628 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:12:18.869156 [debug] [MainThread]: On master: COMMIT
[0m21:12:18.874725 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:18.883465 [debug] [MainThread]: On master: COMMIT
[0m21:12:18.891721 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
[0m21:12:18.892796 [debug] [MainThread]: On master: Close
[0m21:12:18.894788 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:12:18.898149 [info ] [MainThread]: 
[0m21:12:18.950762 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:12:18.952075 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:12:18.955627 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:12:18.956668 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:12:18.957540 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:12:18.985039 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:12:18.990134 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:18.992315 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:12:19.273980 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:12:19.275996 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:12:19.277196 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:12:19.278081 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:19.295095 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:19.296055 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:12:19.297103 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:12:19.300401 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."warehouse"."source"
                          ^

[0m21:12:19.302023 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:12:19.309457 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.310959 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:12:19.317565 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:12:19.320007 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff014634490>]}
[0m21:12:19.324315 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.37s]
[0m21:12:19.329999 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:12:19.334064 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:12:19.337859 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:12:19.346507 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.350385 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:12:19.351629 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:12:19.387827 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:12:19.397331 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.398294 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:12:19.422842 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:12:19.425687 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.426606 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:12:19.427310 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:19.445859 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:19.449003 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.451479 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:12:19.458600 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:12:19.515315 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.516474 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:12:19.519163 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:19.545612 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.550819 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:12:19.554039 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:19.692905 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:12:19.693847 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.694570 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:12:19.722038 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:12:19.785405 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:12:19.786338 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:12:19.811969 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:12:19.815776 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.816862 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:12:19.818934 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0166c47f0>]}
[0m21:12:19.820532 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.47s]
[0m21:12:19.831942 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:12:19.835264 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:12:19.839605 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:12:19.842758 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:12:19.844260 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:12:19.845574 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:12:19.847975 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:12:19.849860 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:12:19.851389 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:12:19.854732 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:19.856040 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:12:19.858128 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:12:19.867247 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:12:19.869269 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:19.870438 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:12:20.142009 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:12:20.143641 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.144584 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:12:20.152237 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:12:20.171412 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:12:20.174068 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.177398 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:12:20.185599 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:12:20.195739 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.196759 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:12:20.199554 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:12:20.206233 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:12:20.207408 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.208190 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:12:20.243881 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:12:20.251177 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:12:20.252449 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:12:20.255517 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:12:20.264495 [debug] [Thread-1  ]: finished collecting timing info
[0m21:12:20.265488 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:12:20.275699 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c1d32072-0ca2-44c9-8e1a-4ef7bbd1fa8c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01565c490>]}
[0m21:12:20.277235 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.42s]
[0m21:12:20.285613 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:12:20.290628 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:12:20.292598 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:20.293585 [debug] [MainThread]: On master: BEGIN
[0m21:12:20.294376 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:12:20.353270 [debug] [MainThread]: SQL status: BEGIN in 0.06 seconds
[0m21:12:20.354829 [debug] [MainThread]: On master: COMMIT
[0m21:12:20.358535 [debug] [MainThread]: Using postgres connection "master"
[0m21:12:20.359580 [debug] [MainThread]: On master: COMMIT
[0m21:12:20.361368 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:12:20.362347 [debug] [MainThread]: On master: Close
[0m21:12:20.371104 [info ] [MainThread]: 
[0m21:12:20.374203 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.99 seconds (1.99s).
[0m21:12:20.377134 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:12:20.378082 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:12:20.382686 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:12:20.413589 [info ] [MainThread]: 
[0m21:12:20.416482 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:12:20.419449 [info ] [MainThread]: 
[0m21:12:20.422847 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:12:20.426274 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:12:20.429625 [error] [MainThread]:   LINE 8:     select * from "warehouse"."warehouse"."source"
[0m21:12:20.432445 [error] [MainThread]:                             ^
[0m21:12:20.434275 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:12:20.438661 [info ] [MainThread]: 
[0m21:12:20.442458 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:12:20.445594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff016b79dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01458cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff01458c610>]}


============================== 2022-07-27 21:13:01.384874 | 660e18c5-7fc5-4858-8f8b-9220ee6122f7 ==============================
[0m21:13:01.385003 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:13:01.393056 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:13:01.396114 [debug] [MainThread]: Tracking: tracking
[0m21:13:01.624382 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c684c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c685b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09c68580>]}
[0m21:13:02.015334 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:13:02.023880 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:13:02.031148 [debug] [MainThread]: Partial parsing: update schema file: dbt_://models/traffic_models/schema.yml
[0m21:13:02.172837 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:13:02.281852 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:13:02.294446 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:13:02.445020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c096dbd60>]}
[0m21:13:02.505326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c096a87f0>]}
[0m21:13:02.506716 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:13:02.516754 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09b4cf70>]}
[0m21:13:02.541888 [info ] [MainThread]: 
[0m21:13:02.548473 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:13:02.585475 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:13:02.752009 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:13:02.754800 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:13:02.757765 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:02.792066 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.03 seconds
[0m21:13:02.798437 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:13:02.810291 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:13:02.869363 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:13:02.870266 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:13:02.872469 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:13:02.896735 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:13:02.897687 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:13:02.900290 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:13:02.910053 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:13:02.914147 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:13:02.916526 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:13:02.943611 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:02.948037 [debug] [MainThread]: On master: BEGIN
[0m21:13:02.950610 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:13:02.972921 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:13:02.973868 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:02.974605 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:13:03.025240 [debug] [MainThread]: SQL status: SELECT 1 in 0.05 seconds
[0m21:13:03.030443 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09aec9a0>]}
[0m21:13:03.031737 [debug] [MainThread]: On master: ROLLBACK
[0m21:13:03.037414 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:03.039261 [debug] [MainThread]: On master: BEGIN
[0m21:13:03.050257 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:13:03.053128 [debug] [MainThread]: On master: COMMIT
[0m21:13:03.054002 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:03.054707 [debug] [MainThread]: On master: COMMIT
[0m21:13:03.063499 [debug] [MainThread]: SQL status: COMMIT in 0.01 seconds
[0m21:13:03.065957 [debug] [MainThread]: On master: Close
[0m21:13:03.075030 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:13:03.096822 [info ] [MainThread]: 
[0m21:13:03.130486 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:13:03.131780 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:13:03.137167 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:13:03.138371 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:13:03.139254 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:13:03.151564 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:13:03.153201 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.154799 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:13:03.514738 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:13:03.519432 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:13:03.523092 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:13:03.524120 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:03.542654 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:13:03.545078 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:13:03.546280 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:13:03.549749 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:13:03.557473 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:13:03.562969 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.564125 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:13:03.566021 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:13:03.578873 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c085e2e20>]}
[0m21:13:03.585107 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.44s]
[0m21:13:03.596421 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:13:03.601889 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:13:03.605585 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:13:03.613610 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.624350 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:13:03.626319 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:13:03.651626 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:13:03.658885 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:03.661739 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:13:03.703769 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:13:03.714299 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.717816 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:13:03.718937 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:03.751296 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:13:03.753677 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.755123 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:13:03.761567 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:13:03.817946 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.824443 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:13:03.826729 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:03.851813 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:03.855471 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:13:03.861909 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:04.242211 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:13:04.243146 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:04.264584 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:13:04.283383 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:13:04.335963 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:13:04.337606 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:13:04.361378 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:13:04.365309 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.366301 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:13:04.373799 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c085e2700>]}
[0m21:13:04.375331 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.76s]
[0m21:13:04.397113 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:13:04.400971 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:13:04.402912 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:13:04.417865 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:13:04.419340 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:13:04.422628 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:13:04.462873 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:13:04.464875 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:13:04.467561 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:13:04.476379 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.478651 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:13:04.480003 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:13:04.490141 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:13:04.491966 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.493972 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:13:04.661896 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:13:04.671454 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.672965 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:13:04.674679 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:13:04.698317 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:13:04.702685 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.709071 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:13:04.717738 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:13:04.729245 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.730834 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:13:04.733323 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:13:04.740991 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:13:04.742072 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.745391 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:13:04.760404 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:13:04.775682 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:13:04.788632 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:13:04.791030 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:13:04.796056 [debug] [Thread-1  ]: finished collecting timing info
[0m21:13:04.797328 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:13:04.808546 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '660e18c5-7fc5-4858-8f8b-9220ee6122f7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c086153d0>]}
[0m21:13:04.813636 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.33s]
[0m21:13:04.823188 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:13:04.829338 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:13:04.830881 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:04.832424 [debug] [MainThread]: On master: BEGIN
[0m21:13:04.834033 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:13:04.878083 [debug] [MainThread]: SQL status: BEGIN in 0.04 seconds
[0m21:13:04.880643 [debug] [MainThread]: On master: COMMIT
[0m21:13:04.881419 [debug] [MainThread]: Using postgres connection "master"
[0m21:13:04.882110 [debug] [MainThread]: On master: COMMIT
[0m21:13:04.884557 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:13:04.885928 [debug] [MainThread]: On master: Close
[0m21:13:04.893426 [info ] [MainThread]: 
[0m21:13:04.903784 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.35 seconds (2.35s).
[0m21:13:04.909047 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:13:04.909864 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:13:04.912811 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:13:05.008576 [info ] [MainThread]: 
[0m21:13:05.010688 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:13:05.020793 [info ] [MainThread]: 
[0m21:13:05.023205 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:13:05.027019 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:13:05.032420 [error] [MainThread]:   LINE 8:     select * from "warehouse"."traffic_source"."source"
[0m21:13:05.043423 [error] [MainThread]:                             ^
[0m21:13:05.045035 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:13:05.057606 [info ] [MainThread]: 
[0m21:13:05.066359 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:13:05.074371 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c09b30d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c08546d60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1c08546a90>]}


============================== 2022-07-27 21:17:00.008397 | 3a2f7873-0cba-4436-946e-fc7cedcbf49b ==============================
[0m21:17:00.008536 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:17:00.018206 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:17:00.024426 [debug] [MainThread]: Tracking: tracking
[0m21:17:00.247309 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d5b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d6a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ee4d670>]}
[0m21:17:00.587397 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:17:00.596495 [debug] [MainThread]: Partial parsing: deleted source source.dbt_.traffic_source.source
[0m21:17:00.599331 [debug] [MainThread]: Partial parsing: update schema file: dbt_://models/traffic_models/schema.yml
[0m21:17:00.755666 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:17:00.815128 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:17:00.828140 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:17:00.937691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e8bf070>]}
[0m21:17:00.967682 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e88d850>]}
[0m21:17:00.969426 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:17:00.971219 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ed360a0>]}
[0m21:17:00.978013 [info ] [MainThread]: 
[0m21:17:00.980691 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:17:00.985404 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:17:01.026280 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:17:01.032413 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:17:01.033537 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:17:01.049066 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:17:01.055088 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:17:01.086831 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:17:01.185282 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:17:01.194821 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:17:01.196286 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:17:01.225283 [debug] [ThreadPool]: SQL status: BEGIN in 0.03 seconds
[0m21:17:01.228869 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:17:01.237260 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:17:01.249532 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:17:01.254517 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:17:01.261359 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:17:01.312018 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.321087 [debug] [MainThread]: On master: BEGIN
[0m21:17:01.321982 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:17:01.334752 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:17:01.336263 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.337186 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:17:01.364446 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:17:01.368651 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e8690a0>]}
[0m21:17:01.370146 [debug] [MainThread]: On master: ROLLBACK
[0m21:17:01.372203 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.373271 [debug] [MainThread]: On master: BEGIN
[0m21:17:01.376546 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:17:01.377761 [debug] [MainThread]: On master: COMMIT
[0m21:17:01.378705 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:01.380232 [debug] [MainThread]: On master: COMMIT
[0m21:17:01.381810 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:17:01.382937 [debug] [MainThread]: On master: Close
[0m21:17:01.390276 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:17:01.392066 [info ] [MainThread]: 
[0m21:17:01.403220 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:17:01.404657 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:17:01.406865 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:17:01.407794 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:17:01.409066 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:17:01.419667 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:17:01.421585 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.422766 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:17:01.671993 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:17:01.685161 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:17:01.686920 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:17:01.687994 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:01.717183 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:17:01.718177 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:17:01.718914 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:17:01.722669 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:17:01.732861 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:17:01.736088 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.741005 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:17:01.752682 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:17:01.754272 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c7cea60>]}
[0m21:17:01.755990 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.35s]
[0m21:17:01.761698 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:17:01.766361 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:17:01.768418 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:17:01.781411 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.790345 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:17:01.792379 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:17:01.817851 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:17:01.826657 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:01.829229 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:17:01.906227 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:17:01.913875 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.916372 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:17:01.920631 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:01.935603 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:17:01.941941 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:01.943064 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:17:01.948986 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.0 seconds
[0m21:17:01.999530 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.005161 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:17:02.009045 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.025972 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.029899 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:17:02.033705 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.223148 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:17:02.227761 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.230009 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:17:02.242347 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:17:02.310678 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:17:02.318105 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:17:02.354086 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.03 seconds
[0m21:17:02.358659 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.368395 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:17:02.387436 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05e847eb0>]}
[0m21:17:02.389277 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.61s]
[0m21:17:02.391936 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:17:02.396518 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:17:02.398156 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:17:02.411660 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:17:02.413293 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:17:02.414595 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:17:02.416821 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:17:02.420248 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:17:02.424049 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:17:02.429488 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.435687 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:17:02.439640 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:17:02.459166 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:17:02.462998 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.478556 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:17:02.705778 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:17:02.712326 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.714319 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:17:02.716292 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:17:02.731402 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:17:02.733570 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.734934 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:17:02.743291 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:17:02.759750 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.761245 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:17:02.763445 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:17:02.770916 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:17:02.771995 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.772937 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:17:02.808832 [debug] [Thread-1  ]: SQL status: COMMIT in 0.04 seconds
[0m21:17:02.818637 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:17:02.819724 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:17:02.823075 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:17:02.832060 [debug] [Thread-1  ]: finished collecting timing info
[0m21:17:02.836100 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:17:02.848975 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '3a2f7873-0cba-4436-946e-fc7cedcbf49b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05d7fa520>]}
[0m21:17:02.850664 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.42s]
[0m21:17:02.853819 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:17:02.867109 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:17:02.868352 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:02.869331 [debug] [MainThread]: On master: BEGIN
[0m21:17:02.870174 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:17:02.881520 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:17:02.883508 [debug] [MainThread]: On master: COMMIT
[0m21:17:02.884720 [debug] [MainThread]: Using postgres connection "master"
[0m21:17:02.885612 [debug] [MainThread]: On master: COMMIT
[0m21:17:02.887128 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:17:02.888274 [debug] [MainThread]: On master: Close
[0m21:17:02.897980 [info ] [MainThread]: 
[0m21:17:02.899759 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.92 seconds (1.92s).
[0m21:17:02.903333 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:17:02.906338 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:17:02.908451 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:17:02.967193 [info ] [MainThread]: 
[0m21:17:02.973246 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:17:02.975025 [info ] [MainThread]: 
[0m21:17:02.976606 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:17:02.980883 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:17:02.984560 [error] [MainThread]:   LINE 8:     select * from "warehouse"."traffic_source"."source"
[0m21:17:02.993170 [error] [MainThread]:                             ^
[0m21:17:02.994718 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:17:02.996241 [info ] [MainThread]: 
[0m21:17:02.997768 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:17:02.999515 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05ed15e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c72e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05c72e9a0>]}


============================== 2022-07-27 21:18:08.486201 | a62dc8d8-f3fd-46a7-b2c1-e2759cd22814 ==============================
[0m21:18:08.486321 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:18:08.512519 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:18:08.514710 [debug] [MainThread]: Tracking: tracking
[0m21:18:08.749342 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa120520>]}
[0m21:18:09.147596 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:18:09.149689 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/dim_types.sql
[0m21:18:09.217613 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:18:09.357150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f9fc9130>]}
[0m21:18:09.385499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa09a8e0>]}
[0m21:18:09.387200 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:18:09.389032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f9fc8f10>]}
[0m21:18:09.404600 [info ] [MainThread]: 
[0m21:18:09.417965 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:18:09.429240 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:18:09.495026 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:18:09.496140 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:18:09.497091 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:09.515008 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:18:09.529788 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:18:09.546577 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:18:09.651523 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:18:09.652495 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:18:09.656777 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:18:09.671846 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:18:09.675374 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:18:09.676125 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:18:09.688185 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.01 seconds
[0m21:18:09.694025 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:18:09.695650 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:18:09.742603 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.745577 [debug] [MainThread]: On master: BEGIN
[0m21:18:09.749878 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:18:09.806291 [debug] [MainThread]: SQL status: BEGIN in 0.06 seconds
[0m21:18:09.807226 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.807925 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:18:09.841271 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:18:09.845202 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa137af0>]}
[0m21:18:09.847659 [debug] [MainThread]: On master: ROLLBACK
[0m21:18:09.849417 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.850315 [debug] [MainThread]: On master: BEGIN
[0m21:18:09.853225 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:18:09.854222 [debug] [MainThread]: On master: COMMIT
[0m21:18:09.855413 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:09.858131 [debug] [MainThread]: On master: COMMIT
[0m21:18:09.860522 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:18:09.862810 [debug] [MainThread]: On master: Close
[0m21:18:09.873631 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:18:09.875574 [info ] [MainThread]: 
[0m21:18:09.894875 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:18:09.896356 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:18:09.899471 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:18:09.900795 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:18:09.901732 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:18:09.912615 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:18:09.914309 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:09.917839 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:18:10.204590 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:18:10.208478 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:18:10.209403 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:18:10.210214 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.233057 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:10.233992 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:18:10.235485 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:18:10.242680 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from "warehouse"."source"
                          ^

[0m21:18:10.245295 [debug] [Thread-1  ]: On model.dbt_.dim_types: ROLLBACK
[0m21:18:10.248827 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.252752 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:18:10.262922 [debug] [Thread-1  ]: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from "warehouse"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:18:10.264358 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f82c9910>]}
[0m21:18:10.269481 [error] [Thread-1  ]: 1 of 5 ERROR creating table model warehouse.dim_types .......................... [[31mERROR[0m in 0.37s]
[0m21:18:10.271665 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:18:10.275410 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:18:10.277477 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:18:10.281843 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.284383 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:18:10.286592 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:18:10.305761 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:18:10.310999 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.313530 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:18:10.349161 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:18:10.360062 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.362592 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:18:10.364848 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:10.381635 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:10.386115 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.387674 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:18:10.396536 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:18:10.460143 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.469972 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:18:10.481691 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:10.511499 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.515155 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:18:10.521079 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:10.710601 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:18:10.711713 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.712730 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:18:10.726869 [debug] [Thread-1  ]: SQL status: COMMIT in 0.01 seconds
[0m21:18:10.780072 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:18:10.788904 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:18:10.816207 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:18:10.821116 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.822275 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:18:10.825881 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f8af61f0>]}
[0m21:18:10.827816 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.55s]
[0m21:18:10.840595 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:18:10.843781 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:18:10.845972 [info ] [Thread-1  ]: 3 of 5 SKIP relation warehouse.fct_summary ..................................... [[33mSKIP[0m]
[0m21:18:10.850578 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:18:10.869946 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:18:10.874893 [info ] [Thread-1  ]: 4 of 5 SKIP relation warehouse.fct_trajectory .................................. [[33mSKIP[0m]
[0m21:18:10.880340 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:18:10.881697 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:18:10.883841 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:18:10.890344 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:10.891514 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:18:10.892497 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:18:10.902929 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:18:10.904804 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:10.905989 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:18:11.098543 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:18:11.100590 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.102397 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:18:11.103346 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:18:11.125704 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:18:11.132043 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.133988 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:18:11.147332 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:18:11.166252 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.167165 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:18:11.172968 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:18:11.190304 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:18:11.202400 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.205429 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:18:11.269371 [debug] [Thread-1  ]: SQL status: COMMIT in 0.06 seconds
[0m21:18:11.277925 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:18:11.278833 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:18:11.280642 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.0 seconds
[0m21:18:11.288620 [debug] [Thread-1  ]: finished collecting timing info
[0m21:18:11.289616 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:18:11.292860 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a62dc8d8-f3fd-46a7-b2c1-e2759cd22814', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3f82a9d30>]}
[0m21:18:11.298114 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.40s]
[0m21:18:11.302472 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:18:11.309297 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:18:11.310320 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:11.311076 [debug] [MainThread]: On master: BEGIN
[0m21:18:11.311752 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:18:11.329131 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:18:11.330918 [debug] [MainThread]: On master: COMMIT
[0m21:18:11.333136 [debug] [MainThread]: Using postgres connection "master"
[0m21:18:11.333902 [debug] [MainThread]: On master: COMMIT
[0m21:18:11.338814 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:18:11.339777 [debug] [MainThread]: On master: Close
[0m21:18:11.364557 [info ] [MainThread]: 
[0m21:18:11.366600 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 1.95 seconds (1.95s).
[0m21:18:11.372358 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:18:11.373301 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:18:11.373992 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:18:11.423871 [info ] [MainThread]: 
[0m21:18:11.450902 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m21:18:11.456720 [info ] [MainThread]: 
[0m21:18:11.458678 [error] [MainThread]: [33mDatabase Error in model dim_types (models/traffic_models/dim_types.sql)[0m
[0m21:18:11.465415 [error] [MainThread]:   relation "warehouse.source" does not exist
[0m21:18:11.469878 [error] [MainThread]:   LINE 8:     select * from "warehouse"."source"
[0m21:18:11.471286 [error] [MainThread]:                             ^
[0m21:18:11.472596 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/dim_types.sql
[0m21:18:11.473928 [info ] [MainThread]: 
[0m21:18:11.486186 [info ] [MainThread]: Done. PASS=2 WARN=0 ERROR=1 SKIP=2 TOTAL=5
[0m21:18:11.487939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe3fa05a370>]}


============================== 2022-07-27 21:20:50.147062 | 9b8add4c-8101-4c32-b3f0-892f2e8a0370 ==============================
[0m21:20:50.147178 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:20:50.162022 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:20:50.163893 [debug] [MainThread]: Tracking: tracking
[0m21:20:50.325722 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf1520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf1610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8bf15e0>]}
[0m21:20:50.609331 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m21:20:50.611186 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/dim_types.sql
[0m21:20:50.738683 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/dim_types.sql
[0m21:20:50.913565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8ad01f0>]}
[0m21:20:50.942038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8baa9a0>]}
[0m21:20:50.943734 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:20:50.945860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8ad2fd0>]}
[0m21:20:50.965849 [info ] [MainThread]: 
[0m21:20:50.972908 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:20:50.987882 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:20:51.066193 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:20:51.067131 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:20:51.067847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:20:51.091229 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:20:51.096348 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:20:51.110289 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:20:51.159376 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:20:51.160673 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:20:51.161614 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:20:51.180022 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.181159 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:20:51.188955 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:20:51.206194 [debug] [ThreadPool]: SQL status: SELECT 2 in 0.02 seconds
[0m21:20:51.213291 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:20:51.224375 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:20:51.257069 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.263762 [debug] [MainThread]: On master: BEGIN
[0m21:20:51.269570 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:20:51.289049 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.293795 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.296839 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:20:51.331827 [debug] [MainThread]: SQL status: SELECT 1 in 0.03 seconds
[0m21:20:51.336105 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8c07760>]}
[0m21:20:51.337413 [debug] [MainThread]: On master: ROLLBACK
[0m21:20:51.339185 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.340094 [debug] [MainThread]: On master: BEGIN
[0m21:20:51.342636 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:20:51.349366 [debug] [MainThread]: On master: COMMIT
[0m21:20:51.350901 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:51.352527 [debug] [MainThread]: On master: COMMIT
[0m21:20:51.356094 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:20:51.357099 [debug] [MainThread]: On master: Close
[0m21:20:51.366724 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:20:51.371111 [info ] [MainThread]: 
[0m21:20:51.438489 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:20:51.440017 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:20:51.442450 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:20:51.443615 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:20:51.444504 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:20:51.458525 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:20:51.460317 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:51.472759 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:20:51.825281 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:20:51.828847 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:51.835753 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:20:51.836568 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:51.855978 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:51.857049 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:51.857958 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:20:52.016590 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.16 seconds
[0m21:20:52.063777 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.075730 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:20:52.081196 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:52.221370 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:20:52.222479 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.223365 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:20:52.248360 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:20:52.267250 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:20:52.268408 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:20:52.270404 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:20:52.274465 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.275630 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:20:52.284761 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de85d8190>]}
[0m21:20:52.286519 [info ] [Thread-1  ]: 1 of 5 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 0.84s]
[0m21:20:52.291066 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:20:52.300849 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:20:52.303364 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:20:52.311145 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.312854 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:20:52.316455 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:20:52.338155 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:20:52.339982 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.341188 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:20:52.383706 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:20:52.388693 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.389928 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:20:52.390863 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.411931 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.415319 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.418031 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:20:52.430307 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:20:52.454127 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.455008 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:20:52.467326 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:52.509337 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.510225 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:20:52.516850 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:20:52.570694 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:20:52.582401 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.583143 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:20:52.614473 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:20:52.622089 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:20:52.622972 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:20:52.659675 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.04 seconds
[0m21:20:52.663806 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.664853 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:20:52.666568 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b01340>]}
[0m21:20:52.672760 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.36s]
[0m21:20:52.677248 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:20:52.681870 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:20:52.685755 [info ] [Thread-1  ]: 3 of 5 START table model warehouse.fct_summary ................................. [RUN]
[0m21:20:52.707036 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:20:52.709415 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:20:52.710202 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:20:52.740048 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:20:52.741554 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.742455 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:20:52.770921 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:20:52.773574 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:20:52.774419 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:20:52.779632 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.794999 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.796520 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:20:52.798253 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:20:52.802315 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 9:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:20:52.803289 [debug] [Thread-1  ]: On model.dbt_.fct_summary: ROLLBACK
[0m21:20:52.810635 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.814113 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:20:52.822993 [debug] [Thread-1  ]: Database Error in model fct_summary (models/traffic_models/fct_summary.sql)
  relation "traffic_source.source" does not exist
  LINE 9:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/fct_summary.sql
[0m21:20:52.824283 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de856f100>]}
[0m21:20:52.829493 [error] [Thread-1  ]: 3 of 5 ERROR creating table model warehouse.fct_summary ........................ [[31mERROR[0m in 0.12s]
[0m21:20:52.834051 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:20:52.835108 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:20:52.857301 [info ] [Thread-1  ]: 4 of 5 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:20:52.865083 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.866400 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:20:52.868348 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:20:52.883940 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:20:52.885680 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:52.900068 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:20:52.955069 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:20:52.962369 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.967124 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:20:52.970222 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:52.987208 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:20:52.990606 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:20:52.992040 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:20:53.008227 [debug] [Thread-1  ]: Postgres adapter: Postgres error: relation "traffic_source.source" does not exist
LINE 9:     select * from "warehouse"."traffic_source"."source"
                          ^

[0m21:20:53.016013 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: ROLLBACK
[0m21:20:53.044368 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.048434 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:20:53.050568 [debug] [Thread-1  ]: Database Error in model fct_trajectory (models/traffic_models/fct_trajectory.sql)
  relation "traffic_source.source" does not exist
  LINE 9:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at target/run/dbt_/models/traffic_models/fct_trajectory.sql
[0m21:20:53.052022 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8513dc0>]}
[0m21:20:53.053696 [error] [Thread-1  ]: 4 of 5 ERROR creating table model warehouse.fct_trajectory ..................... [[31mERROR[0m in 0.19s]
[0m21:20:53.057669 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:20:53.058997 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:20:53.060634 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:20:53.066496 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.067589 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:20:53.069006 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:20:53.077976 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:20:53.087499 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.090713 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:20:53.257935 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:20:53.264810 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.266893 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:20:53.267785 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:20:53.303728 [debug] [Thread-1  ]: SQL status: BEGIN in 0.04 seconds
[0m21:20:53.304923 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.324356 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:20:53.332263 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:20:53.341664 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.342539 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:20:53.351910 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:20:53.365371 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:20:53.366281 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.366976 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:20:53.391250 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:20:53.415188 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:20:53.416068 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:20:53.425702 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.01 seconds
[0m21:20:53.431737 [debug] [Thread-1  ]: finished collecting timing info
[0m21:20:53.435042 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:20:53.445872 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b8add4c-8101-4c32-b3f0-892f2e8a0370', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8532d60>]}
[0m21:20:53.449490 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.38s]
[0m21:20:53.464942 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:20:53.470223 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:20:53.471867 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:53.473046 [debug] [MainThread]: On master: BEGIN
[0m21:20:53.473991 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:20:53.487916 [debug] [MainThread]: SQL status: BEGIN in 0.01 seconds
[0m21:20:53.492874 [debug] [MainThread]: On master: COMMIT
[0m21:20:53.499349 [debug] [MainThread]: Using postgres connection "master"
[0m21:20:53.504065 [debug] [MainThread]: On master: COMMIT
[0m21:20:53.513882 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:20:53.519569 [debug] [MainThread]: On master: Close
[0m21:20:53.527051 [info ] [MainThread]: 
[0m21:20:53.540948 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 2.56 seconds (2.56s).
[0m21:20:53.556415 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:20:53.561215 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:20:53.611577 [info ] [MainThread]: 
[0m21:20:53.621395 [info ] [MainThread]: [31mCompleted with 2 errors and 0 warnings:[0m
[0m21:20:53.622865 [info ] [MainThread]: 
[0m21:20:53.632274 [error] [MainThread]: [33mDatabase Error in model fct_summary (models/traffic_models/fct_summary.sql)[0m
[0m21:20:53.633838 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:20:53.635074 [error] [MainThread]:   LINE 9:     select * from "warehouse"."traffic_source"."source"
[0m21:20:53.636320 [error] [MainThread]:                             ^
[0m21:20:53.637533 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/fct_summary.sql
[0m21:20:53.638769 [info ] [MainThread]: 
[0m21:20:53.640019 [error] [MainThread]: [33mDatabase Error in model fct_trajectory (models/traffic_models/fct_trajectory.sql)[0m
[0m21:20:53.650616 [error] [MainThread]:   relation "traffic_source.source" does not exist
[0m21:20:53.651859 [error] [MainThread]:   LINE 9:     select * from "warehouse"."traffic_source"."source"
[0m21:20:53.653170 [error] [MainThread]:                             ^
[0m21:20:53.654377 [error] [MainThread]:   compiled SQL at target/run/dbt_/models/traffic_models/fct_trajectory.sql
[0m21:20:53.655657 [info ] [MainThread]: 
[0m21:20:53.657004 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=2 SKIP=0 TOTAL=5
[0m21:20:53.658528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1de8b6c430>]}


============================== 2022-07-27 21:23:24.914759 | a676c8f2-cadb-40cb-afcb-8120b89d9038 ==============================
[0m21:23:24.914901 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:23:24.940533 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:23:24.943326 [debug] [MainThread]: Tracking: tracking
[0m21:23:25.132732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39419b8640>]}
[0m21:23:25.604412 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 2 files changed.
[0m21:23:25.606269 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/fct_trajectory.sql
[0m21:23:25.607577 [debug] [MainThread]: Partial parsing: updated file: dbt_://models/traffic_models/fct_summary.sql
[0m21:23:25.663061 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_trajectory.sql
[0m21:23:25.741015 [debug] [MainThread]: 1699: static parser successfully parsed traffic_models/fct_summary.sql
[0m21:23:25.793503 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394186aa00>]}
[0m21:23:25.820059 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f2670>]}
[0m21:23:25.821504 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:23:25.823106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f25e0>]}
[0m21:23:25.828820 [info ] [MainThread]: 
[0m21:23:25.831261 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:23:25.835284 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:23:25.876639 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:23:25.877583 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:23:25.878339 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:25.957778 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.08 seconds
[0m21:23:25.962667 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:23:25.985518 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:23:26.055464 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:23:26.056584 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:23:26.057460 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:23:26.071750 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:23:26.075061 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:23:26.077918 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:23:26.090082 [debug] [ThreadPool]: SQL status: SELECT 3 in 0.01 seconds
[0m21:23:26.113696 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:23:26.124462 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:23:26.165107 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.166027 [debug] [MainThread]: On master: BEGIN
[0m21:23:26.166729 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:23:26.185494 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:23:26.186429 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.187140 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:23:26.226658 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:23:26.230632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941931820>]}
[0m21:23:26.231909 [debug] [MainThread]: On master: ROLLBACK
[0m21:23:26.234112 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.235283 [debug] [MainThread]: On master: BEGIN
[0m21:23:26.238716 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:23:26.240186 [debug] [MainThread]: On master: COMMIT
[0m21:23:26.242289 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:26.243495 [debug] [MainThread]: On master: COMMIT
[0m21:23:26.245067 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:23:26.246024 [debug] [MainThread]: On master: Close
[0m21:23:26.254397 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:23:26.256031 [info ] [MainThread]: 
[0m21:23:26.290953 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:23:26.292353 [info ] [Thread-1  ]: 1 of 5 START table model warehouse.dim_types ................................... [RUN]
[0m21:23:26.294603 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:23:26.295578 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:23:26.296486 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:23:26.306861 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:23:26.308447 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:26.320290 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:23:26.688057 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:23:26.702814 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.703768 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:23:26.704522 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:26.725177 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:23:26.727387 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.728886 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:23:26.828681 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.1 seconds
[0m21:23:26.925458 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.926369 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
[0m21:23:26.932389 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:23:26.946569 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:26.947466 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:23:26.953694 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:23:27.189386 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:23:27.190335 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:27.191062 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:23:27.599219 [debug] [Thread-1  ]: SQL status: COMMIT in 0.41 seconds
[0m21:23:27.624086 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:23:27.625107 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:23:27.767093 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.14 seconds
[0m21:23:27.771300 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:27.772517 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:23:27.782233 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394035b130>]}
[0m21:23:27.784214 [info ] [Thread-1  ]: 1 of 5 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 1.49s]
[0m21:23:27.786580 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:23:27.788054 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:23:27.793548 [info ] [Thread-1  ]: 2 of 5 START table model warehouse.my_first_dbt_model .......................... [RUN]
[0m21:23:27.795974 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.804793 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:23:27.805805 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:23:27.825494 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:23:27.829534 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:27.832978 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:23:27.904605 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_first_dbt_model"
[0m21:23:27.906202 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.916820 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: BEGIN
[0m21:23:27.918040 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:27.932616 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:23:27.934329 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.937546 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */


  create  table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp"
  as (
    /*
    Welcome to your first dbt model!
    Did you know that you can also configure models directly within SQL files?
    This will override configurations stated in dbt_project.yml

    Try changing "table" to "view" below
*/



with source_data as (

    select 1 as id
    union all
    select null as id

)

select *
from source_data

/*
    Uncomment the line below to remove records with null `id` values
*/

-- where id is not null
  );
[0m21:23:27.944422 [debug] [Thread-1  ]: SQL status: SELECT 2 in 0.01 seconds
[0m21:23:27.966496 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.967390 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model" rename to "my_first_dbt_model__dbt_backup"
[0m21:23:27.969542 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:27.978840 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.979826 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
alter table "warehouse"."warehouse"."my_first_dbt_model__dbt_tmp" rename to "my_first_dbt_model"
[0m21:23:27.982130 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:27.991852 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:23:27.992863 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:27.993581 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: COMMIT
[0m21:23:28.201033 [debug] [Thread-1  ]: SQL status: COMMIT in 0.21 seconds
[0m21:23:28.212005 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_first_dbt_model"
[0m21:23:28.239075 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_first_dbt_model"} */
drop table if exists "warehouse"."warehouse"."my_first_dbt_model__dbt_backup" cascade
[0m21:23:28.268375 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.02 seconds
[0m21:23:28.273387 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.274536 [debug] [Thread-1  ]: On model.dbt_.my_first_dbt_model: Close
[0m21:23:28.283448 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941888970>]}
[0m21:23:28.285199 [info ] [Thread-1  ]: 2 of 5 OK created table model warehouse.my_first_dbt_model ..................... [[32mSELECT 2[0m in 0.49s]
[0m21:23:28.291873 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:23:28.296481 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:23:28.300897 [info ] [Thread-1  ]: 3 of 5 START table model warehouse.fct_summary ................................. [RUN]
[0m21:23:28.308053 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:23:28.315320 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:23:28.325112 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:23:28.360623 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:23:28.369246 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.370807 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:23:28.412640 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:23:28.414449 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.415328 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:23:28.416240 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:28.428577 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:23:28.432485 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.437712 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:23:28.496231 [debug] [Thread-1  ]: SQL status: SELECT 922 in 0.05 seconds
[0m21:23:28.513704 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.515856 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
[0m21:23:28.521670 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:28.535946 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:23:28.540005 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.542571 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:23:28.568195 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:23:28.582361 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:23:28.588404 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
[0m21:23:28.593744 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:23:28.606960 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.611453 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:23:28.616634 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39402ec6d0>]}
[0m21:23:28.624675 [info ] [Thread-1  ]: 3 of 5 OK created table model warehouse.fct_summary ............................ [[32mSELECT 922[0m in 0.31s]
[0m21:23:28.629119 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:23:28.635865 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:23:28.645892 [info ] [Thread-1  ]: 4 of 5 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:23:28.649787 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.673168 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:23:28.692417 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:23:28.771084 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:23:28.776133 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:28.777142 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:23:28.823481 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:23:28.845351 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.846379 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:23:28.847215 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:28.865049 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:23:28.866640 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:28.867599 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:23:32.825530 [debug] [Thread-1  ]: SQL status: SELECT 922 in 3.95 seconds
[0m21:23:32.838953 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:32.839837 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
[0m21:23:32.841906 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:32.853719 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:23:32.854854 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:32.855741 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:23:33.154016 [debug] [Thread-1  ]: SQL status: COMMIT in 0.3 seconds
[0m21:23:33.202160 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:23:33.203228 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
[0m21:23:33.205580 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.0 seconds
[0m21:23:33.214894 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.217165 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:23:33.221502 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39402a3130>]}
[0m21:23:33.223392 [info ] [Thread-1  ]: 4 of 5 OK created table model warehouse.fct_trajectory ......................... [[32mSELECT 922[0m in 4.57s]
[0m21:23:33.226582 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:23:33.227896 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:23:33.230269 [info ] [Thread-1  ]: 5 of 5 START view model warehouse.my_second_dbt_model .......................... [RUN]
[0m21:23:33.234343 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.235501 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:23:33.237176 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:23:33.268517 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:23:33.270384 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.271505 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:23:33.432436 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.my_second_dbt_model"
[0m21:23:33.445664 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.446583 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: BEGIN
[0m21:23:33.447285 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:23:33.474718 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:23:33.475830 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.481598 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */

  create view "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" as (
    -- Use the `ref` function to select from other models

select *
from "warehouse"."warehouse"."my_first_dbt_model"
where id = 1
  );
[0m21:23:33.489522 [debug] [Thread-1  ]: SQL status: CREATE VIEW in 0.01 seconds
[0m21:23:33.515545 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.516494 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
alter table "warehouse"."warehouse"."my_second_dbt_model__dbt_tmp" rename to "my_second_dbt_model"
[0m21:23:33.520366 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:23:33.552479 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:23:33.553445 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.554179 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: COMMIT
[0m21:23:33.732420 [debug] [Thread-1  ]: SQL status: COMMIT in 0.18 seconds
[0m21:23:33.744282 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.my_second_dbt_model"
[0m21:23:33.745194 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.my_second_dbt_model"} */
drop view if exists "warehouse"."warehouse"."my_second_dbt_model__dbt_backup" cascade
[0m21:23:33.760606 [debug] [Thread-1  ]: SQL status: DROP VIEW in 0.01 seconds
[0m21:23:33.768673 [debug] [Thread-1  ]: finished collecting timing info
[0m21:23:33.769661 [debug] [Thread-1  ]: On model.dbt_.my_second_dbt_model: Close
[0m21:23:33.782408 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a676c8f2-cadb-40cb-afcb-8120b89d9038', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39413f3100>]}
[0m21:23:33.789584 [info ] [Thread-1  ]: 5 of 5 OK created view model warehouse.my_second_dbt_model ..................... [[32mCREATE VIEW[0m in 0.55s]
[0m21:23:33.796892 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:23:33.802258 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:23:33.803483 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:33.824829 [debug] [MainThread]: On master: BEGIN
[0m21:23:33.825889 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:23:33.851210 [debug] [MainThread]: SQL status: BEGIN in 0.03 seconds
[0m21:23:33.852396 [debug] [MainThread]: On master: COMMIT
[0m21:23:33.855166 [debug] [MainThread]: Using postgres connection "master"
[0m21:23:33.857985 [debug] [MainThread]: On master: COMMIT
[0m21:23:33.864385 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:23:33.868473 [debug] [MainThread]: On master: Close
[0m21:23:33.873945 [info ] [MainThread]: 
[0m21:23:33.885251 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 8.04 seconds (8.04s).
[0m21:23:33.892634 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:23:33.895804 [debug] [MainThread]: Connection 'list_warehouse' was properly closed.
[0m21:23:33.906055 [debug] [MainThread]: Connection 'model.dbt_.my_second_dbt_model' was properly closed.
[0m21:23:33.958805 [info ] [MainThread]: 
[0m21:23:33.967208 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:23:33.972469 [info ] [MainThread]: 
[0m21:23:33.975525 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
[0m21:23:33.981075 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39418f2520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3941880e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f394183f610>]}


============================== 2022-07-27 21:26:16.553911 | 016b495a-9ff1-4450-9e2b-8900dbe57597 ==============================
[0m21:26:16.554034 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:26:16.563072 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m21:26:16.564100 [debug] [MainThread]: Tracking: tracking
[0m21:26:16.775196 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848640>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848610>]}
[0m21:26:16.962694 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m21:26:16.963876 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m21:26:16.988387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644732a00>]}
[0m21:26:17.026099 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644788e20>]}
[0m21:26:17.027651 [info ] [MainThread]: Found 5 models, 4 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:26:17.029595 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644788d30>]}
[0m21:26:17.038026 [info ] [MainThread]: 
[0m21:26:17.043606 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:26:17.047954 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:26:17.114424 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:26:17.119491 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:26:17.120754 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:17.134964 [debug] [ThreadPool]: SQL status: BEGIN in 0.01 seconds
[0m21:26:17.141351 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:26:17.146175 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:26:17.158169 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
[0m21:26:17.165131 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:26:17.171917 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:26:17.219562 [debug] [MainThread]: Using postgres connection "master"
[0m21:26:17.222779 [debug] [MainThread]: On master: BEGIN
[0m21:26:17.226614 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:26:17.244341 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:26:17.246698 [debug] [MainThread]: Using postgres connection "master"
[0m21:26:17.247600 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:26:17.287318 [debug] [MainThread]: SQL status: SELECT 1 in 0.04 seconds
[0m21:26:17.291398 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '016b495a-9ff1-4450-9e2b-8900dbe57597', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb64485b7c0>]}
[0m21:26:17.292984 [debug] [MainThread]: On master: ROLLBACK
[0m21:26:17.294615 [debug] [MainThread]: On master: Close
[0m21:26:17.301869 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:26:17.303880 [info ] [MainThread]: 
[0m21:26:17.333778 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:26:17.335354 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:26:17.336384 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:26:17.337253 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:26:17.346380 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:26:17.348249 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.349567 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:26:17.352739 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.361584 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:26:17.363100 [debug] [Thread-1  ]: Began running node model.dbt_.my_first_dbt_model
[0m21:26:17.365416 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_first_dbt_model"
[0m21:26:17.366516 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_first_dbt_model
[0m21:26:17.367443 [debug] [Thread-1  ]: Compiling model.dbt_.my_first_dbt_model
[0m21:26:17.386501 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_first_dbt_model"
[0m21:26:17.393902 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.395200 [debug] [Thread-1  ]: Began executing node model.dbt_.my_first_dbt_model
[0m21:26:17.396241 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.398191 [debug] [Thread-1  ]: Finished running node model.dbt_.my_first_dbt_model
[0m21:26:17.403017 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:26:17.406139 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:26:17.412820 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:26:17.413898 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:26:17.443288 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:26:17.453506 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.454677 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:26:17.461608 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.465248 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:26:17.469185 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:26:17.477417 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:26:17.478534 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:26:17.479470 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:26:17.496569 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:26:17.502535 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.503668 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:26:17.504722 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.506626 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:26:17.511958 [debug] [Thread-1  ]: Began running node model.dbt_.my_second_dbt_model
[0m21:26:17.514161 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.my_second_dbt_model"
[0m21:26:17.516950 [debug] [Thread-1  ]: Began compiling node model.dbt_.my_second_dbt_model
[0m21:26:17.518038 [debug] [Thread-1  ]: Compiling model.dbt_.my_second_dbt_model
[0m21:26:17.535753 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.my_second_dbt_model"
[0m21:26:17.537665 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.541515 [debug] [Thread-1  ]: Began executing node model.dbt_.my_second_dbt_model
[0m21:26:17.542924 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.546375 [debug] [Thread-1  ]: Finished running node model.dbt_.my_second_dbt_model
[0m21:26:17.547689 [debug] [Thread-1  ]: Began running node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.550163 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:26:17.552472 [debug] [Thread-1  ]: Began compiling node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.555552 [debug] [Thread-1  ]: Compiling test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.675162 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710"
[0m21:26:17.677012 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.677958 [debug] [Thread-1  ]: Began executing node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.678768 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.680603 [debug] [Thread-1  ]: Finished running node test.dbt_.not_null_my_first_dbt_model_id.5fb22c2710
[0m21:26:17.681673 [debug] [Thread-1  ]: Began running node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.683522 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.unique_my_first_dbt_model_id.16e066b321"
[0m21:26:17.684528 [debug] [Thread-1  ]: Began compiling node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.685301 [debug] [Thread-1  ]: Compiling test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.715970 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.unique_my_first_dbt_model_id.16e066b321"
[0m21:26:17.717481 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.718388 [debug] [Thread-1  ]: Began executing node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.720467 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.722282 [debug] [Thread-1  ]: Finished running node test.dbt_.unique_my_first_dbt_model_id.16e066b321
[0m21:26:17.723324 [debug] [Thread-1  ]: Began running node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.725164 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.not_null_my_second_dbt_model_id.151b76d778"
[0m21:26:17.726248 [debug] [Thread-1  ]: Began compiling node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.727186 [debug] [Thread-1  ]: Compiling test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.745985 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.not_null_my_second_dbt_model_id.151b76d778"
[0m21:26:17.747760 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.749145 [debug] [Thread-1  ]: Began executing node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.750183 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.752074 [debug] [Thread-1  ]: Finished running node test.dbt_.not_null_my_second_dbt_model_id.151b76d778
[0m21:26:17.753353 [debug] [Thread-1  ]: Began running node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.755556 [debug] [Thread-1  ]: Acquiring new postgres connection "test.dbt_.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:26:17.756726 [debug] [Thread-1  ]: Began compiling node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.757660 [debug] [Thread-1  ]: Compiling test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.773297 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_.unique_my_second_dbt_model_id.57a0f8c493"
[0m21:26:17.774948 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.775995 [debug] [Thread-1  ]: Began executing node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.777049 [debug] [Thread-1  ]: finished collecting timing info
[0m21:26:17.778958 [debug] [Thread-1  ]: Finished running node test.dbt_.unique_my_second_dbt_model_id.57a0f8c493
[0m21:26:17.782462 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:26:17.783353 [debug] [MainThread]: Connection 'test.dbt_.unique_my_second_dbt_model_id.57a0f8c493' was properly closed.
[0m21:26:17.807238 [info ] [MainThread]: Done.
[0m21:26:17.864839 [debug] [MainThread]: Acquiring new postgres connection "generate_catalog"
[0m21:26:17.865873 [info ] [MainThread]: Building catalog
[0m21:26:17.883317 [debug] [ThreadPool]: Acquiring new postgres connection "warehouse.information_schema"
[0m21:26:17.959344 [debug] [ThreadPool]: Using postgres connection "warehouse.information_schema"
[0m21:26:17.960899 [debug] [ThreadPool]: On warehouse.information_schema: BEGIN
[0m21:26:17.961961 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:26:17.979299 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:26:17.983030 [debug] [ThreadPool]: Using postgres connection "warehouse.information_schema"
[0m21:26:17.985419 [debug] [ThreadPool]: On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        tbl_desc.description as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        col_desc.description as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid
    left outer join pg_catalog.pg_description tbl_desc on (tbl_desc.objoid = tbl.oid and tbl_desc.objsubid = 0)
    left outer join pg_catalog.pg_description col_desc on (col_desc.objoid = tbl.oid and col_desc.objsubid = col.attnum)

    where (upper(sch.nspname) = upper('warehouse') or upper(sch.nspname) = upper('traffic_source'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence in ('p', 'u') -- [p]ermanent table or [u]nlogged table. Exclude [t]emporary tables
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
[0m21:26:18.442086 [debug] [ThreadPool]: SQL status: SELECT 11 in 0.45 seconds
[0m21:26:18.511323 [debug] [ThreadPool]: On warehouse.information_schema: ROLLBACK
[0m21:26:18.513793 [debug] [ThreadPool]: On warehouse.information_schema: Close
[0m21:26:18.561310 [info ] [MainThread]: Catalog written to /home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/target/catalog.json
[0m21:26:18.563307 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb644848550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb643e90ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb643e90d90>]}
[0m21:26:21.692619 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m21:26:21.694461 [debug] [MainThread]: Connection 'warehouse.information_schema' was properly closed.


============================== 2022-07-27 21:26:47.108713 | 501303cb-3088-4b70-9289-bc0960616cda ==============================
[0m21:26:47.108833 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:26:47.127122 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'port': 8080, 'open_browser': True, 'which': 'serve', 'indirect_selection': 'eager'}
[0m21:26:47.128143 [debug] [MainThread]: Tracking: tracking
[0m21:26:47.390794 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1da0c18730>]}
[0m21:26:47.407920 [info ] [MainThread]: Serving docs at 0.0.0.0:8080
[0m21:26:47.413581 [info ] [MainThread]: To access from your browser, navigate to:  http://localhost:8080
[0m21:26:47.415170 [info ] [MainThread]: 
[0m21:26:47.423949 [info ] [MainThread]: 
[0m21:26:47.426885 [info ] [MainThread]: Press Ctrl+C to exit.


============================== 2022-07-27 21:29:03.002006 | 9029f63b-320e-413e-9865-cf604216577b ==============================
[0m21:29:03.002127 [info ] [MainThread]: Running with dbt=1.2.0
[0m21:29:03.014055 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/home/bini/10_acad/week_12/ELT_Pipeline_Project_2/dbt_/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m21:29:03.016650 [debug] [MainThread]: Tracking: tracking
[0m21:29:03.224704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f154c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f155b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71f15580>]}
[0m21:29:03.415142 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 0 files changed.
[0m21:29:03.416668 [debug] [MainThread]: Partial parsing: deleted file: dbt_://models/example/my_first_dbt_model.sql
[0m21:29:03.417548 [debug] [MainThread]: Partial parsing: deleted file: dbt_://models/example/my_second_dbt_model.sql
[0m21:29:03.437810 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

[0m21:29:03.461434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e04970>]}
[0m21:29:03.486401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e4f1f0>]}
[0m21:29:03.487863 [info ] [MainThread]: Found 3 models, 0 tests, 0 snapshots, 0 analyses, 256 macros, 0 operations, 0 seed files, 1 source, 0 exposures, 0 metrics
[0m21:29:03.489547 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e4f2e0>]}
[0m21:29:03.494801 [info ] [MainThread]: 
[0m21:29:03.497400 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:29:03.517496 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse"
[0m21:29:03.629220 [debug] [ThreadPool]: Using postgres connection "list_warehouse"
[0m21:29:03.630423 [debug] [ThreadPool]: On list_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
[0m21:29:03.631360 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m21:29:03.651131 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.02 seconds
[0m21:29:03.661517 [debug] [ThreadPool]: On list_warehouse: Close
[0m21:29:03.674946 [debug] [ThreadPool]: Acquiring new postgres connection "list_warehouse_warehouse"
[0m21:29:03.701617 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:29:03.712340 [debug] [ThreadPool]: On list_warehouse_warehouse: BEGIN
[0m21:29:03.713465 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m21:29:03.729987 [debug] [ThreadPool]: SQL status: BEGIN in 0.02 seconds
[0m21:29:03.731074 [debug] [ThreadPool]: Using postgres connection "list_warehouse_warehouse"
[0m21:29:03.733555 [debug] [ThreadPool]: On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
[0m21:29:03.746222 [debug] [ThreadPool]: SQL status: SELECT 5 in 0.01 seconds
[0m21:29:03.754017 [debug] [ThreadPool]: On list_warehouse_warehouse: ROLLBACK
[0m21:29:03.761232 [debug] [ThreadPool]: On list_warehouse_warehouse: Close
[0m21:29:03.802220 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.803146 [debug] [MainThread]: On master: BEGIN
[0m21:29:03.803843 [debug] [MainThread]: Opening a new connection, currently in state init
[0m21:29:03.839207 [debug] [MainThread]: SQL status: BEGIN in 0.04 seconds
[0m21:29:03.846546 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.850329 [debug] [MainThread]: On master: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select distinct
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
[0m21:29:03.903369 [debug] [MainThread]: SQL status: SELECT 1 in 0.05 seconds
[0m21:29:03.907321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e8e430>]}
[0m21:29:03.909578 [debug] [MainThread]: On master: ROLLBACK
[0m21:29:03.913158 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.914510 [debug] [MainThread]: On master: BEGIN
[0m21:29:03.918550 [debug] [MainThread]: SQL status: BEGIN in 0.0 seconds
[0m21:29:03.921133 [debug] [MainThread]: On master: COMMIT
[0m21:29:03.922231 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:03.924186 [debug] [MainThread]: On master: COMMIT
[0m21:29:03.927829 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:29:03.929393 [debug] [MainThread]: On master: Close
[0m21:29:03.932872 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m21:29:03.940467 [info ] [MainThread]: 
[0m21:29:03.994250 [debug] [Thread-1  ]: Began running node model.dbt_.dim_types
[0m21:29:03.995664 [info ] [Thread-1  ]: 1 of 3 START table model warehouse.dim_types ................................... [RUN]
[0m21:29:03.998427 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.dim_types"
[0m21:29:03.999623 [debug] [Thread-1  ]: Began compiling node model.dbt_.dim_types
[0m21:29:04.000564 [debug] [Thread-1  ]: Compiling model.dbt_.dim_types
[0m21:29:04.017343 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.dim_types"
[0m21:29:04.018849 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.025086 [debug] [Thread-1  ]: Began executing node model.dbt_.dim_types
[0m21:29:04.429499 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.dim_types"
[0m21:29:04.431490 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.433330 [debug] [Thread-1  ]: On model.dbt_.dim_types: BEGIN
[0m21:29:04.434493 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:04.447130 [debug] [Thread-1  ]: SQL status: BEGIN in 0.01 seconds
[0m21:29:04.449840 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.451067 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
[0m21:29:04.511420 [debug] [Thread-1  ]: SQL status: SELECT 6 in 0.06 seconds
[0m21:29:04.536357 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.537451 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
[0m21:29:04.539767 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:04.550486 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.551727 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
[0m21:29:04.554059 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:04.657697 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:29:04.658832 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.659733 [debug] [Thread-1  ]: On model.dbt_.dim_types: COMMIT
[0m21:29:04.685823 [debug] [Thread-1  ]: SQL status: COMMIT in 0.03 seconds
[0m21:29:04.704878 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.dim_types"
[0m21:29:04.705964 [debug] [Thread-1  ]: On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
[0m21:29:04.720190 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.01 seconds
[0m21:29:04.724520 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.725687 [debug] [Thread-1  ]: On model.dbt_.dim_types: Close
[0m21:29:04.734165 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71569d90>]}
[0m21:29:04.735915 [info ] [Thread-1  ]: 1 of 3 OK created table model warehouse.dim_types .............................. [[32mSELECT 6[0m in 0.74s]
[0m21:29:04.740020 [debug] [Thread-1  ]: Finished running node model.dbt_.dim_types
[0m21:29:04.752509 [debug] [Thread-1  ]: Began running node model.dbt_.fct_summary
[0m21:29:04.758670 [info ] [Thread-1  ]: 2 of 3 START table model warehouse.fct_summary ................................. [RUN]
[0m21:29:04.762474 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_summary"
[0m21:29:04.764717 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_summary
[0m21:29:04.767117 [debug] [Thread-1  ]: Compiling model.dbt_.fct_summary
[0m21:29:04.786741 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_summary"
[0m21:29:04.794474 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:04.797062 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_summary
[0m21:29:04.835245 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_summary"
[0m21:29:04.837227 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.838550 [debug] [Thread-1  ]: On model.dbt_.fct_summary: BEGIN
[0m21:29:04.839537 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:04.856937 [debug] [Thread-1  ]: SQL status: BEGIN in 0.02 seconds
[0m21:29:04.858705 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.862311 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:29:04.925655 [debug] [Thread-1  ]: SQL status: SELECT 922 in 0.06 seconds
[0m21:29:04.954352 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:04.963268 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
[0m21:29:04.970234 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:05.000059 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.001026 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
[0m21:29:05.010088 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.01 seconds
[0m21:29:05.067107 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:29:05.068029 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.073685 [debug] [Thread-1  ]: On model.dbt_.fct_summary: COMMIT
[0m21:29:05.097148 [debug] [Thread-1  ]: SQL status: COMMIT in 0.02 seconds
[0m21:29:05.125135 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_summary"
[0m21:29:05.126133 [debug] [Thread-1  ]: On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
[0m21:29:05.205353 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.08 seconds
[0m21:29:05.210623 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:05.211795 [debug] [Thread-1  ]: On model.dbt_.fct_summary: Close
[0m21:29:05.215198 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b705245e0>]}
[0m21:29:05.216983 [info ] [Thread-1  ]: 2 of 3 OK created table model warehouse.fct_summary ............................ [[32mSELECT 922[0m in 0.45s]
[0m21:29:05.250056 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_summary
[0m21:29:05.253783 [debug] [Thread-1  ]: Began running node model.dbt_.fct_trajectory
[0m21:29:05.260207 [info ] [Thread-1  ]: 3 of 3 START table model warehouse.fct_trajectory .............................. [RUN]
[0m21:29:05.268650 [debug] [Thread-1  ]: Acquiring new postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.270953 [debug] [Thread-1  ]: Began compiling node model.dbt_.fct_trajectory
[0m21:29:05.275084 [debug] [Thread-1  ]: Compiling model.dbt_.fct_trajectory
[0m21:29:05.295548 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_.fct_trajectory"
[0m21:29:05.303106 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:05.307960 [debug] [Thread-1  ]: Began executing node model.dbt_.fct_trajectory
[0m21:29:05.368955 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_.fct_trajectory"
[0m21:29:05.370547 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.376668 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: BEGIN
[0m21:29:05.377427 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m21:29:05.406328 [debug] [Thread-1  ]: SQL status: BEGIN in 0.03 seconds
[0m21:29:05.407598 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:05.408429 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
[0m21:29:08.279601 [debug] [Thread-1  ]: SQL status: SELECT 922 in 2.87 seconds
[0m21:29:08.294761 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.295659 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
[0m21:29:08.300470 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:08.311663 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.313166 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
[0m21:29:08.315950 [debug] [Thread-1  ]: SQL status: ALTER TABLE in 0.0 seconds
[0m21:29:08.328321 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:29:08.329398 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.330336 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: COMMIT
[0m21:29:08.368289 [debug] [Thread-1  ]: SQL status: COMMIT in 0.04 seconds
[0m21:29:08.376276 [debug] [Thread-1  ]: Using postgres connection "model.dbt_.fct_trajectory"
[0m21:29:08.377725 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
[0m21:29:08.435389 [debug] [Thread-1  ]: SQL status: DROP TABLE in 0.06 seconds
[0m21:29:08.442629 [debug] [Thread-1  ]: finished collecting timing info
[0m21:29:08.444296 [debug] [Thread-1  ]: On model.dbt_.fct_trajectory: Close
[0m21:29:08.457247 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9029f63b-320e-413e-9865-cf604216577b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b704bc580>]}
[0m21:29:08.458943 [info ] [Thread-1  ]: 3 of 3 OK created table model warehouse.fct_trajectory ......................... [[32mSELECT 922[0m in 3.19s]
[0m21:29:08.476650 [debug] [Thread-1  ]: Finished running node model.dbt_.fct_trajectory
[0m21:29:08.488931 [debug] [MainThread]: Acquiring new postgres connection "master"
[0m21:29:08.490116 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:08.490883 [debug] [MainThread]: On master: BEGIN
[0m21:29:08.491572 [debug] [MainThread]: Opening a new connection, currently in state closed
[0m21:29:08.508828 [debug] [MainThread]: SQL status: BEGIN in 0.02 seconds
[0m21:29:08.509793 [debug] [MainThread]: On master: COMMIT
[0m21:29:08.510515 [debug] [MainThread]: Using postgres connection "master"
[0m21:29:08.511202 [debug] [MainThread]: On master: COMMIT
[0m21:29:08.512656 [debug] [MainThread]: SQL status: COMMIT in 0.0 seconds
[0m21:29:08.513618 [debug] [MainThread]: On master: Close
[0m21:29:08.515756 [info ] [MainThread]: 
[0m21:29:08.521781 [info ] [MainThread]: Finished running 3 table models in 0 hours 0 minutes and 5.02 seconds (5.02s).
[0m21:29:08.524233 [debug] [MainThread]: Connection 'master' was properly closed.
[0m21:29:08.526922 [debug] [MainThread]: Connection 'model.dbt_.fct_trajectory' was properly closed.
[0m21:29:08.589151 [info ] [MainThread]: 
[0m21:29:08.595585 [info ] [MainThread]: [32mCompleted successfully[0m
[0m21:29:08.597390 [info ] [MainThread]: 
[0m21:29:08.598842 [info ] [MainThread]: Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
[0m21:29:08.600442 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71e8e220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b7156c160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7b71ddcd60>]}
2022-07-28 00:32:11.150893 (MainThread): Running with dbt=0.16.1
2022-07-28 00:32:11.514781 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 00:32:11.566477 (MainThread): Tracking: tracking
2022-07-28 00:32:11.698661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d790e6510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d7913d4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d78f87850>]}
2022-07-28 00:32:11.861962 (MainThread): Partial parsing not enabled
2022-07-28 00:32:11.940918 (MainThread): Parsing macros/core.sql
2022-07-28 00:32:11.962447 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 00:32:11.997903 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 00:32:12.007088 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 00:32:12.147313 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 00:32:12.246055 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 00:32:12.276333 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 00:32:12.286892 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 00:32:12.394453 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 00:32:12.458764 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 00:32:12.489309 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 00:32:12.532735 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 00:32:12.567577 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 00:32:12.767575 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 00:32:12.773897 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 00:32:12.784847 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 00:32:12.795968 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 00:32:12.801033 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 00:32:12.811262 (MainThread): Parsing macros/etc/query.sql
2022-07-28 00:32:12.819732 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 00:32:12.862045 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 00:32:12.866958 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 00:32:12.877254 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 00:32:12.885765 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 00:32:12.895773 (MainThread): Parsing macros/relations.sql
2022-07-28 00:32:12.904275 (MainThread): Parsing macros/adapters.sql
2022-07-28 00:32:13.024957 (MainThread): Parsing macros/catalog.sql
2022-07-28 00:32:13.039806 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 00:32:13.131146 (MainThread): Partial parsing not enabled
2022-07-28 00:32:13.326965 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:32:13.327429 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:13.420366 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:32:13.420803 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:13.447795 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:32:13.448295 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:14.271731 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 00:32:15.414955 (MainThread): scipy not found, skipping conversion test.
2022-07-28 00:32:15.425105 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 00:32:15.426056 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 00:32:15.436987 (MainThread): 
2022-07-28 00:32:15.438685 (MainThread): Acquiring new postgres connection "master".
2022-07-28 00:32:15.439111 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:32:15.557077 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 00:32:15.557824 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 00:32:16.609463 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 00:32:16.612300 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:32:16.735234 (ThreadPoolExecutor-0_0): Got an error when attempting to open a postgres connection: 'could not connect to server: Connection refused
	Is the server running on host "localhost" (127.0.0.1) and accepting
	TCP/IP connections on port 5432?
could not connect to server: Cannot assign requested address
	Is the server running on host "localhost" (::1) and accepting
	TCP/IP connections on port 5432?
'
2022-07-28 00:32:16.740561 (ThreadPoolExecutor-0_0): Error running SQL: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:32:16.741046 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 00:32:16.741367 (ThreadPoolExecutor-0_0): On list_warehouse: No close available on handle
2022-07-28 00:32:16.741804 (ThreadPoolExecutor-0_0): Error running SQL: macro list_schemas
2022-07-28 00:32:16.742084 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 00:32:16.747596 (MainThread): Connection 'master' was properly closed.
2022-07-28 00:32:16.749186 (MainThread): Connection 'list_warehouse' was properly closed.
2022-07-28 00:32:16.749678 (MainThread): ERROR: Database Error
  could not connect to server: Connection refused
  	Is the server running on host "localhost" (127.0.0.1) and accepting
  	TCP/IP connections on port 5432?
  could not connect to server: Cannot assign requested address
  	Is the server running on host "localhost" (::1) and accepting
  	TCP/IP connections on port 5432?
  
2022-07-28 00:32:16.752667 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d687631d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d68763990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9d68763510>]}
2022-07-28 00:32:16.753418 (MainThread): Flushing usage events
2022-07-28 00:37:52.205092 (MainThread): Running with dbt=0.16.1
2022-07-28 00:37:52.655231 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 00:37:52.702734 (MainThread): Tracking: tracking
2022-07-28 00:37:52.752627 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee0b4490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee172550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2ee177f10>]}
2022-07-28 00:37:52.895856 (MainThread): Partial parsing not enabled
2022-07-28 00:37:52.927645 (MainThread): Parsing macros/core.sql
2022-07-28 00:37:52.948888 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 00:37:52.991752 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 00:37:53.002290 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 00:37:53.151051 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 00:37:53.236308 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 00:37:53.265645 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 00:37:53.274887 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 00:37:53.371223 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 00:37:53.429718 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 00:37:53.461440 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 00:37:53.517160 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 00:37:53.549208 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 00:37:53.731881 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 00:37:53.737205 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 00:37:53.746953 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 00:37:53.757737 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 00:37:53.762599 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 00:37:53.770843 (MainThread): Parsing macros/etc/query.sql
2022-07-28 00:37:53.776390 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 00:37:53.818348 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 00:37:53.823448 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 00:37:53.833155 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 00:37:53.838758 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 00:37:53.846775 (MainThread): Parsing macros/relations.sql
2022-07-28 00:37:53.854372 (MainThread): Parsing macros/adapters.sql
2022-07-28 00:37:53.976886 (MainThread): Parsing macros/catalog.sql
2022-07-28 00:37:53.994295 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 00:37:54.134882 (MainThread): Partial parsing not enabled
2022-07-28 00:37:54.321484 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:54.322132 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:54.426389 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:54.426990 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:54.454370 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:37:54.454983 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:55.161614 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 00:37:56.276738 (MainThread): scipy not found, skipping conversion test.
2022-07-28 00:37:56.286898 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 00:37:56.289050 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 00:37:56.298397 (MainThread): 
2022-07-28 00:37:56.299754 (MainThread): Acquiring new postgres connection "master".
2022-07-28 00:37:56.300394 (MainThread): Opening a new connection, currently in state init
2022-07-28 00:37:56.379593 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 00:37:56.380778 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 00:37:56.918092 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 00:37:56.919010 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 00:37:56.950243 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-28 00:37:57.049150 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.050144 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 00:37:57.060968 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.061487 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 00:37:57.063552 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:57.064049 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 00:37:57.064484 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 00:37:57.155506 (ThreadPoolExecutor-1_0): SQL status: SELECT 6 in 0.09 seconds
2022-07-28 00:37:57.179388 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 00:37:57.268022 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.269542 (MainThread): On master: BEGIN
2022-07-28 00:37:57.281262 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 00:37:57.281968 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.282422 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 00:37:57.909873 (MainThread): SQL status: SELECT 1 in 0.63 seconds
2022-07-28 00:37:57.920256 (MainThread): On master: ROLLBACK
2022-07-28 00:37:57.923132 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.923580 (MainThread): On master: BEGIN
2022-07-28 00:37:57.927050 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:57.928494 (MainThread): On master: COMMIT
2022-07-28 00:37:57.933782 (MainThread): Using postgres connection "master".
2022-07-28 00:37:57.934789 (MainThread): On master: COMMIT
2022-07-28 00:37:57.938088 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 00:37:57.939684 (MainThread): 00:37:57 | Concurrency: 1 threads (target='dev')
2022-07-28 00:37:57.946805 (MainThread): 00:37:57 | 
2022-07-28 00:37:58.000752 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 00:37:58.002309 (Thread-1): 00:37:57 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 00:37:58.003327 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.003701 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 00:37:58.005027 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 00:37:58.105501 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 00:37:58.133863 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.228089 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.228890 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 00:37:58.253155 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 00:37:58.269169 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.269623 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 00:37:58.270666 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.335573 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 00:37:58.337094 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.337489 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 00:37:58.339093 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:58.339933 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.340644 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 00:37:58.512950 (Thread-1): SQL status: SELECT 6 in 0.17 seconds
2022-07-28 00:37:58.528446 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.528896 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 00:37:58.530331 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:58.540443 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.541144 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 00:37:58.542391 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:58.545546 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 00:37:58.545986 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.546263 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 00:37:58.586169 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 00:37:58.593840 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 00:37:58.594339 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 00:37:58.862630 (Thread-1): SQL status: DROP TABLE in 0.27 seconds
2022-07-28 00:37:58.872968 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.876703 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb882c50>]}
2022-07-28 00:37:58.879176 (Thread-1): 00:37:58 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.87s]
2022-07-28 00:37:58.882897 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 00:37:58.885088 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 00:37:58.885722 (Thread-1): 00:37:58 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 00:37:58.887471 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.887908 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 00:37:58.888287 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 00:37:58.917763 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 00:37:58.918688 (Thread-1): finished collecting timing info
2022-07-28 00:37:58.950876 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.951360 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 00:37:58.952679 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.961467 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.961943 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 00:37:58.963257 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:37:58.969047 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 00:37:58.970340 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.970756 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 00:37:58.971916 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:37:58.972572 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:58.972869 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 00:37:59.637997 (Thread-1): SQL status: SELECT 922 in 0.66 seconds
2022-07-28 00:37:59.655610 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.656074 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 00:37:59.657770 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:59.666831 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.667299 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 00:37:59.668654 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:37:59.671929 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 00:37:59.672470 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.672766 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 00:37:59.732429 (Thread-1): SQL status: COMMIT in 0.06 seconds
2022-07-28 00:37:59.739400 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 00:37:59.739846 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 00:37:59.839979 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2022-07-28 00:37:59.857515 (Thread-1): finished collecting timing info
2022-07-28 00:37:59.861673 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb843d90>]}
2022-07-28 00:37:59.862560 (Thread-1): 00:37:59 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.97s]
2022-07-28 00:37:59.863026 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 00:37:59.863475 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 00:37:59.867437 (Thread-1): 00:37:59 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 00:37:59.868604 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:37:59.869000 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 00:37:59.870631 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 00:37:59.949694 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 00:37:59.950887 (Thread-1): finished collecting timing info
2022-07-28 00:38:00.016227 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.016875 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 00:38:00.018231 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:38:00.046213 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.046835 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 00:38:00.049712 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 00:38:00.087138 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 00:38:00.089213 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.089674 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 00:38:00.090423 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:38:00.090854 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:00.091139 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 00:38:32.091043 (Thread-1): SQL status: SELECT 922 in 32.00 seconds
2022-07-28 00:38:32.113927 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.114568 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 00:38:32.116211 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:38:32.125976 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.126662 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 00:38:32.128436 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 00:38:32.132127 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 00:38:32.133009 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.133472 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 00:38:32.170898 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 00:38:32.193682 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 00:38:32.194143 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 00:38:32.217721 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 00:38:32.244294 (Thread-1): finished collecting timing info
2022-07-28 00:38:32.255718 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6194795f-5561-4cbc-bc08-e5233ecb8bdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb996790>]}
2022-07-28 00:38:32.256707 (Thread-1): 00:38:32 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 32.38s]
2022-07-28 00:38:32.261237 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 00:38:32.323457 (MainThread): Using postgres connection "master".
2022-07-28 00:38:32.323961 (MainThread): On master: BEGIN
2022-07-28 00:38:32.328506 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 00:38:32.329048 (MainThread): On master: COMMIT
2022-07-28 00:38:32.329362 (MainThread): Using postgres connection "master".
2022-07-28 00:38:32.329629 (MainThread): On master: COMMIT
2022-07-28 00:38:32.332698 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 00:38:32.333984 (MainThread): 00:38:32 | 
2022-07-28 00:38:32.334976 (MainThread): 00:38:32 | Finished running 3 table models in 36.03s.
2022-07-28 00:38:32.335774 (MainThread): Connection 'master' was left open.
2022-07-28 00:38:32.340511 (MainThread): On master: Close
2022-07-28 00:38:32.341269 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 00:38:32.341696 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 00:38:32.450035 (MainThread): 
2022-07-28 00:38:32.453875 (MainThread): Completed successfully
2022-07-28 00:38:32.455693 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 00:38:32.462940 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eba13e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb991490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc2eb97f490>]}
2022-07-28 00:38:32.463732 (MainThread): Flushing usage events
2022-07-28 17:27:44.931951 (MainThread): Running with dbt=0.16.1
2022-07-28 17:27:47.442307 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 17:27:47.495315 (MainThread): Tracking: tracking
2022-07-28 17:27:44.931917 (MainThread): Running with dbt=0.16.1
2022-07-28 17:27:47.542916 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 17:27:47.544298 (MainThread): Tracking: tracking
2022-07-28 17:27:48.443914 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a22252c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a2238cc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a22249310>]}
2022-07-28 17:27:48.443949 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4beab9ebd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4bead517d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4beac26f50>]}
2022-07-28 17:27:48.856693 (MainThread): Partial parsing not enabled
2022-07-28 17:27:48.866226 (MainThread): Partial parsing not enabled
2022-07-28 17:27:49.041374 (MainThread): Parsing macros/core.sql
2022-07-28 17:27:49.041426 (MainThread): Parsing macros/core.sql
2022-07-28 17:27:49.110263 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 17:27:49.124781 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 17:27:49.250575 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 17:27:49.269097 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 17:27:49.276125 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 17:27:49.304098 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 17:27:49.653079 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 17:27:49.780328 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 17:27:50.201270 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 17:27:50.207143 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 17:27:50.399483 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 17:27:50.400796 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 17:27:50.453632 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 17:27:50.459761 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 17:27:51.117039 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 17:27:51.117059 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 17:27:51.560786 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 17:27:51.563892 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 17:27:51.832227 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 17:27:51.836226 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 17:27:52.171134 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 17:27:52.171270 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 17:27:52.769819 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 17:27:52.769859 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 17:27:53.379879 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 17:27:53.383822 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 17:27:53.990360 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 17:27:53.990392 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 17:27:54.004807 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 17:27:54.005493 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 17:27:54.024367 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 17:27:54.025125 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 17:27:54.032054 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 17:27:54.034652 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 17:27:54.055240 (MainThread): Parsing macros/etc/query.sql
2022-07-28 17:27:54.067968 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 17:27:54.079536 (MainThread): Parsing macros/etc/query.sql
2022-07-28 17:27:54.128843 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 17:27:55.688690 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 17:27:55.688713 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 17:27:55.694363 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 17:27:55.695202 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 17:27:55.943710 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 17:27:55.943709 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 17:27:55.976933 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 17:27:55.976939 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 17:27:57.010844 (MainThread): Parsing macros/relations.sql
2022-07-28 17:27:57.010857 (MainThread): Parsing macros/relations.sql
2022-07-28 17:27:57.035467 (MainThread): Parsing macros/adapters.sql
2022-07-28 17:27:57.046497 (MainThread): Parsing macros/adapters.sql
2022-07-28 17:27:57.743080 (MainThread): Parsing macros/catalog.sql
2022-07-28 17:27:57.743077 (MainThread): Parsing macros/catalog.sql
2022-07-28 17:27:57.755136 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 17:27:57.772155 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 17:27:58.492431 (MainThread): Partial parsing not enabled
2022-07-28 17:27:58.492887 (MainThread): Partial parsing not enabled
2022-07-28 17:28:00.907260 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:00.911776 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:00.930056 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:00.930642 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.029969 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:02.035852 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.037414 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:02.037992 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.337641 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:02.338093 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:02.347278 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:02.347776 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:06.637177 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 17:28:06.713194 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 17:28:16.690952 (MainThread): scipy not found, skipping conversion test.
2022-07-28 17:28:16.721684 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 17:28:16.722939 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 17:28:16.748861 (MainThread): 
2022-07-28 17:28:16.752907 (MainThread): Acquiring new postgres connection "master".
2022-07-28 17:28:16.753325 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:16.930209 (MainThread): scipy not found, skipping conversion test.
2022-07-28 17:28:16.966484 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 17:28:16.979544 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 17:28:17.019261 (MainThread): 
2022-07-28 17:28:17.030828 (MainThread): Acquiring new postgres connection "master".
2022-07-28 17:28:17.031324 (MainThread): Opening a new connection, currently in state init
2022-07-28 17:28:19.273134 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 17:28:19.274112 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 17:28:19.278597 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 17:28:19.279192 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 17:28:20.555764 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 17:28:20.559660 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 17:28:20.916858 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 17:28:20.917369 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 17:28:21.127079 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.57 seconds
2022-07-28 17:28:21.129215 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.21 seconds
2022-07-28 17:28:22.199661 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.200004 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.200114 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 17:28:22.200429 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 17:28:22.200426 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-28 17:28:22.200691 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-28 17:28:22.206938 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.214761 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.215597 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.218057 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:22.218573 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.218879 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-28 17:28:22.223884 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.227914 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:22.228449 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.228752 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-28 17:28:22.590948 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.37 seconds
2022-07-28 17:28:22.596741 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-28 17:28:22.597234 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-28 17:28:22.597534 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-28 17:28:22.708515 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.11 seconds
2022-07-28 17:28:22.709216 (ThreadPoolExecutor-0_0): Postgres error: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.

2022-07-28 17:28:22.715175 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: ROLLBACK
2022-07-28 17:28:22.720493 (ThreadPoolExecutor-0_0): Error running SQL: macro create_schema
2022-07-28 17:28:22.720947 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-07-28 17:28:22.724267 (MainThread): Connection 'master' was properly closed.
2022-07-28 17:28:22.724891 (MainThread): Connection 'create_warehouse_warehouse' was left open.
2022-07-28 17:28:22.725321 (MainThread): On create_warehouse_warehouse: Close
2022-07-28 17:28:22.726299 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4bda0ec9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4be8378910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4be83641d0>]}
2022-07-28 17:28:22.727737 (MainThread): Flushing usage events
2022-07-28 17:28:22.808927 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.809735 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 17:28:22.816732 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.817209 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 17:28:22.831087 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 17:28:22.831604 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 17:28:22.831905 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 17:28:24.051290 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 1.22 seconds
2022-07-28 17:28:24.055223 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 17:28:24.300696 (MainThread): Using postgres connection "master".
2022-07-28 17:28:24.302953 (MainThread): On master: BEGIN
2022-07-28 17:28:24.323195 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 17:28:24.323983 (MainThread): Using postgres connection "master".
2022-07-28 17:28:24.324307 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 17:28:25.386565 (MainThread): SQL status: SELECT 0 in 1.06 seconds
2022-07-28 17:28:25.389940 (MainThread): On master: ROLLBACK
2022-07-28 17:28:25.392067 (MainThread): Using postgres connection "master".
2022-07-28 17:28:25.392536 (MainThread): On master: BEGIN
2022-07-28 17:28:25.394702 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:25.395225 (MainThread): On master: COMMIT
2022-07-28 17:28:25.395631 (MainThread): Using postgres connection "master".
2022-07-28 17:28:25.395913 (MainThread): On master: COMMIT
2022-07-28 17:28:25.396783 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 17:28:25.397878 (MainThread): 17:28:25 | Concurrency: 1 threads (target='dev')
2022-07-28 17:28:25.398378 (MainThread): 17:28:25 | 
2022-07-28 17:28:25.752642 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 17:28:25.753661 (Thread-1): 17:28:25 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 17:28:25.758727 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:25.759151 (Thread-1): Opening a new connection, currently in state init
2022-07-28 17:28:25.771698 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 17:28:25.884176 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 17:28:26.307864 (Thread-1): finished collecting timing info
2022-07-28 17:28:26.523080 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:26.523659 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 17:28:26.646932 (Thread-1): SQL status: DROP TABLE in 0.12 seconds
2022-07-28 17:28:26.658443 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:26.665630 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 17:28:26.667898 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:26.806877 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 17:28:27.004845 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.005299 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 17:28:27.011069 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:27.011733 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.012048 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 17:28:27.474428 (Thread-1): SQL status: SELECT 6 in 0.46 seconds
2022-07-28 17:28:27.493454 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.493981 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 17:28:27.495297 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 17:28:27.498581 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 17:28:27.499033 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.499319 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 17:28:27.547076 (Thread-1): SQL status: COMMIT in 0.05 seconds
2022-07-28 17:28:27.555239 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 17:28:27.560101 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 17:28:27.561493 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:27.581316 (Thread-1): finished collecting timing info
2022-07-28 17:28:27.589718 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a11a00650>]}
2022-07-28 17:28:27.590609 (Thread-1): 17:28:27 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.83s]
2022-07-28 17:28:27.591093 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 17:28:27.596632 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 17:28:27.597470 (Thread-1): 17:28:27 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 17:28:27.598522 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.598898 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 17:28:27.599198 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 17:28:27.686302 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 17:28:27.688006 (Thread-1): finished collecting timing info
2022-07-28 17:28:27.782861 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.783402 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 17:28:27.792032 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 17:28:27.837345 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.840376 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 17:28:27.841489 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:27.853652 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 17:28:27.855008 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.855733 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 17:28:27.856510 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:27.856936 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:27.857223 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 17:28:28.253221 (Thread-1): SQL status: SELECT 922 in 0.40 seconds
2022-07-28 17:28:28.272179 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.272648 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 17:28:28.276775 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 17:28:28.287548 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 17:28:28.288031 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.288353 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 17:28:28.391051 (Thread-1): SQL status: COMMIT in 0.10 seconds
2022-07-28 17:28:28.408946 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 17:28:28.409846 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 17:28:28.414678 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:28.435906 (Thread-1): finished collecting timing info
2022-07-28 17:28:28.443455 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb8b450>]}
2022-07-28 17:28:28.446543 (Thread-1): 17:28:28 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.84s]
2022-07-28 17:28:28.447080 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 17:28:28.447646 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 17:28:28.448134 (Thread-1): 17:28:28 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 17:28:28.451184 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.451776 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 17:28:28.452142 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 17:28:28.527570 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 17:28:28.532767 (Thread-1): finished collecting timing info
2022-07-28 17:28:28.597566 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.605762 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 17:28:28.606966 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:28.629913 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.630373 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 17:28:28.636887 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 17:28:28.649023 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 17:28:28.650305 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.655220 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 17:28:28.656048 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:28.656479 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:28.658193 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 17:28:31.225175 (MainThread): Encountered an error:
2022-07-28 17:28:31.229086 (MainThread): Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.
2022-07-28 17:28:33.499505 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 371, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 331, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/run.py", line 199, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 472, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.7/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 440, in create_schema
    adapter.create_schema(db, schema)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/impl.py", line 183, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 952, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 163, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 97, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 73, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.

2022-07-28 17:28:40.657022 (Thread-1): SQL status: SELECT 922 in 12.00 seconds
2022-07-28 17:28:40.689169 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:40.693498 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 17:28:40.924017 (Thread-1): SQL status: ALTER TABLE in 0.23 seconds
2022-07-28 17:28:40.927216 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 17:28:40.936371 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:40.936713 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 17:28:40.980045 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 17:28:41.017859 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 17:28:41.025977 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 17:28:41.036536 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 17:28:41.047094 (Thread-1): finished collecting timing info
2022-07-28 17:28:41.050305 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '21dd752f-99dc-4c8c-8fc8-7dc44c89b526', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1faf7b10>]}
2022-07-28 17:28:41.052057 (Thread-1): 17:28:41 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 12.60s]
2022-07-28 17:28:41.052606 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 17:28:41.115226 (MainThread): Using postgres connection "master".
2022-07-28 17:28:41.115816 (MainThread): On master: BEGIN
2022-07-28 17:28:41.119830 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 17:28:41.120429 (MainThread): On master: COMMIT
2022-07-28 17:28:41.120756 (MainThread): Using postgres connection "master".
2022-07-28 17:28:41.121025 (MainThread): On master: COMMIT
2022-07-28 17:28:41.127677 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-07-28 17:28:41.128991 (MainThread): 17:28:41 | 
2022-07-28 17:28:41.129482 (MainThread): 17:28:41 | Finished running 3 table models in 24.38s.
2022-07-28 17:28:41.129834 (MainThread): Connection 'master' was left open.
2022-07-28 17:28:41.130316 (MainThread): On master: Close
2022-07-28 17:28:41.130938 (MainThread): Connection 'create_warehouse_warehouse' was left open.
2022-07-28 17:28:41.131310 (MainThread): On create_warehouse_warehouse: Close
2022-07-28 17:28:41.141748 (MainThread): Connection 'list_warehouse_warehouse' was left open.
2022-07-28 17:28:41.143807 (MainThread): On list_warehouse_warehouse: Close
2022-07-28 17:28:41.151621 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 17:28:41.152095 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 17:28:41.301464 (MainThread): 
2022-07-28 17:28:41.309245 (MainThread): Completed successfully
2022-07-28 17:28:41.309976 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 17:28:41.310729 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb29690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb29650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3a1fb34a10>]}
2022-07-28 17:28:41.321065 (MainThread): Flushing usage events
2022-07-28 20:27:33.187691 (MainThread): Running with dbt=0.16.1
2022-07-28 20:27:34.388379 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:27:34.472419 (MainThread): Tracking: tracking
2022-07-28 20:27:34.686581 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c150a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c087510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3c087f90>]}
2022-07-28 20:27:35.062666 (MainThread): Partial parsing not enabled
2022-07-28 20:27:35.128239 (MainThread): Parsing macros/core.sql
2022-07-28 20:27:35.217092 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:27:35.347026 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:27:35.384061 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:27:35.857714 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:27:36.103991 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:27:36.166944 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:27:36.187767 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:27:36.401345 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:27:36.543393 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:27:36.578000 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:27:36.623410 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:27:36.680140 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:27:37.283216 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:27:37.294101 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:27:37.327966 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:27:37.359747 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:27:37.374985 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:27:37.399863 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:27:37.416130 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:27:37.552204 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:27:37.557898 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:27:37.571585 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:27:37.579190 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:27:37.612941 (MainThread): Parsing macros/relations.sql
2022-07-28 20:27:37.635018 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:27:37.863304 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:27:37.894566 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:27:38.029168 (MainThread): Partial parsing not enabled
2022-07-28 20:27:38.203001 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:27:38.203556 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.293838 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:27:38.294286 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.335716 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:27:38.336175 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:27:38.373066 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc3979f810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc397aae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdc397ff750>]}
2022-07-28 20:27:38.373827 (MainThread): Flushing usage events
2022-07-28 20:27:39.718351 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 20:27:39.720822 (MainThread): Encountered an error:
2022-07-28 20:27:39.722165 (MainThread): Compilation Error
  Error reading dbt_: traffic_models/schema.yml - Runtime Error
    Syntax error near line 16
    ------------------------------
    13 |       - name: Id
    14 |         tests:
    15 |         tags: [dim_type_tests]
    16 |           - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 13, column 9:
              - name: Id
                ^
    expected <block end>, but found '<block sequence start>'
      in "<unicode string>", line 16, column 11:
                  - unique
                  ^
2022-07-28 20:27:39.789953 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/yaml_helper.py", line 49, in load_yaml_text
    return yaml.safe_load(contents)
  File "/usr/local/lib/python3.7/site-packages/yaml/__init__.py", line 162, in safe_load
    return load(stream, SafeLoader)
  File "/usr/local/lib/python3.7/site-packages/yaml/__init__.py", line 114, in load
    return loader.get_single_data()
  File "/usr/local/lib/python3.7/site-packages/yaml/constructor.py", line 41, in get_single_data
    node = self.get_single_node()
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 36, in get_single_node
    document = self.compose_document()
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 55, in compose_document
    node = self.compose_node(None, None)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 82, in compose_node
    node = self.compose_sequence_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 111, in compose_sequence_node
    node.value.append(self.compose_node(node, index))
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 133, in compose_mapping_node
    item_value = self.compose_node(node, item_key)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 82, in compose_node
    node = self.compose_sequence_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 111, in compose_sequence_node
    node.value.append(self.compose_node(node, index))
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 84, in compose_node
    node = self.compose_mapping_node(anchor)
  File "/usr/local/lib/python3.7/site-packages/yaml/composer.py", line 127, in compose_mapping_node
    while not self.check_event(MappingEndEvent):
  File "/usr/local/lib/python3.7/site-packages/yaml/parser.py", line 98, in check_event
    self.current_event = self.state()
  File "/usr/local/lib/python3.7/site-packages/yaml/parser.py", line 439, in parse_block_mapping_key
    "expected <block end>, but found %r" % token.id, token.start_mark)
yaml.parser.ParserError: while parsing a block mapping
  in "<unicode string>", line 13, column 9:
          - name: Id
            ^
expected <block end>, but found '<block sequence start>'
  in "<unicode string>", line 16, column 11:
              - unique
              ^

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 163, in _yaml_from_file
    return load_yaml_text(source_file.contents)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/yaml_helper.py", line 56, in load_yaml_text
    raise dbt.exceptions.ValidationException(error)
dbt.exceptions.ValidationException: Runtime Error
  Syntax error near line 16
  ------------------------------
  13 |       - name: Id
  14 |         tests:
  15 |         tags: [dim_type_tests]
  16 |           - unique
  
  Raw Error:
  ------------------------------
  while parsing a block mapping
    in "<unicode string>", line 13, column 9:
            - name: Id
              ^
  expected <block end>, but found '<block sequence start>'
    in "<unicode string>", line 16, column 11:
                - unique
                ^

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 336, in load_all
    loader.load(internal_manifest=internal_manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 208, in load
    self.parse_project(project, macro_manifest, old_results)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 182, in parse_project
    self.parse_with_cache(path, parser, old_results)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 138, in parse_with_cache
    parser.parse_file(block)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 283, in parse_file
    dct = self._yaml_from_file(block.file)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/schemas.py", line 168, in _yaml_from_file
    .format(self.project.project_name, path, reason)
dbt.exceptions.CompilationException: Compilation Error
  Error reading dbt_: traffic_models/schema.yml - Runtime Error
    Syntax error near line 16
    ------------------------------
    13 |       - name: Id
    14 |         tests:
    15 |         tags: [dim_type_tests]
    16 |           - unique
    
    Raw Error:
    ------------------------------
    while parsing a block mapping
      in "<unicode string>", line 13, column 9:
              - name: Id
                ^
    expected <block end>, but found '<block sequence start>'
      in "<unicode string>", line 16, column 11:
                  - unique
                  ^

2022-07-28 20:41:45.656994 (MainThread): Running with dbt=0.16.1
2022-07-28 20:41:46.009741 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:41:46.034064 (MainThread): Tracking: tracking
2022-07-28 20:41:46.084984 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284cabd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284e0c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd7284ea0d0>]}
2022-07-28 20:41:46.177682 (MainThread): Partial parsing not enabled
2022-07-28 20:41:46.205485 (MainThread): Parsing macros/core.sql
2022-07-28 20:41:46.226980 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:41:46.271583 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:41:46.280818 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:41:46.421216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:41:46.503771 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:41:46.533632 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:41:46.543178 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:41:46.665321 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:41:46.722778 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:41:46.746179 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:41:46.775790 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:41:46.809712 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:41:47.081901 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:41:47.087771 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:41:47.098131 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:41:47.115057 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:41:47.127505 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:41:47.139806 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:41:47.145714 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:41:47.210184 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:41:47.216984 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:41:47.239173 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:41:47.251911 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:41:47.265292 (MainThread): Parsing macros/relations.sql
2022-07-28 20:41:47.281291 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:41:47.384937 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:41:47.399602 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:41:47.520034 (MainThread): Partial parsing not enabled
2022-07-28 20:41:47.738578 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:47.739218 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:47.821483 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:47.822138 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:47.848863 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:47.849484 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:48.785420 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:41:50.043143 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:41:50.085481 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:41:50.086693 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:41:50.104466 (MainThread): 
2022-07-28 20:41:50.108434 (MainThread): Acquiring new postgres connection "master".
2022-07-28 20:41:50.109056 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:41:50.353689 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 20:41:50.355896 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 20:41:51.210473 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 20:41:51.211107 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 20:41:51.233347 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 20:41:51.349621 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.350070 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 20:41:51.359164 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.359893 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 20:41:51.361229 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:51.361834 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:41:51.362276 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 20:41:51.999449 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.64 seconds
2022-07-28 20:41:52.015834 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 20:41:52.081369 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.082029 (MainThread): On master: BEGIN
2022-07-28 20:41:52.093168 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 20:41:52.093828 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.094289 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 20:41:52.728452 (MainThread): SQL status: SELECT 0 in 0.63 seconds
2022-07-28 20:41:52.732507 (MainThread): On master: ROLLBACK
2022-07-28 20:41:52.733886 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.734484 (MainThread): On master: BEGIN
2022-07-28 20:41:52.735921 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:52.737319 (MainThread): On master: COMMIT
2022-07-28 20:41:52.743006 (MainThread): Using postgres connection "master".
2022-07-28 20:41:52.743495 (MainThread): On master: COMMIT
2022-07-28 20:41:52.744278 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:41:52.748574 (MainThread): 20:41:52 | Concurrency: 1 threads (target='dev')
2022-07-28 20:41:52.749293 (MainThread): 20:41:52 | 
2022-07-28 20:41:52.792873 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 20:41:52.793577 (Thread-1): 20:41:52 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 20:41:52.795308 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.795988 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 20:41:52.796352 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 20:41:52.863557 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 20:41:52.864596 (Thread-1): finished collecting timing info
2022-07-28 20:41:52.950738 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.951452 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 20:41:52.953008 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:52.961806 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:52.962467 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:41:52.963648 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:53.044248 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 20:41:53.045670 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.046387 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 20:41:53.048561 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:53.049236 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.049671 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 20:41:53.737208 (Thread-1): SQL status: SELECT 6 in 0.69 seconds
2022-07-28 20:41:53.756382 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.756900 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 20:41:53.804841 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2022-07-28 20:41:53.814403 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.814841 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 20:41:53.816356 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:53.823928 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 20:41:53.824802 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.825497 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 20:41:53.852750 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 20:41:53.859812 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:41:53.860326 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:41:53.887015 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-07-28 20:41:53.903996 (Thread-1): finished collecting timing info
2022-07-28 20:41:53.906606 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725c88390>]}
2022-07-28 20:41:53.907982 (Thread-1): 20:41:53 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.11s]
2022-07-28 20:41:53.908648 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 20:41:53.910630 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 20:41:53.911517 (Thread-1): 20:41:53 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 20:41:53.913195 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.913787 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 20:41:53.914245 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 20:41:53.941946 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 20:41:53.943165 (Thread-1): finished collecting timing info
2022-07-28 20:41:53.984465 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.984936 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 20:41:53.986057 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:53.995114 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:53.995854 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 20:41:53.997045 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.002967 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 20:41:54.004452 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.005099 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 20:41:54.006359 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:54.006985 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.007484 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 20:41:54.229349 (Thread-1): SQL status: SELECT 922 in 0.22 seconds
2022-07-28 20:41:54.245678 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.246354 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 20:41:54.247926 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:54.257624 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.258311 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 20:41:54.260228 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:41:54.263728 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 20:41:54.264357 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.264856 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 20:41:54.296627 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 20:41:54.303663 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 20:41:54.304545 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 20:41:54.341959 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 20:41:54.351688 (Thread-1): finished collecting timing info
2022-07-28 20:41:54.354656 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725bc5d90>]}
2022-07-28 20:41:54.355992 (Thread-1): 20:41:54 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.44s]
2022-07-28 20:41:54.357152 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 20:41:54.358815 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 20:41:54.360351 (Thread-1): 20:41:54 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 20:41:54.362542 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.363168 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 20:41:54.363734 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 20:41:54.399043 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 20:41:54.400365 (Thread-1): finished collecting timing info
2022-07-28 20:41:54.428853 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.429552 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 20:41:54.431230 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.440288 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.440959 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 20:41:54.442141 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:41:54.448458 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 20:41:54.449986 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.450613 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 20:41:54.451851 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:41:54.452778 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:41:54.453286 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 20:42:01.794009 (Thread-1): SQL status: SELECT 922 in 7.34 seconds
2022-07-28 20:42:01.820403 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.823880 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 20:42:01.826146 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 20:42:01.850453 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.850915 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 20:42:01.859652 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 20:42:01.862735 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 20:42:01.863171 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.863517 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 20:42:01.883048 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 20:42:01.889914 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:01.890374 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 20:42:01.922128 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-07-28 20:42:01.936644 (Thread-1): finished collecting timing info
2022-07-28 20:42:01.945351 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0ec9ac10-d481-4a25-a666-45e4018bb38f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725c88390>]}
2022-07-28 20:42:01.956059 (Thread-1): 20:42:01 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 7.58s]
2022-07-28 20:42:01.957736 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 20:42:02.051267 (MainThread): Using postgres connection "master".
2022-07-28 20:42:02.052254 (MainThread): On master: BEGIN
2022-07-28 20:42:02.059742 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:42:02.060738 (MainThread): On master: COMMIT
2022-07-28 20:42:02.061208 (MainThread): Using postgres connection "master".
2022-07-28 20:42:02.062376 (MainThread): On master: COMMIT
2022-07-28 20:42:02.067521 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:42:02.068931 (MainThread): 20:42:02 | 
2022-07-28 20:42:02.070353 (MainThread): 20:42:02 | Finished running 3 table models in 11.96s.
2022-07-28 20:42:02.071566 (MainThread): Connection 'master' was left open.
2022-07-28 20:42:02.072128 (MainThread): On master: Close
2022-07-28 20:42:02.072800 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 20:42:02.076477 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 20:42:02.135852 (MainThread): 
2022-07-28 20:42:02.137046 (MainThread): Completed successfully
2022-07-28 20:42:02.138076 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 20:42:02.139490 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd717cc7b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725d16a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd725dd2710>]}
2022-07-28 20:42:02.140389 (MainThread): Flushing usage events
2022-07-28 20:42:21.257344 (MainThread): Running with dbt=0.16.1
2022-07-28 20:42:21.562674 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 20:42:21.564201 (MainThread): Tracking: tracking
2022-07-28 20:42:21.584065 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b04b6550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b04aff10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb3b05a9fd0>]}
2022-07-28 20:42:21.707763 (MainThread): Partial parsing not enabled
2022-07-28 20:42:21.713974 (MainThread): Parsing macros/core.sql
2022-07-28 20:42:21.736129 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:42:21.826841 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:42:21.846368 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:42:22.177223 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:42:22.397539 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:42:22.504308 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:42:22.522352 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:42:22.668052 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:42:22.776733 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:42:22.824127 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:42:22.868399 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:42:22.916921 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:42:23.233480 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:42:23.240880 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:42:23.250382 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:42:23.264164 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:42:23.268901 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:42:23.281069 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:42:23.286181 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:42:23.348857 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:42:23.353548 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:42:23.363897 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:42:23.371218 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:42:23.395755 (MainThread): Parsing macros/relations.sql
2022-07-28 20:42:23.421559 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:42:23.545676 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:42:23.562697 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:42:23.734272 (MainThread): Partial parsing not enabled
2022-07-28 20:42:23.956861 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:42:23.957331 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:24.081159 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:42:24.081635 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:24.137312 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:42:24.137797 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:42:25.351112 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:42:26.787772 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:42:26.808055 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:42:26.809141 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:42:26.828670 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-07-28 20:42:26.829457 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc6cb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc74dd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb39fc74d50>]}
2022-07-28 20:42:26.830145 (MainThread): Flushing usage events
2022-07-28 20:42:27.769757 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 20:54:30.265180 (MainThread): Running with dbt=0.16.1
2022-07-28 20:54:30.635886 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 20:54:30.649124 (MainThread): Tracking: tracking
2022-07-28 20:54:30.667911 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc63bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc70f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e4bc70a10>]}
2022-07-28 20:54:30.776906 (MainThread): Partial parsing not enabled
2022-07-28 20:54:30.799910 (MainThread): Parsing macros/core.sql
2022-07-28 20:54:30.821320 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 20:54:30.857304 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 20:54:30.865937 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 20:54:31.022216 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 20:54:31.113941 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 20:54:31.196880 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 20:54:31.212002 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 20:54:31.378401 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 20:54:31.436662 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 20:54:31.467285 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 20:54:31.495926 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 20:54:31.526183 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 20:54:31.708497 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 20:54:31.713457 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 20:54:31.722648 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 20:54:31.732798 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 20:54:31.737321 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 20:54:31.745090 (MainThread): Parsing macros/etc/query.sql
2022-07-28 20:54:31.750213 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 20:54:31.791953 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 20:54:31.796506 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 20:54:31.805776 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 20:54:31.810954 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 20:54:31.840529 (MainThread): Parsing macros/relations.sql
2022-07-28 20:54:31.849956 (MainThread): Parsing macros/adapters.sql
2022-07-28 20:54:31.922408 (MainThread): Parsing macros/catalog.sql
2022-07-28 20:54:31.933017 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 20:54:32.031994 (MainThread): Partial parsing not enabled
2022-07-28 20:54:32.169145 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:32.169574 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.243595 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 20:54:32.244041 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.281924 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 20:54:32.282672 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:32.955152 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 20:54:34.081571 (MainThread): scipy not found, skipping conversion test.
2022-07-28 20:54:34.090945 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 20:54:34.092158 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 20:54:34.101144 (MainThread): 
2022-07-28 20:54:34.102676 (MainThread): Acquiring new postgres connection "master".
2022-07-28 20:54:34.103283 (MainThread): Opening a new connection, currently in state init
2022-07-28 20:54:34.166035 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 20:54:34.166555 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 20:54:34.829832 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 20:54:34.831648 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 20:54:34.853990 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 20:54:35.067091 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.068397 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 20:54:35.078032 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.078683 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 20:54:35.081756 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:35.084526 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 20:54:35.085662 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 20:54:35.098456 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 20:54:35.125080 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 20:54:35.338895 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.348993 (MainThread): On master: BEGIN
2022-07-28 20:54:35.385896 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-07-28 20:54:35.386381 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.386671 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 20:54:35.415369 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-28 20:54:35.430402 (MainThread): On master: ROLLBACK
2022-07-28 20:54:35.448186 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.448691 (MainThread): On master: BEGIN
2022-07-28 20:54:35.451615 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:35.452163 (MainThread): On master: COMMIT
2022-07-28 20:54:35.463823 (MainThread): Using postgres connection "master".
2022-07-28 20:54:35.464243 (MainThread): On master: COMMIT
2022-07-28 20:54:35.467646 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:54:35.468818 (MainThread): 20:54:35 | Concurrency: 1 threads (target='dev')
2022-07-28 20:54:35.474003 (MainThread): 20:54:35 | 
2022-07-28 20:54:35.590344 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 20:54:35.591097 (Thread-1): 20:54:35 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 20:54:35.592629 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.593044 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 20:54:35.593384 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 20:54:35.757621 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 20:54:35.758856 (Thread-1): finished collecting timing info
2022-07-28 20:54:35.932540 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.933412 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 20:54:35.934921 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:54:35.947391 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:35.948027 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 20:54:35.949170 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 20:54:36.038926 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 20:54:36.040341 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:36.041059 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 20:54:36.043192 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:36.044669 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 20:54:36.045320 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from "warehouse"."traffic_source"."source"
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 20:54:36.073574 (Thread-1): Postgres error: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^

2022-07-28 20:54:36.074249 (Thread-1): On model.dbt_.dim_types: ROLLBACK
2022-07-28 20:54:36.075833 (Thread-1): finished collecting timing info
2022-07-28 20:54:36.077860 (Thread-1): Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "traffic_source.source" does not exist
LINE 8:     select * from "warehouse"."traffic_source"."source"
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "traffic_source.source" does not exist
  LINE 8:     select * from "warehouse"."traffic_source"."source"
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-28 20:54:36.684079 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9327968d-4386-49c3-a1b3-75026fde49d3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e3b08a3d0>]}
2022-07-28 20:54:36.685090 (Thread-1): 20:54:36 | 1 of 3 ERROR creating table model warehouse.dim_types................ [ERROR in 1.09s]
2022-07-28 20:54:36.686308 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 20:54:36.688056 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 20:54:36.688808 (Thread-1): 20:54:36 | 2 of 3 SKIP relation warehouse.fct_summary........................... [SKIP]
2022-07-28 20:54:36.689838 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 20:54:36.690528 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 20:54:36.691313 (Thread-1): 20:54:36 | 3 of 3 SKIP relation warehouse.fct_trajectory........................ [SKIP]
2022-07-28 20:54:36.692473 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 20:54:36.705310 (MainThread): Using postgres connection "master".
2022-07-28 20:54:36.705782 (MainThread): On master: BEGIN
2022-07-28 20:54:36.706535 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 20:54:36.706983 (MainThread): On master: COMMIT
2022-07-28 20:54:36.707276 (MainThread): Using postgres connection "master".
2022-07-28 20:54:36.707661 (MainThread): On master: COMMIT
2022-07-28 20:54:36.708527 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 20:54:36.709751 (MainThread): 20:54:36 | 
2022-07-28 20:54:36.710244 (MainThread): 20:54:36 | Finished running 3 table models in 2.61s.
2022-07-28 20:54:36.710607 (MainThread): Connection 'master' was left open.
2022-07-28 20:54:36.710946 (MainThread): On master: Close
2022-07-28 20:54:36.713226 (MainThread): Connection 'model.dbt_.dim_types' was left open.
2022-07-28 20:54:36.713873 (MainThread): On model.dbt_.dim_types: Close
2022-07-28 20:54:36.786952 (MainThread): 
2022-07-28 20:54:36.788771 (MainThread): Completed with 1 error and 0 warnings:
2022-07-28 20:54:36.789830 (MainThread): 
2022-07-28 20:54:36.790828 (MainThread): Database Error in model dim_types (models/traffic_models/dim_types.sql)
2022-07-28 20:54:36.791842 (MainThread):   relation "traffic_source.source" does not exist
2022-07-28 20:54:36.792752 (MainThread):   LINE 8:     select * from "warehouse"."traffic_source"."source"
2022-07-28 20:54:36.793627 (MainThread):                             ^
2022-07-28 20:54:36.794507 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-28 20:54:36.795475 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2022-07-28 20:54:36.797629 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493f3310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493ceed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9e493abb10>]}
2022-07-28 20:54:36.799197 (MainThread): Flushing usage events
2022-07-28 21:02:06.366377 (MainThread): Running with dbt=0.16.1
2022-07-28 21:02:06.716212 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:02:06.771197 (MainThread): Tracking: tracking
2022-07-28 21:02:06.818772 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337be1850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337bd82d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337b14610>]}
2022-07-28 21:02:06.930306 (MainThread): Partial parsing not enabled
2022-07-28 21:02:06.959562 (MainThread): Parsing macros/core.sql
2022-07-28 21:02:07.005376 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:02:07.041421 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:02:07.050458 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:02:07.206286 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:02:07.286428 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:02:07.316773 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:02:07.326665 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:02:07.420638 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:02:07.476588 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:02:07.500620 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:02:07.538434 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:02:07.569996 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:02:07.762412 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:02:07.768244 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:02:07.778830 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:02:07.789913 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:02:07.795455 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:02:07.803898 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:02:07.819841 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:02:07.927710 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:02:07.934286 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:02:07.944495 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:02:07.950237 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:02:07.957784 (MainThread): Parsing macros/relations.sql
2022-07-28 21:02:07.965414 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:02:08.035397 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:02:08.046200 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:02:08.167525 (MainThread): Partial parsing not enabled
2022-07-28 21:02:08.343845 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:08.344436 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:08.452752 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:08.453344 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:08.479670 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:08.480264 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:09.165987 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:02:10.309421 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:02:10.324366 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:02:10.325714 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:02:10.335496 (MainThread): 
2022-07-28 21:02:10.337247 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:02:10.337850 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:10.411907 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:02:10.413113 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:02:10.910160 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:02:10.910803 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:02:10.960756 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.05 seconds
2022-07-28 21:02:11.042444 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.042973 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:02:11.054700 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.055144 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:02:11.057776 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.060785 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:02:11.061868 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:02:11.070087 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:02:11.090162 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:02:11.152750 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.153415 (MainThread): On master: BEGIN
2022-07-28 21:02:11.166372 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:02:11.166849 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.167134 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:02:11.191698 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:02:11.195811 (MainThread): On master: ROLLBACK
2022-07-28 21:02:11.197186 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.197680 (MainThread): On master: BEGIN
2022-07-28 21:02:11.199184 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.199709 (MainThread): On master: COMMIT
2022-07-28 21:02:11.200018 (MainThread): Using postgres connection "master".
2022-07-28 21:02:11.200289 (MainThread): On master: COMMIT
2022-07-28 21:02:11.200995 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:02:11.202012 (MainThread): 21:02:11 | Concurrency: 1 threads (target='dev')
2022-07-28 21:02:11.202502 (MainThread): 21:02:11 | 
2022-07-28 21:02:11.241728 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:02:11.242439 (Thread-1): 21:02:11 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:02:11.243428 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.243812 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:02:11.244142 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:02:11.319269 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:02:11.320359 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.399187 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.399810 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:02:11.401737 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.410106 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.410563 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:02:11.411680 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.477382 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:02:11.478619 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.479148 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:02:11.481864 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.482352 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.482639 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:02:11.572626 (Thread-1): SQL status: SELECT 6 in 0.09 seconds
2022-07-28 21:02:11.587136 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.587699 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:02:11.590840 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.600017 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.600465 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:02:11.601850 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.605117 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:02:11.605561 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.605840 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:02:11.636023 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 21:02:11.642766 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:11.643228 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:02:11.658345 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:02:11.667477 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.670049 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63351f5c50>]}
2022-07-28 21:02:11.670919 (Thread-1): 21:02:11 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.43s]
2022-07-28 21:02:11.671451 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:02:11.673066 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:02:11.673801 (Thread-1): 21:02:11 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:02:11.675256 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.675756 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:02:11.676064 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:02:11.711690 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:02:11.713043 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.744658 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.745144 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:02:11.746209 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.754627 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.755275 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:02:11.756361 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:11.761657 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:02:11.762885 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.763478 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:02:11.764251 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:11.764676 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.764960 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:02:11.846148 (Thread-1): SQL status: SELECT 922 in 0.08 seconds
2022-07-28 21:02:11.866355 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.866821 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:02:11.879444 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 21:02:11.894249 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.894726 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:02:11.899640 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:11.902760 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:02:11.903214 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.903576 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:02:11.924564 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:02:11.931499 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:11.935670 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:02:11.948015 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:02:11.959563 (Thread-1): finished collecting timing info
2022-07-28 21:02:11.962457 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6337ab6950>]}
2022-07-28 21:02:11.963606 (Thread-1): 21:02:11 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.29s]
2022-07-28 21:02:11.964721 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:02:11.965447 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:02:11.967045 (Thread-1): 21:02:11 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:02:11.968969 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:11.969570 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:02:11.970479 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:02:11.999188 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:02:12.000474 (Thread-1): finished collecting timing info
2022-07-28 21:02:12.026444 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.027153 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:02:12.029027 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:12.037979 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.038654 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:02:12.040563 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:02:12.046782 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:02:12.048300 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.049223 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:02:12.050717 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:12.051544 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:12.052078 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from "source"
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:02:23.537423 (Thread-1): SQL status: SELECT 922 in 11.48 seconds
2022-07-28 21:02:23.566920 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.567657 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:02:23.569175 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:23.580824 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.581477 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:02:23.582949 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:02:23.586429 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:02:23.587107 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.587609 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:02:23.635495 (Thread-1): SQL status: COMMIT in 0.05 seconds
2022-07-28 21:02:23.646982 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:23.647721 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:02:23.819708 (Thread-1): SQL status: DROP TABLE in 0.17 seconds
2022-07-28 21:02:23.843671 (Thread-1): finished collecting timing info
2022-07-28 21:02:23.851957 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a311e44b-b0a4-4574-88a8-3fb7c9840a22', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335329ad0>]}
2022-07-28 21:02:23.852829 (Thread-1): 21:02:23 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 11.88s]
2022-07-28 21:02:23.853927 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:02:23.941744 (MainThread): Using postgres connection "master".
2022-07-28 21:02:23.942217 (MainThread): On master: BEGIN
2022-07-28 21:02:23.943015 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:02:23.943511 (MainThread): On master: COMMIT
2022-07-28 21:02:23.943808 (MainThread): Using postgres connection "master".
2022-07-28 21:02:23.944070 (MainThread): On master: COMMIT
2022-07-28 21:02:23.945004 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:02:23.946182 (MainThread): 21:02:23 | 
2022-07-28 21:02:23.948730 (MainThread): 21:02:23 | Finished running 3 table models in 13.61s.
2022-07-28 21:02:23.953844 (MainThread): Connection 'master' was left open.
2022-07-28 21:02:23.955462 (MainThread): On master: Close
2022-07-28 21:02:23.965882 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:02:23.966832 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:02:24.047708 (MainThread): 
2022-07-28 21:02:24.048869 (MainThread): Completed successfully
2022-07-28 21:02:24.049898 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:02:24.051543 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335398b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f633529c110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6335388190>]}
2022-07-28 21:02:24.052604 (MainThread): Flushing usage events
2022-07-28 21:02:40.055489 (MainThread): Running with dbt=0.16.1
2022-07-28 21:02:40.327589 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 21:02:40.328805 (MainThread): Tracking: tracking
2022-07-28 21:02:40.355054 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61a214e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61cfabb10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa61a20d110>]}
2022-07-28 21:02:40.440838 (MainThread): Partial parsing not enabled
2022-07-28 21:02:40.446779 (MainThread): Parsing macros/core.sql
2022-07-28 21:02:40.468057 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:02:40.504969 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:02:40.513345 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:02:40.646557 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:02:40.748239 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:02:40.776804 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:02:40.785719 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:02:40.883233 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:02:40.957293 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:02:40.979957 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:02:41.008275 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:02:41.074809 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:02:41.267842 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:02:41.272890 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:02:41.282232 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:02:41.293042 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:02:41.297603 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:02:41.305660 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:02:41.310920 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:02:41.351657 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:02:41.356083 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:02:41.365472 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:02:41.370576 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:02:41.376848 (MainThread): Parsing macros/relations.sql
2022-07-28 21:02:41.386335 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:02:41.455581 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:02:41.466119 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:02:41.572649 (MainThread): Partial parsing not enabled
2022-07-28 21:02:41.734103 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:02:41.734733 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:41.814608 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:02:41.815255 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:41.842760 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:02:41.843410 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:02:42.521387 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:02:43.447699 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:02:43.457278 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:02:43.458550 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:02:43.467311 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-07-28 21:02:43.468571 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097ae410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097ae510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa6097b77d0>]}
2022-07-28 21:02:43.469443 (MainThread): Flushing usage events
2022-07-28 21:02:44.368954 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:08:32.454563 (MainThread): Running with dbt=0.16.1
2022-07-28 21:08:33.562640 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:08:33.617636 (MainThread): Tracking: tracking
2022-07-28 21:08:33.803057 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aac563990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aaac2c250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aaac33390>]}
2022-07-28 21:08:34.062511 (MainThread): Partial parsing not enabled
2022-07-28 21:08:34.144508 (MainThread): Parsing macros/core.sql
2022-07-28 21:08:34.247916 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:08:34.326751 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:08:34.352600 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:08:34.828219 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:08:35.140105 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:08:35.173819 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:08:35.183695 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:08:35.367886 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:08:35.430863 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:08:35.454523 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:08:35.483564 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:08:35.513670 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:08:35.851841 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:08:35.857323 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:08:35.867254 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:08:35.878164 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:08:35.882984 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:08:35.891420 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:08:35.897109 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:08:35.968302 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:08:35.975898 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:08:35.985696 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:08:35.991504 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:08:36.021834 (MainThread): Parsing macros/relations.sql
2022-07-28 21:08:36.032904 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:08:36.116839 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:08:36.127660 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:08:36.256712 (MainThread): Partial parsing not enabled
2022-07-28 21:08:36.777634 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:36.778066 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:36.993125 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:36.993745 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:37.083531 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:37.083986 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:37.983624 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:08:39.110893 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:08:39.124410 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:08:39.125610 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:08:39.135434 (MainThread): 
2022-07-28 21:08:39.136586 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:08:39.136982 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:08:39.246201 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:08:39.246896 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:08:39.744071 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:08:39.744728 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:08:39.761213 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-28 21:08:39.845981 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.846636 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:08:39.852543 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.852996 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:08:39.854431 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:39.854904 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:08:39.855190 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:08:39.863527 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:08:39.879583 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:08:39.945034 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.945485 (MainThread): On master: BEGIN
2022-07-28 21:08:39.957507 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:08:39.958040 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.958355 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:08:39.978181 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:08:39.982949 (MainThread): On master: ROLLBACK
2022-07-28 21:08:39.984088 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.984541 (MainThread): On master: BEGIN
2022-07-28 21:08:39.985991 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:39.986523 (MainThread): On master: COMMIT
2022-07-28 21:08:39.986834 (MainThread): Using postgres connection "master".
2022-07-28 21:08:39.987103 (MainThread): On master: COMMIT
2022-07-28 21:08:39.987859 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:08:39.989168 (MainThread): 21:08:39 | Concurrency: 1 threads (target='dev')
2022-07-28 21:08:39.989695 (MainThread): 21:08:39 | 
2022-07-28 21:08:40.031700 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:08:40.032384 (Thread-1): 21:08:40 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:08:40.033382 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.033744 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:08:40.034060 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:08:40.108003 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:08:40.117092 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.216823 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.217319 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:08:40.218693 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.228213 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.228660 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:08:40.229734 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.296737 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:08:40.297917 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.298377 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:08:40.299439 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.299897 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.300175 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:08:40.379417 (Thread-1): SQL status: SELECT 6 in 0.08 seconds
2022-07-28 21:08:40.393824 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.394264 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:08:40.395690 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.405123 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.405569 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:08:40.406829 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.410083 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:08:40.410527 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.410805 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:08:40.429692 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:08:40.437071 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:08:40.437553 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:08:40.452523 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 21:08:40.463111 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.471164 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa8277910>]}
2022-07-28 21:08:40.472145 (Thread-1): 21:08:40 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.43s]
2022-07-28 21:08:40.472626 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:08:40.473729 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:08:40.474280 (Thread-1): 21:08:40 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:08:40.475628 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.476020 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:08:40.476315 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:08:40.515530 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:08:40.516573 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.543221 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.543761 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:08:40.545077 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.567098 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.568868 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:08:40.571734 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.578228 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:08:40.579946 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.580385 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:08:40.581121 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.581544 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.582534 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:08:40.662469 (Thread-1): SQL status: SELECT 922 in 0.08 seconds
2022-07-28 21:08:40.677600 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.678049 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:08:40.679502 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.688460 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.688908 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:08:40.690235 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:40.693859 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:08:40.694308 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.694586 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:08:40.708743 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 21:08:40.715871 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:08:40.716318 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:08:40.732239 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 21:08:40.742323 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.744766 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa829cfd0>]}
2022-07-28 21:08:40.745606 (Thread-1): 21:08:40 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.27s]
2022-07-28 21:08:40.746060 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:08:40.746629 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:08:40.747410 (Thread-1): 21:08:40 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:08:40.748691 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.749069 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:08:40.749365 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:08:40.784354 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:08:40.785410 (Thread-1): finished collecting timing info
2022-07-28 21:08:40.812471 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.812946 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:08:40.814037 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.831247 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.831764 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:08:40.832820 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:08:40.838304 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:08:40.839845 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.840280 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:08:40.841031 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:40.841448 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:40.841726 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:08:41.817798 (Thread-1): SQL status: SELECT 922 in 0.98 seconds
2022-07-28 21:08:41.833710 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.834160 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:08:41.835593 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:41.844951 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.845411 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:08:41.846728 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:08:41.850150 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:08:41.850597 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.850883 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:08:41.910796 (Thread-1): SQL status: COMMIT in 0.06 seconds
2022-07-28 21:08:41.917620 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:08:41.918114 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:08:41.955096 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 21:08:41.964534 (Thread-1): finished collecting timing info
2022-07-28 21:08:41.967916 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'da4e3e9d-5a3a-43e4-80d5-d3d4c8808a47', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa83f9e50>]}
2022-07-28 21:08:41.968784 (Thread-1): 21:08:41 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.22s]
2022-07-28 21:08:41.969242 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:08:42.069905 (MainThread): Using postgres connection "master".
2022-07-28 21:08:42.071076 (MainThread): On master: BEGIN
2022-07-28 21:08:42.073520 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:08:42.074730 (MainThread): On master: COMMIT
2022-07-28 21:08:42.076357 (MainThread): Using postgres connection "master".
2022-07-28 21:08:42.076685 (MainThread): On master: COMMIT
2022-07-28 21:08:42.077739 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:08:42.078992 (MainThread): 21:08:42 | 
2022-07-28 21:08:42.079594 (MainThread): 21:08:42 | Finished running 3 table models in 2.94s.
2022-07-28 21:08:42.079987 (MainThread): Connection 'master' was left open.
2022-07-28 21:08:42.080281 (MainThread): On master: Close
2022-07-28 21:08:42.086552 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:08:42.087038 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:08:42.149643 (MainThread): 
2022-07-28 21:08:42.151950 (MainThread): Completed successfully
2022-07-28 21:08:42.152482 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:08:42.153085 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4a9a04b350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa83b7790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4aa831a890>]}
2022-07-28 21:08:42.160856 (MainThread): Flushing usage events
2022-07-28 21:08:58.638216 (MainThread): Running with dbt=0.16.1
2022-07-28 21:08:58.902426 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 21:08:58.903687 (MainThread): Tracking: tracking
2022-07-28 21:08:58.927798 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558ef8d690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558f0d4410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558efc1290>]}
2022-07-28 21:08:59.018592 (MainThread): Partial parsing not enabled
2022-07-28 21:08:59.024735 (MainThread): Parsing macros/core.sql
2022-07-28 21:08:59.045579 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:08:59.080100 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:08:59.088161 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:08:59.219569 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:08:59.302332 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:08:59.330551 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:08:59.339577 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:08:59.443109 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:08:59.522748 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:08:59.572887 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:08:59.605065 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:08:59.634598 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:08:59.814984 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:08:59.820077 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:08:59.829304 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:08:59.839481 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:08:59.843972 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:08:59.851944 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:08:59.857080 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:08:59.896976 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:08:59.901850 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:08:59.916750 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:08:59.922427 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:08:59.929108 (MainThread): Parsing macros/relations.sql
2022-07-28 21:08:59.936044 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:09:00.019814 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:09:00.030796 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:09:00.122311 (MainThread): Partial parsing not enabled
2022-07-28 21:09:00.261992 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:09:00.262438 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:00.335481 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:09:00.335926 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:00.366593 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:09:00.367019 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:01.009566 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:09:02.291637 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:09:02.314423 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:09:02.323232 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:09:02.345732 (MainThread): 
2022-07-28 21:09:02.346416 (MainThread): 21:09:02 | Concurrency: 1 threads (target='dev')
2022-07-28 21:09:02.346813 (MainThread): 21:09:02 | 
2022-07-28 21:09:02.396025 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:09:02.397303 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:09:02.403609 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.404260 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:09:02.540942 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:09:02.542021 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.543423 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.545047 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:09:02.546548 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:09:02.548166 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:09:02.548822 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.549284 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:09:02.698337 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:09:02.699433 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.700442 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.701968 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:09:02.702531 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:09:02.708990 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:09:02.709432 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:09:02.709736 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:09:02.744799 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:09:02.745995 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.747223 (Thread-1): finished collecting timing info
2022-07-28 21:09:02.749032 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:09:02.781824 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:09:02.782335 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:09:02.818180 (MainThread): 21:09:02 | Done.
2022-07-28 21:09:03.009471 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 21:09:03.010172 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:09:03.011471 (MainThread): 21:09:03 | Building catalog
2022-07-28 21:09:03.250533 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 21:09:03.252844 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:09:04.042102 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.042576 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:09:04.068189 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.03 seconds
2022-07-28 21:09:04.068734 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.069033 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:09:04.495177 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.43 seconds
2022-07-28 21:09:04.610562 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:09:04.649804 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.650271 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:09:04.654572 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:09:04.655066 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:09:04.655447 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:09:04.658686 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 21:09:04.665607 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:09:05.166273 (MainThread): 21:09:05 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 21:09:05.169699 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558c70cdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f55814538d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f558c70c7d0>]}
2022-07-28 21:09:05.170443 (MainThread): Flushing usage events
2022-07-28 21:09:06.055242 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 21:09:06.057769 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 21:09:06.058258 (MainThread): On warehouse.information_schema: Close
2022-07-28 21:09:14.126461 (MainThread): Running with dbt=0.16.1
2022-07-28 21:09:14.915677 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=8080, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 21:09:14.916906 (MainThread): Tracking: tracking
2022-07-28 21:09:14.957003 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549c5c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549c0cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd549e5510>]}
2022-07-28 21:09:14.977570 (MainThread): Serving docs at 0.0.0.0:8080
2022-07-28 21:09:14.978605 (MainThread): To access from your browser, navigate to:  http://localhost:8080
2022-07-28 21:09:14.979225 (MainThread): Press Ctrl+C to exit.


2022-07-28 21:09:14.983837 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54aa80d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54ab8710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fdd54ab8e50>]}
2022-07-28 21:09:14.985121 (MainThread): Flushing usage events
2022-07-28 21:09:15.912719 (MainThread): Encountered an error:
2022-07-28 21:09:15.914453 (MainThread): [Errno 98] Address already in use
2022-07-28 21:09:15.959532 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/serve.py", line 31, in run
    SimpleHTTPRequestHandler  # type: ignore
  File "/usr/local/lib/python3.7/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/usr/local/lib/python3.7/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use

2022-07-28 21:25:19.703695 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:20.033141 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 21:25:20.097706 (MainThread): Tracking: tracking
2022-07-28 21:25:20.157754 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786974a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7869804250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f78697a3e90>]}
2022-07-28 21:25:20.264712 (MainThread): Partial parsing not enabled
2022-07-28 21:25:20.320058 (MainThread): Parsing macros/core.sql
2022-07-28 21:25:20.341931 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:25:20.378518 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:25:20.387712 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:25:20.519641 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:25:20.599742 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:25:20.630074 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:25:20.639693 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:25:20.736683 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:25:20.827797 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:25:20.851892 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:25:20.880736 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:25:20.968318 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:25:21.528681 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:25:21.543410 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:25:21.574256 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:25:21.601904 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:25:21.622223 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:25:21.762952 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:25:21.795493 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:25:21.917429 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:25:21.932702 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:25:21.964052 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:25:21.978343 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:25:22.076416 (MainThread): Parsing macros/relations.sql
2022-07-28 21:25:22.096148 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:25:22.322848 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:25:22.362939 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:25:22.704615 (MainThread): Partial parsing not enabled
2022-07-28 21:25:23.311061 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:23.312878 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:23.431488 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:23.432496 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:23.481906 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:23.482940 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:25.115741 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:25:27.947018 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:25:27.999674 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:25:28.003187 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:25:28.038510 (MainThread): 
2022-07-28 21:25:28.040508 (MainThread): Acquiring new postgres connection "master".
2022-07-28 21:25:28.041157 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:28.207719 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 21:25:28.213440 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:25:29.091647 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 21:25:29.092299 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 21:25:29.132231 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.04 seconds
2022-07-28 21:25:29.227619 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.228047 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 21:25:29.236494 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.237185 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 21:25:29.238368 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.238836 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 21:25:29.239132 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 21:25:29.247438 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 21:25:29.263056 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 21:25:29.326529 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.326979 (MainThread): On master: BEGIN
2022-07-28 21:25:29.342925 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 21:25:29.343973 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.345141 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 21:25:29.367548 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-28 21:25:29.372201 (MainThread): On master: ROLLBACK
2022-07-28 21:25:29.373144 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.373543 (MainThread): On master: BEGIN
2022-07-28 21:25:29.374836 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.375321 (MainThread): On master: COMMIT
2022-07-28 21:25:29.375944 (MainThread): Using postgres connection "master".
2022-07-28 21:25:29.376247 (MainThread): On master: COMMIT
2022-07-28 21:25:29.376973 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:25:29.378209 (MainThread): 21:25:29 | Concurrency: 1 threads (target='dev')
2022-07-28 21:25:29.378720 (MainThread): 21:25:29 | 
2022-07-28 21:25:29.426580 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:25:29.427280 (Thread-1): 21:25:29 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 21:25:29.428341 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.428750 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 21:25:29.429080 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:25:29.497192 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:29.517117 (Thread-1): finished collecting timing info
2022-07-28 21:25:29.601028 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.601516 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 21:25:29.602863 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:29.610977 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.611509 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:25:29.612510 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:29.678829 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:29.731985 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.732497 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 21:25:29.733490 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:29.734043 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.734335 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 21:25:29.904909 (Thread-1): SQL status: SELECT 6 in 0.17 seconds
2022-07-28 21:25:29.920690 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.921341 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 21:25:29.923224 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:29.934069 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.934753 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 21:25:29.937199 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:29.940832 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:25:29.941458 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:29.942177 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 21:25:30.014988 (Thread-1): SQL status: COMMIT in 0.07 seconds
2022-07-28 21:25:30.027577 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:30.028184 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 21:25:30.070910 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 21:25:30.086978 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.089474 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f786978c250>]}
2022-07-28 21:25:30.090391 (Thread-1): 21:25:30 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.66s]
2022-07-28 21:25:30.091440 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:25:30.092956 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:25:30.093538 (Thread-1): 21:25:30 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 21:25:30.094961 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.095413 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 21:25:30.095742 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:25:30.138622 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:30.140012 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.166547 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.167027 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 21:25:30.168703 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.177468 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.177927 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:25:30.179716 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.185696 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:30.186971 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.187545 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 21:25:30.188886 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:30.189411 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.189702 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:25:30.310984 (Thread-1): SQL status: SELECT 922 in 0.12 seconds
2022-07-28 21:25:30.334207 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.334673 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 21:25:30.337983 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:30.355763 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.356212 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 21:25:30.358630 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:30.365734 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:25:30.366326 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.368642 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 21:25:30.391874 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 21:25:30.399105 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:30.399806 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 21:25:30.415605 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-07-28 21:25:30.424655 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.427587 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fa1390>]}
2022-07-28 21:25:30.428614 (Thread-1): 21:25:30 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.33s]
2022-07-28 21:25:30.429560 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:25:30.430285 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:25:30.430953 (Thread-1): 21:25:30 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 21:25:30.433054 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.433644 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 21:25:30.434098 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:25:30.469035 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:30.470302 (Thread-1): finished collecting timing info
2022-07-28 21:25:30.500598 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.501318 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 21:25:30.503018 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.517361 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.518019 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:25:30.519221 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 21:25:30.524658 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:30.526439 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.527059 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 21:25:30.527978 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:30.528614 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:30.529066 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 21:25:31.842555 (Thread-1): SQL status: SELECT 922 in 1.31 seconds
2022-07-28 21:25:31.864682 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.865704 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 21:25:31.868330 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:31.880465 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.881233 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 21:25:31.882987 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 21:25:31.886976 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:25:31.888230 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:31.888818 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 21:25:32.047934 (Thread-1): SQL status: COMMIT in 0.16 seconds
2022-07-28 21:25:32.081201 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:32.091861 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 21:25:32.253226 (Thread-1): SQL status: DROP TABLE in 0.16 seconds
2022-07-28 21:25:32.267962 (Thread-1): finished collecting timing info
2022-07-28 21:25:32.270361 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fe3b5d0f-1f7d-491d-99e9-ce1526535d2f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fa1390>]}
2022-07-28 21:25:32.271190 (Thread-1): 21:25:32 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.84s]
2022-07-28 21:25:32.275638 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:25:32.296067 (MainThread): Using postgres connection "master".
2022-07-28 21:25:32.296713 (MainThread): On master: BEGIN
2022-07-28 21:25:32.297651 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:32.298291 (MainThread): On master: COMMIT
2022-07-28 21:25:32.298742 (MainThread): Using postgres connection "master".
2022-07-28 21:25:32.299147 (MainThread): On master: COMMIT
2022-07-28 21:25:32.300451 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 21:25:32.301839 (MainThread): 21:25:32 | 
2022-07-28 21:25:32.302970 (MainThread): 21:25:32 | Finished running 3 table models in 4.26s.
2022-07-28 21:25:32.304036 (MainThread): Connection 'master' was left open.
2022-07-28 21:25:32.304568 (MainThread): On master: Close
2022-07-28 21:25:32.310580 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 21:25:32.311258 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 21:25:32.370685 (MainThread): 
2022-07-28 21:25:32.372164 (MainThread): Completed successfully
2022-07-28 21:25:32.373381 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 21:25:32.374831 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866eef750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7866fddb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7869804f50>]}
2022-07-28 21:25:32.380243 (MainThread): Flushing usage events
2022-07-28 21:25:49.135999 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:49.454608 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 21:25:49.456087 (MainThread): Tracking: tracking
2022-07-28 21:25:49.475656 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f909f590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93fa314c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f9045d10>]}
2022-07-28 21:25:49.564025 (MainThread): Partial parsing not enabled
2022-07-28 21:25:49.570571 (MainThread): Parsing macros/core.sql
2022-07-28 21:25:49.592070 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 21:25:49.627323 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 21:25:49.635940 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 21:25:49.764637 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 21:25:49.843860 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 21:25:49.873279 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 21:25:49.882434 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 21:25:49.978255 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 21:25:50.034598 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 21:25:50.057629 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 21:25:50.086011 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 21:25:50.116194 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 21:25:50.350677 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 21:25:50.355737 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 21:25:50.365384 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 21:25:50.376058 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 21:25:50.380877 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 21:25:50.388738 (MainThread): Parsing macros/etc/query.sql
2022-07-28 21:25:50.394210 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 21:25:50.437366 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 21:25:50.450277 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 21:25:50.474963 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 21:25:50.480598 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 21:25:50.487015 (MainThread): Parsing macros/relations.sql
2022-07-28 21:25:50.494733 (MainThread): Parsing macros/adapters.sql
2022-07-28 21:25:50.575523 (MainThread): Parsing macros/catalog.sql
2022-07-28 21:25:50.586539 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 21:25:50.685498 (MainThread): Partial parsing not enabled
2022-07-28 21:25:50.827249 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:50.827780 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:50.909269 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:50.909718 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:50.939607 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:50.940060 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:51.655915 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 21:25:52.542091 (MainThread): scipy not found, skipping conversion test.
2022-07-28 21:25:52.551922 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 21:25:52.553239 (MainThread): Found 3 models, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 21:25:52.562948 (MainThread): 
2022-07-28 21:25:52.564370 (MainThread): 21:25:52 | Concurrency: 1 threads (target='dev')
2022-07-28 21:25:52.565461 (MainThread): 21:25:52 | 
2022-07-28 21:25:52.584428 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 21:25:52.585925 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 21:25:52.586570 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.587067 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 21:25:52.656958 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 21:25:52.658280 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.659672 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.661493 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 21:25:52.662863 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 21:25:52.664183 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 21:25:52.664804 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.665280 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 21:25:52.805505 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 21:25:52.806766 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.808184 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.809952 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 21:25:52.810739 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 21:25:52.812178 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 21:25:52.812813 (Thread-1): Opening a new connection, currently in state init
2022-07-28 21:25:52.813291 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 21:25:52.845464 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 21:25:52.846708 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.848030 (Thread-1): finished collecting timing info
2022-07-28 21:25:52.849769 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 21:25:52.888175 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:25:52.888841 (MainThread): Connection 'model.dbt_.fct_trajectory' was properly closed.
2022-07-28 21:25:52.920313 (MainThread): 21:25:52 | Done.
2022-07-28 21:25:52.989275 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 21:25:52.989732 (MainThread): Opening a new connection, currently in state init
2022-07-28 21:25:52.990030 (MainThread): 21:25:52 | Building catalog
2022-07-28 21:25:53.136343 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.136805 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 21:25:53.470129 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.470569 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:25:53.490583 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-07-28 21:25:53.491667 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.492204 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:25:53.683156 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.19 seconds
2022-07-28 21:25:53.702559 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:25:53.719212 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.719894 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 21:25:53.721685 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 21:25:53.722695 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 21:25:53.723181 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 21:25:53.726692 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 21:25:53.732097 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 21:25:53.954993 (MainThread): 21:25:53 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 21:25:53.955875 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e83b0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f672ac10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93f6863510>]}
2022-07-28 21:25:53.956878 (MainThread): Flushing usage events
2022-07-28 21:25:54.896497 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 21:25:54.898348 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 21:25:54.898890 (MainThread): On warehouse.information_schema: Close
2022-07-28 21:25:58.111235 (MainThread): Running with dbt=0.16.1
2022-07-28 21:25:58.475181 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 21:25:58.484392 (MainThread): Tracking: tracking
2022-07-28 21:25:58.519452 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd63d9990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd648cb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2dd6491e90>]}
2022-07-28 21:25:58.545972 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 21:25:58.552690 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 21:25:58.558434 (MainThread): Press Ctrl+C to exit.


2022-07-28 22:15:21.026669 (MainThread): Running with dbt=0.16.1
2022-07-28 22:15:21.366822 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 22:15:21.414560 (MainThread): Tracking: tracking
2022-07-28 22:15:21.490661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b48b35c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b489fcc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b489fcbd0>]}
2022-07-28 22:15:21.633658 (MainThread): Partial parsing not enabled
2022-07-28 22:15:21.665214 (MainThread): Parsing macros/core.sql
2022-07-28 22:15:21.687439 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:15:21.725629 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:15:21.735061 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:15:21.874273 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:15:21.962565 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:15:21.992136 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:15:22.001693 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:15:22.144832 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:15:22.204394 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:15:22.269729 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:15:22.314349 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:15:22.364116 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:15:22.555827 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:15:22.565415 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:15:22.582569 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:15:22.593773 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:15:22.599012 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:15:22.607762 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:15:22.613706 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:15:22.656602 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:15:22.661570 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:15:22.671790 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:15:22.679564 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:15:22.719756 (MainThread): Parsing macros/relations.sql
2022-07-28 22:15:22.731044 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:15:22.923195 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:15:22.956237 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:15:23.160773 (MainThread): Partial parsing not enabled
2022-07-28 22:15:23.360710 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:23.361144 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.474958 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:23.487646 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.588174 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:23.588611 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:23.977705 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:15:23.978159 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:25.101929 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:15:26.605605 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:15:26.656468 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:15:26.658212 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:15:26.682310 (MainThread): 
2022-07-28 22:15:26.696731 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:15:26.697162 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:15:27.266338 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 22:15:27.267229 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:15:27.854478 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 22:15:27.855559 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 22:15:27.888373 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-28 22:15:28.088275 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.089563 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-28 22:15:28.100139 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.101081 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:15:28.102556 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:28.103271 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:15:28.104094 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:15:28.114914 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 22:15:28.137134 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:15:28.270769 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.271450 (MainThread): On master: BEGIN
2022-07-28 22:15:28.284598 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:15:28.285244 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.285676 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:15:28.327478 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-28 22:15:28.330630 (MainThread): On master: ROLLBACK
2022-07-28 22:15:28.336647 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.337268 (MainThread): On master: BEGIN
2022-07-28 22:15:28.338642 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:28.339682 (MainThread): On master: COMMIT
2022-07-28 22:15:28.340222 (MainThread): Using postgres connection "master".
2022-07-28 22:15:28.340647 (MainThread): On master: COMMIT
2022-07-28 22:15:28.342752 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:15:28.344082 (MainThread): 22:15:28 | Concurrency: 1 threads (target='dev')
2022-07-28 22:15:28.347113 (MainThread): 22:15:28 | 
2022-07-28 22:15:28.434035 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:15:28.434687 (Thread-1): 22:15:28 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 22:15:28.435783 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.436157 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:15:28.436532 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:15:28.560765 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:15:28.575554 (Thread-1): finished collecting timing info
2022-07-28 22:15:28.817587 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.818236 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 22:15:28.820609 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:28.838381 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:28.843871 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:15:28.845037 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:28.997082 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 22:15:29.008066 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.008534 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 22:15:29.009541 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:29.009984 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.010254 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 22:15:29.125537 (Thread-1): SQL status: SELECT 6 in 0.11 seconds
2022-07-28 22:15:29.163782 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.166014 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 22:15:29.167640 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.180078 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.181071 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 22:15:29.182445 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.185552 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:15:29.185981 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.186247 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:15:29.210409 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-28 22:15:29.218386 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:15:29.218858 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:15:29.232852 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 22:15:29.248094 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.253970 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b462d9150>]}
2022-07-28 22:15:29.254869 (Thread-1): 22:15:29 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.82s]
2022-07-28 22:15:29.258124 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:15:29.259425 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:15:29.260009 (Thread-1): 22:15:29 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 22:15:29.261458 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.261855 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 22:15:29.262154 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:15:29.302977 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:15:29.304422 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.338371 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.338860 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 22:15:29.340011 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:29.349388 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.349848 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:15:29.350876 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:29.356937 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 22:15:29.358228 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.358648 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 22:15:29.359430 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:29.359869 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.360143 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:15:29.607297 (Thread-1): SQL status: SELECT 922 in 0.25 seconds
2022-07-28 22:15:29.629477 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.630091 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 22:15:29.631549 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.644696 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.645156 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 22:15:29.646533 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:29.659861 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:15:29.660326 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.660609 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:15:29.699096 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-28 22:15:29.706641 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:15:29.707135 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:15:29.811249 (Thread-1): SQL status: DROP TABLE in 0.10 seconds
2022-07-28 22:15:29.821890 (Thread-1): finished collecting timing info
2022-07-28 22:15:29.832714 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b46296350>]}
2022-07-28 22:15:29.836413 (Thread-1): 22:15:29 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.56s]
2022-07-28 22:15:29.837382 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:15:29.837964 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:15:29.838674 (Thread-1): 22:15:29 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 22:15:29.844398 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:29.844912 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 22:15:29.845358 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:15:29.928017 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:15:29.929107 (Thread-1): finished collecting timing info
2022-07-28 22:15:30.021514 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.022071 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 22:15:30.027588 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:30.045539 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.051666 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:15:30.054133 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:15:30.063547 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:15:30.068185 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.068610 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 22:15:30.071671 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:30.072201 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:30.072496 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:15:47.959397 (Thread-1): SQL status: SELECT 922 in 17.89 seconds
2022-07-28 22:15:48.030098 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.030544 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 22:15:48.032039 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:48.042749 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.044932 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 22:15:48.046471 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:15:48.050284 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:15:48.050730 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.051013 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:15:48.708028 (Thread-1): SQL status: COMMIT in 0.66 seconds
2022-07-28 22:15:48.717063 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:15:48.717506 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:15:49.083632 (Thread-1): SQL status: DROP TABLE in 0.37 seconds
2022-07-28 22:15:49.092949 (Thread-1): finished collecting timing info
2022-07-28 22:15:49.265531 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '273c9572-1c03-4263-99f3-51eeac0449de', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b462ff550>]}
2022-07-28 22:15:49.266393 (Thread-1): 22:15:49 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 19.42s]
2022-07-28 22:15:49.266856 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:15:49.366096 (MainThread): Using postgres connection "master".
2022-07-28 22:15:49.366561 (MainThread): On master: BEGIN
2022-07-28 22:15:49.367631 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:15:49.368087 (MainThread): On master: COMMIT
2022-07-28 22:15:49.368369 (MainThread): Using postgres connection "master".
2022-07-28 22:15:49.368630 (MainThread): On master: COMMIT
2022-07-28 22:15:49.369562 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:15:49.370787 (MainThread): 22:15:49 | 
2022-07-28 22:15:49.371276 (MainThread): 22:15:49 | Finished running 3 table models in 22.67s.
2022-07-28 22:15:49.372731 (MainThread): Connection 'master' was left open.
2022-07-28 22:15:49.373049 (MainThread): On master: Close
2022-07-28 22:15:49.379082 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 22:15:49.379789 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 22:15:49.428725 (MainThread): 
2022-07-28 22:15:49.430526 (MainThread): Completed successfully
2022-07-28 22:15:49.431469 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 22:15:49.432707 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b461e27d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b461fdfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2b48a3cd50>]}
2022-07-28 22:15:49.433413 (MainThread): Flushing usage events
2022-07-28 22:16:58.071110 (MainThread): Running with dbt=0.16.1
2022-07-28 22:16:58.656935 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 22:16:58.687105 (MainThread): Tracking: tracking
2022-07-28 22:16:58.718001 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016e9f650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016fd46d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc016fdad50>]}
2022-07-28 22:16:58.887884 (MainThread): Partial parsing not enabled
2022-07-28 22:16:58.999953 (MainThread): Parsing macros/core.sql
2022-07-28 22:16:59.059886 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:16:59.182856 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:16:59.239160 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:16:59.589707 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:16:59.787925 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:16:59.819423 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:16:59.828800 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:16:59.928146 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:16:59.987533 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:17:00.012778 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:17:00.042210 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:17:00.073145 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:17:00.278230 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:17:00.284271 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:17:00.294937 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:17:00.330588 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:17:00.350400 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:17:00.383935 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:17:00.395246 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:17:00.516101 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:17:00.533578 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:17:00.559891 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:17:00.574516 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:17:00.616419 (MainThread): Parsing macros/relations.sql
2022-07-28 22:17:00.631908 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:17:00.773883 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:17:00.787590 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:17:00.921877 (MainThread): Partial parsing not enabled
2022-07-28 22:17:01.275973 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:17:01.276618 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:01.471482 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:17:01.471940 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:01.545002 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:17:01.545627 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:02.094844 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:17:02.102194 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:03.538715 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:17:05.603023 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:17:05.637931 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:17:05.643637 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:17:05.677293 (MainThread): 
2022-07-28 22:17:05.678467 (MainThread): 22:17:05 | Concurrency: 1 threads (target='dev')
2022-07-28 22:17:05.684069 (MainThread): 22:17:05 | 
2022-07-28 22:17:05.798082 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:17:05.799457 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:17:05.799900 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:05.800220 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:17:06.208004 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:17:06.209628 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.214523 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.216160 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:17:06.220634 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:17:06.225233 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:17:06.226029 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.226595 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:17:06.281285 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:17:06.283146 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.285594 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.288800 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:17:06.289366 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:17:06.300454 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:17:06.300930 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.301242 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:17:06.387321 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:17:06.394260 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.395488 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.397001 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:17:06.397560 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.399050 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:17:06.401010 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:17:06.401498 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.539915 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:17:06.563142 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.564567 (Thread-1): finished collecting timing info
2022-07-28 22:17:06.569158 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:17:06.640026 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:17:06.640495 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:17:06.738109 (MainThread): 22:17:06 | Done.
2022-07-28 22:17:06.789431 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 22:17:06.792221 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:17:06.792658 (MainThread): 22:17:06 | Building catalog
2022-07-28 22:17:07.039548 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.040602 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:17:07.644416 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.645017 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:17:07.705186 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.06 seconds
2022-07-28 22:17:07.705770 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.706143 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:17:07.805527 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.10 seconds
2022-07-28 22:17:07.851252 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:17:07.875574 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.876066 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:17:07.878599 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:17:07.879275 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:17:07.879654 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:17:07.882961 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-28 22:17:07.890470 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:17:08.163485 (MainThread): 22:17:08 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 22:17:08.164354 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459df90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459d690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc01459d7d0>]}
2022-07-28 22:17:08.165152 (MainThread): Flushing usage events
2022-07-28 22:17:09.237337 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 22:17:09.239077 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 22:17:09.239458 (MainThread): On warehouse.information_schema: Close
2022-07-28 22:17:13.104226 (MainThread): Running with dbt=0.16.1
2022-07-28 22:17:13.614043 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 22:17:13.618711 (MainThread): Tracking: tracking
2022-07-28 22:17:13.671297 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d7d58d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d82dbd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f731d6b6390>]}
2022-07-28 22:17:13.696274 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 22:17:13.700651 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 22:17:13.702273 (MainThread): Press Ctrl+C to exit.


2022-07-28 22:21:43.372757 (MainThread): Running with dbt=0.16.1
2022-07-28 22:21:44.685939 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 22:21:44.898380 (MainThread): Tracking: tracking
2022-07-28 22:21:46.039943 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f2ebd10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f2eb050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374f307810>]}
2022-07-28 22:21:46.216624 (MainThread): Partial parsing not enabled
2022-07-28 22:21:46.327744 (MainThread): Parsing macros/core.sql
2022-07-28 22:21:46.358497 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:21:46.403298 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:21:46.414912 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:21:46.615015 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:21:46.706229 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:21:46.802476 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:21:46.814615 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:21:47.002594 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:21:47.138371 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:21:47.204724 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:21:47.277072 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:21:47.385625 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:21:48.000576 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:21:48.011866 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:21:48.033683 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:21:48.071775 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:21:48.081312 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:21:48.107841 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:21:48.121836 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:21:48.234290 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:21:48.243588 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:21:48.271371 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:21:48.287704 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:21:48.381468 (MainThread): Parsing macros/relations.sql
2022-07-28 22:21:48.414684 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:21:48.575786 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:21:48.622327 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:21:48.875583 (MainThread): Partial parsing not enabled
2022-07-28 22:21:49.187688 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:21:49.188177 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.292413 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:21:49.292863 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.344079 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:21:49.344562 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:49.544530 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:49.544966 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:50.590779 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:21:52.678516 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:21:52.692153 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:21:52.693245 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:21:52.705514 (MainThread): 
2022-07-28 22:21:52.708177 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:21:52.708636 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:21:53.092702 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:53.093597 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:21:53.933944 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:53.934422 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:21:53.999030 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.06 seconds
2022-07-28 22:21:54.000380 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:21:54.002144 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:21:54.018015 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.02 seconds
2022-07-28 22:21:54.138963 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:21:54.388524 (MainThread): Using postgres connection "master".
2022-07-28 22:21:54.388980 (MainThread): On master: BEGIN
2022-07-28 22:21:54.408573 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 22:21:54.409069 (MainThread): Using postgres connection "master".
2022-07-28 22:21:54.409430 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:21:54.452240 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-28 22:21:54.455237 (MainThread): On master: ROLLBACK
2022-07-28 22:21:54.456843 (MainThread): 22:21:54 | Concurrency: 1 threads (target='dev')
2022-07-28 22:21:54.457382 (MainThread): 22:21:54 | 
2022-07-28 22:21:54.517308 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.517993 (Thread-1): 22:21:54 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-28 22:21:54.523864 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.532549 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:21:54.535700 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.736989 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:21:54.782911 (Thread-1): finished collecting timing info
2022-07-28 22:21:54.784150 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.784683 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-28 22:21:54.786209 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:21:54.786722 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:21:54.787025 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-28 22:21:54.901268 (Thread-1): SQL status: SELECT 1 in 0.11 seconds
2022-07-28 22:21:54.902275 (Thread-1): finished collecting timing info
2022-07-28 22:21:54.903330 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-28 22:21:54.905098 (Thread-1): 22:21:54 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.38s]
2022-07-28 22:21:54.905627 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:21:54.926221 (MainThread): 22:21:54 | 
2022-07-28 22:21:54.928160 (MainThread): 22:21:54 | Finished running 1 test in 2.22s.
2022-07-28 22:21:54.928648 (MainThread): Connection 'master' was left open.
2022-07-28 22:21:54.928945 (MainThread): On master: Close
2022-07-28 22:21:54.936271 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-28 22:21:54.937305 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-28 22:21:54.964828 (MainThread): 
2022-07-28 22:21:54.965396 (MainThread): Completed successfully
2022-07-28 22:21:54.965802 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-28 22:21:54.966423 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374c9f3d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f374ca08550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f373e7c3350>]}
2022-07-28 22:21:54.967153 (MainThread): Flushing usage events
2022-07-28 22:29:18.061486 (MainThread): Running with dbt=0.16.1
2022-07-28 22:29:18.683074 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-28 22:29:18.738599 (MainThread): Tracking: tracking
2022-07-28 22:29:18.786431 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce99a9e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cea4b4a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce9ae56d0>]}
2022-07-28 22:29:18.918216 (MainThread): Partial parsing not enabled
2022-07-28 22:29:18.955853 (MainThread): Parsing macros/core.sql
2022-07-28 22:29:18.984961 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:29:19.031508 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:29:19.040180 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:29:19.180459 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:29:19.267484 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:29:19.298818 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:29:19.308836 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:29:19.412542 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:29:19.486390 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:29:19.516669 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:29:19.549525 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:29:19.581532 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:29:19.781941 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:29:19.788399 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:29:19.800828 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:29:19.847945 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:29:19.860196 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:29:19.878597 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:29:19.888802 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:29:19.944125 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:29:19.952843 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:29:19.990876 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:29:20.007844 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:29:20.053784 (MainThread): Parsing macros/relations.sql
2022-07-28 22:29:20.064722 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:29:20.206688 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:29:20.233778 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:29:20.357965 (MainThread): Partial parsing not enabled
2022-07-28 22:29:20.583932 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:20.584390 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.696731 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:20.697175 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.760769 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:20.761214 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:20.936519 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:29:20.936948 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:21.773088 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:29:24.696176 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:29:24.744233 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:29:24.745382 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:29:24.836865 (MainThread): 
2022-07-28 22:29:24.839067 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:29:24.848536 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:29:25.781197 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-28 22:29:25.782386 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:29:26.299945 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-28 22:29:26.300421 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-28 22:29:26.448331 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.15 seconds
2022-07-28 22:29:26.680747 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.681234 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:29:26.691412 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.692140 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:29:26.704702 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:29:26.705223 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:29:26.705506 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:29:26.845801 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.14 seconds
2022-07-28 22:29:26.871606 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:29:27.020777 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.021235 (MainThread): On master: BEGIN
2022-07-28 22:29:27.032527 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:29:27.032991 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.033268 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:29:27.131932 (MainThread): SQL status: SELECT 0 in 0.10 seconds
2022-07-28 22:29:27.134959 (MainThread): On master: ROLLBACK
2022-07-28 22:29:27.136057 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.136509 (MainThread): On master: BEGIN
2022-07-28 22:29:27.137826 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:27.138733 (MainThread): On master: COMMIT
2022-07-28 22:29:27.139781 (MainThread): Using postgres connection "master".
2022-07-28 22:29:27.140114 (MainThread): On master: COMMIT
2022-07-28 22:29:27.143251 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:29:27.144471 (MainThread): 22:29:27 | Concurrency: 1 threads (target='dev')
2022-07-28 22:29:27.144970 (MainThread): 22:29:27 | 
2022-07-28 22:29:27.225540 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:29:27.226387 (Thread-1): 22:29:27 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-28 22:29:27.228861 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.229668 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:29:27.230208 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:29:27.445724 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:29:27.604320 (Thread-1): finished collecting timing info
2022-07-28 22:29:27.720977 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.724544 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-28 22:29:27.795632 (Thread-1): SQL status: DROP TABLE in 0.07 seconds
2022-07-28 22:29:27.804688 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.805153 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:29:27.806393 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:27.916070 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-28 22:29:27.950085 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.950701 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-28 22:29:27.953194 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:27.953826 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:27.954241 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-28 22:29:28.221450 (Thread-1): SQL status: SELECT 6 in 0.27 seconds
2022-07-28 22:29:28.237766 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.238230 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-28 22:29:28.239837 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.261853 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.262514 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-28 22:29:28.264852 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.272056 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:29:28.273158 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.273785 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-28 22:29:28.302960 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-07-28 22:29:28.314096 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-28 22:29:28.314580 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-28 22:29:28.357925 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 22:29:28.372554 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.376334 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cd90ed790>]}
2022-07-28 22:29:28.377431 (Thread-1): 22:29:28 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.15s]
2022-07-28 22:29:28.378103 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:29:28.388598 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:29:28.389489 (Thread-1): 22:29:28 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-28 22:29:28.390673 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.397025 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-28 22:29:28.397516 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:29:28.503175 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:29:28.507240 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.569801 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.571643 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-28 22:29:28.573039 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:28.594139 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.595308 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:29:28.598189 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:28.614907 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-28 22:29:28.616409 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.616990 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-28 22:29:28.617892 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:28.618645 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.619142 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:29:28.723791 (Thread-1): SQL status: SELECT 922 in 0.10 seconds
2022-07-28 22:29:28.752901 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.753561 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-28 22:29:28.763190 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.781191 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.781854 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-28 22:29:28.783309 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-28 22:29:28.786861 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:29:28.788456 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.793685 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-28 22:29:28.803559 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 22:29:28.810905 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-28 22:29:28.813605 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-28 22:29:28.826382 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-28 22:29:28.840286 (Thread-1): finished collecting timing info
2022-07-28 22:29:28.845227 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce71aa150>]}
2022-07-28 22:29:28.848312 (Thread-1): 22:29:28 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.45s]
2022-07-28 22:29:28.850536 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:29:28.855967 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:29:28.857021 (Thread-1): 22:29:28 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-28 22:29:28.858363 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:28.859754 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-28 22:29:28.860388 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:29:28.966475 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:29:28.974763 (Thread-1): finished collecting timing info
2022-07-28 22:29:29.063221 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.063787 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-28 22:29:29.065260 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:29.074981 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.075573 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:29:29.077344 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-28 22:29:29.083596 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:29:29.086184 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.086643 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-28 22:29:29.087479 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:29.087897 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:29.088177 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-28 22:29:33.950235 (Thread-1): SQL status: SELECT 922 in 4.86 seconds
2022-07-28 22:29:33.978290 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:33.978753 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-28 22:29:33.984891 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 22:29:34.003098 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.003628 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-28 22:29:34.009693 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-07-28 22:29:34.012946 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:29:34.013426 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.013708 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-28 22:29:34.027656 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-28 22:29:34.034364 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:29:34.034834 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-28 22:29:34.072631 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-07-28 22:29:34.084011 (Thread-1): finished collecting timing info
2022-07-28 22:29:34.092931 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1d326e90-8b69-4b3f-b3a4-c88996af35f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5cd90edad0>]}
2022-07-28 22:29:34.093830 (Thread-1): 22:29:34 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 5.23s]
2022-07-28 22:29:34.095800 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:29:34.153743 (MainThread): Using postgres connection "master".
2022-07-28 22:29:34.154422 (MainThread): On master: BEGIN
2022-07-28 22:29:34.155440 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:29:34.156103 (MainThread): On master: COMMIT
2022-07-28 22:29:34.156560 (MainThread): Using postgres connection "master".
2022-07-28 22:29:34.156968 (MainThread): On master: COMMIT
2022-07-28 22:29:34.157939 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-28 22:29:34.159283 (MainThread): 22:29:34 | 
2022-07-28 22:29:34.160337 (MainThread): 22:29:34 | Finished running 3 table models in 9.32s.
2022-07-28 22:29:34.164752 (MainThread): Connection 'master' was left open.
2022-07-28 22:29:34.165297 (MainThread): On master: Close
2022-07-28 22:29:34.166113 (MainThread): Connection 'list_warehouse' was left open.
2022-07-28 22:29:34.166628 (MainThread): On list_warehouse: Close
2022-07-28 22:29:34.169602 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-28 22:29:34.170126 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-28 22:29:34.245845 (MainThread): 
2022-07-28 22:29:34.246958 (MainThread): Completed successfully
2022-07-28 22:29:34.248262 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-28 22:29:34.251712 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce7197b10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce71aae10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ce7282190>]}
2022-07-28 22:29:34.252613 (MainThread): Flushing usage events
2022-07-28 22:30:13.711947 (MainThread): Running with dbt=0.16.1
2022-07-28 22:30:14.301514 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-28 22:30:14.361969 (MainThread): Tracking: tracking
2022-07-28 22:30:14.541694 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7967990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7aa98d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b7b18ed0>]}
2022-07-28 22:30:14.683542 (MainThread): Partial parsing not enabled
2022-07-28 22:30:14.720720 (MainThread): Parsing macros/core.sql
2022-07-28 22:30:14.743429 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:30:14.789022 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:30:14.798541 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:30:14.951679 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:30:15.095097 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:30:15.147135 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:30:15.165237 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:30:15.313457 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:30:15.392748 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:30:15.418041 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:30:15.450271 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:30:15.493232 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:30:15.693974 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:30:15.699807 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:30:15.709878 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:30:15.721688 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:30:15.727757 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:30:15.736846 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:30:15.742548 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:30:15.786837 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:30:15.791971 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:30:15.807487 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:30:15.813525 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:30:15.849264 (MainThread): Parsing macros/relations.sql
2022-07-28 22:30:15.857391 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:30:15.939500 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:30:15.952038 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:30:16.068464 (MainThread): Partial parsing not enabled
2022-07-28 22:30:16.314917 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:16.315406 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.407781 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:16.408240 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.458397 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:16.458862 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:16.674625 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:16.675075 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:17.581068 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:30:19.127175 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:30:19.140838 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:30:19.142116 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:30:19.181047 (MainThread): 
2022-07-28 22:30:19.182438 (MainThread): Acquiring new postgres connection "master".
2022-07-28 22:30:19.183056 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:19.416529 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.419553 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-28 22:30:19.839997 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.840614 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-28 22:30:19.894248 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2022-07-28 22:30:19.896506 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-28 22:30:19.897355 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-28 22:30:19.909611 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-28 22:30:19.981355 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-28 22:30:20.092949 (MainThread): Using postgres connection "master".
2022-07-28 22:30:20.093398 (MainThread): On master: BEGIN
2022-07-28 22:30:20.113670 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-28 22:30:20.114716 (MainThread): Using postgres connection "master".
2022-07-28 22:30:20.115553 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-28 22:30:20.143156 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-28 22:30:20.147328 (MainThread): On master: ROLLBACK
2022-07-28 22:30:20.150331 (MainThread): 22:30:20 | Concurrency: 1 threads (target='dev')
2022-07-28 22:30:20.151457 (MainThread): 22:30:20 | 
2022-07-28 22:30:20.194776 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.196444 (Thread-1): 22:30:20 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-28 22:30:20.197627 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.200908 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-28 22:30:20.203842 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.305612 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:30:20.332502 (Thread-1): finished collecting timing info
2022-07-28 22:30:20.333947 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.334477 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-28 22:30:20.336224 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-28 22:30:20.336882 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:20.337327 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-28 22:30:20.347679 (Thread-1): SQL status: SELECT 1 in 0.01 seconds
2022-07-28 22:30:20.348728 (Thread-1): finished collecting timing info
2022-07-28 22:30:20.349774 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-28 22:30:20.352687 (Thread-1): 22:30:20 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.16s]
2022-07-28 22:30:20.355235 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:20.401307 (MainThread): 22:30:20 | 
2022-07-28 22:30:20.407645 (MainThread): 22:30:20 | Finished running 1 test in 1.22s.
2022-07-28 22:30:20.408735 (MainThread): Connection 'master' was left open.
2022-07-28 22:30:20.409178 (MainThread): On master: Close
2022-07-28 22:30:20.416512 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-28 22:30:20.417179 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-28 22:30:20.434902 (MainThread): 
2022-07-28 22:30:20.436122 (MainThread): Completed successfully
2022-07-28 22:30:20.437270 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-28 22:30:20.438675 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b50370d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b5074bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67b50b38d0>]}
2022-07-28 22:30:20.440204 (MainThread): Flushing usage events
2022-07-28 22:30:41.815562 (MainThread): Running with dbt=0.16.1
2022-07-28 22:30:42.166404 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-28 22:30:42.170680 (MainThread): Tracking: tracking
2022-07-28 22:30:42.212765 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b3635b390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b36350510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b36350d10>]}
2022-07-28 22:30:42.367325 (MainThread): Partial parsing not enabled
2022-07-28 22:30:42.380196 (MainThread): Parsing macros/core.sql
2022-07-28 22:30:42.414043 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-28 22:30:42.466587 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-28 22:30:42.475032 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-28 22:30:42.692294 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-28 22:30:42.785472 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-28 22:30:42.816282 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-28 22:30:42.833694 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-28 22:30:43.016680 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-28 22:30:43.149916 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-28 22:30:43.198862 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-28 22:30:43.334250 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-28 22:30:43.522583 (MainThread): Parsing macros/adapters/common.sql
2022-07-28 22:30:44.037583 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-28 22:30:44.052217 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-28 22:30:44.074804 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-28 22:30:44.102036 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-28 22:30:44.107566 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-28 22:30:44.130478 (MainThread): Parsing macros/etc/query.sql
2022-07-28 22:30:44.137913 (MainThread): Parsing macros/etc/datetime.sql
2022-07-28 22:30:44.237889 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-28 22:30:44.247591 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-28 22:30:44.270671 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-28 22:30:44.278428 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-28 22:30:44.302350 (MainThread): Parsing macros/relations.sql
2022-07-28 22:30:44.317335 (MainThread): Parsing macros/adapters.sql
2022-07-28 22:30:44.470608 (MainThread): Parsing macros/catalog.sql
2022-07-28 22:30:44.489276 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-28 22:30:44.793843 (MainThread): Partial parsing not enabled
2022-07-28 22:30:45.420471 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:45.420932 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:45.788613 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:45.789054 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:45.919039 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:45.925129 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:46.497662 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:46.498130 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:47.824200 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-28 22:30:52.368183 (MainThread): scipy not found, skipping conversion test.
2022-07-28 22:30:52.407861 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-28 22:30:52.408939 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-28 22:30:52.432713 (MainThread): 
2022-07-28 22:30:52.434448 (MainThread): 22:30:52 | Concurrency: 1 threads (target='dev')
2022-07-28 22:30:52.436243 (MainThread): 22:30:52 | 
2022-07-28 22:30:52.534846 (Thread-1): Began running node model.dbt_.dim_types
2022-07-28 22:30:52.538147 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-28 22:30:52.538631 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:52.538958 (Thread-1): Compiling model.dbt_.dim_types
2022-07-28 22:30:53.066345 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-28 22:30:53.084747 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.085973 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.087703 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-28 22:30:53.089429 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-28 22:30:53.090478 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-28 22:30:53.090869 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.091171 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-28 22:30:53.249811 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-28 22:30:53.263791 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.264887 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.266597 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-28 22:30:53.271675 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-28 22:30:53.277180 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-28 22:30:53.277606 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.277911 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-28 22:30:53.495930 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-28 22:30:53.496976 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.501962 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.508821 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-28 22:30:53.509592 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.511856 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-28 22:30:53.512387 (Thread-1): Opening a new connection, currently in state init
2022-07-28 22:30:53.512756 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.624787 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-28 22:30:53.626443 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.627719 (Thread-1): finished collecting timing info
2022-07-28 22:30:53.629600 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-28 22:30:53.728664 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:30:53.729133 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-28 22:30:53.880635 (MainThread): 22:30:53 | Done.
2022-07-28 22:30:54.321898 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-28 22:30:54.325291 (MainThread): Opening a new connection, currently in state init
2022-07-28 22:30:54.325747 (MainThread): 22:30:54 | Building catalog
2022-07-28 22:30:54.644321 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-28 22:30:54.645219 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-28 22:30:55.613070 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:55.613541 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:30:55.667877 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.05 seconds
2022-07-28 22:30:55.669874 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:55.670380 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:30:55.846672 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.18 seconds
2022-07-28 22:30:56.040829 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:30:56.099482 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:56.106261 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-28 22:30:56.121597 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-07-28 22:30:56.122103 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-28 22:30:56.122500 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-28 22:30:56.129011 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-07-28 22:30:56.153946 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-28 22:30:56.703097 (MainThread): 22:30:56 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-28 22:30:56.708733 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b33a57990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b29de62d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6b363da850>]}
2022-07-28 22:30:56.709473 (MainThread): Flushing usage events
2022-07-28 22:30:58.076562 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-28 22:30:58.078229 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-28 22:30:58.078609 (MainThread): On warehouse.information_schema: Close
2022-07-28 22:31:06.158824 (MainThread): Running with dbt=0.16.1
2022-07-28 22:31:06.864648 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-28 22:31:06.865852 (MainThread): Tracking: tracking
2022-07-28 22:31:07.058942 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02aeef10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02bd6910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a02c82cd0>]}
2022-07-28 22:31:07.071948 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-28 22:31:07.073150 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-28 22:31:07.074140 (MainThread): Press Ctrl+C to exit.


2022-07-29 00:02:15.217180 (MainThread): Running with dbt=0.16.1
2022-07-29 00:02:15.596579 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-29 00:02:15.633028 (MainThread): Tracking: tracking
2022-07-29 00:02:15.688429 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f94bc650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f959b7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f95f7ed0>]}
2022-07-29 00:02:15.784425 (MainThread): Partial parsing not enabled
2022-07-29 00:02:15.849083 (MainThread): Parsing macros/core.sql
2022-07-29 00:02:15.875609 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:02:15.915623 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:02:15.926915 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:02:16.107872 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:02:16.221804 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:02:16.255159 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:02:16.268143 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:02:16.396162 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:02:16.460894 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:02:16.485534 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:02:16.516682 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:02:16.548798 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:02:16.740867 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:02:16.746686 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:02:16.757392 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:02:16.768437 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:02:16.774260 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:02:16.782981 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:02:16.789423 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:02:16.833688 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:02:16.839224 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:02:16.849476 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:02:16.858771 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:02:16.899050 (MainThread): Parsing macros/relations.sql
2022-07-29 00:02:16.907785 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:02:16.981953 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:02:16.993516 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:02:17.107745 (MainThread): Partial parsing not enabled
2022-07-29 00:02:17.290837 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:17.291275 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.369431 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:17.369847 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.416706 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:17.417121 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:17.587786 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:17.588431 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:18.347155 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:02:19.360617 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:02:19.374856 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:02:19.376329 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:02:19.389075 (MainThread): 
2022-07-29 00:02:19.390465 (MainThread): Acquiring new postgres connection "master".
2022-07-29 00:02:19.391093 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:19.568925 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-29 00:02:19.569808 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-29 00:02:20.013167 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-29 00:02:20.013624 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-29 00:02:20.045922 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-07-29 00:02:20.186725 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.187631 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-29 00:02:20.193695 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.194354 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-29 00:02:20.196171 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:20.197052 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:20.197774 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-29 00:02:20.240937 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.04 seconds
2022-07-29 00:02:20.257414 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-29 00:02:20.343198 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.343921 (MainThread): On master: BEGIN
2022-07-29 00:02:20.361220 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-29 00:02:20.361890 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.362325 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-29 00:02:20.391963 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-07-29 00:02:20.395143 (MainThread): On master: ROLLBACK
2022-07-29 00:02:20.396511 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.397141 (MainThread): On master: BEGIN
2022-07-29 00:02:20.399106 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:20.399800 (MainThread): On master: COMMIT
2022-07-29 00:02:20.400337 (MainThread): Using postgres connection "master".
2022-07-29 00:02:20.400746 (MainThread): On master: COMMIT
2022-07-29 00:02:20.401559 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-29 00:02:20.402774 (MainThread): 00:02:20 | Concurrency: 1 threads (target='dev')
2022-07-29 00:02:20.403518 (MainThread): 00:02:20 | 
2022-07-29 00:02:20.807164 (Thread-1): Began running node model.dbt_.dim_types
2022-07-29 00:02:20.807887 (Thread-1): 00:02:20 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-29 00:02:20.808883 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:20.809249 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-29 00:02:20.809584 (Thread-1): Compiling model.dbt_.dim_types
2022-07-29 00:02:20.887718 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-29 00:02:21.003888 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.087481 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.087999 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-29 00:02:21.090048 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.100560 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.101043 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-29 00:02:21.102411 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.168740 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-29 00:02:21.170069 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.170502 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-29 00:02:21.171632 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:21.172074 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.172357 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-29 00:02:21.615301 (Thread-1): SQL status: SELECT 6 in 0.44 seconds
2022-07-29 00:02:21.631921 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.632392 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-07-29 00:02:21.634945 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:21.644380 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.644881 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-29 00:02:21.647023 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:21.650722 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-29 00:02:21.651188 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.651652 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-29 00:02:21.676760 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-07-29 00:02:21.690614 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:21.691111 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-29 00:02:21.754759 (Thread-1): SQL status: DROP TABLE in 0.06 seconds
2022-07-29 00:02:21.772932 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.775577 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6c8ee10>]}
2022-07-29 00:02:21.776469 (Thread-1): 00:02:21 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.97s]
2022-07-29 00:02:21.776942 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-29 00:02:21.778461 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-29 00:02:21.779072 (Thread-1): 00:02:21 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-29 00:02:21.780725 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.781099 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-29 00:02:21.781392 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-29 00:02:21.814629 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-29 00:02:21.815853 (Thread-1): finished collecting timing info
2022-07-29 00:02:21.842729 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.843212 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-29 00:02:21.844647 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.854124 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.854595 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-29 00:02:21.855752 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:21.861648 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-29 00:02:21.863559 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.864007 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-29 00:02:21.865239 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:21.865752 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:21.866031 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-29 00:02:22.126854 (Thread-1): SQL status: SELECT 922 in 0.26 seconds
2022-07-29 00:02:22.154656 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.155141 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-07-29 00:02:22.158357 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:22.168882 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.169349 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-29 00:02:22.170958 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:22.175153 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-29 00:02:22.175632 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.175918 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-29 00:02:22.189039 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-29 00:02:22.203449 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:22.203914 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-29 00:02:22.211898 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-29 00:02:22.224610 (Thread-1): finished collecting timing info
2022-07-29 00:02:22.227777 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6cc2b10>]}
2022-07-29 00:02:22.229432 (Thread-1): 00:02:22 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.45s]
2022-07-29 00:02:22.229963 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-29 00:02:22.230473 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-29 00:02:22.231206 (Thread-1): 00:02:22 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-29 00:02:22.232310 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.232696 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-29 00:02:22.232990 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-29 00:02:22.268815 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:02:22.269883 (Thread-1): finished collecting timing info
2022-07-29 00:02:22.302784 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.303330 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-29 00:02:22.304535 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:22.314554 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.315068 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-29 00:02:22.316446 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-29 00:02:22.322090 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:02:22.323325 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.323781 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-29 00:02:22.324689 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:22.325076 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:22.325347 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-29 00:02:29.744450 (Thread-1): SQL status: SELECT 922 in 7.42 seconds
2022-07-29 00:02:29.760624 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.761262 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-07-29 00:02:29.763152 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:29.773329 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.773921 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-29 00:02:29.775747 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-29 00:02:29.778890 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-29 00:02:29.780744 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.781245 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-29 00:02:29.826366 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-07-29 00:02:29.833619 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:29.834219 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-29 00:02:29.984752 (Thread-1): SQL status: DROP TABLE in 0.15 seconds
2022-07-29 00:02:29.994994 (Thread-1): finished collecting timing info
2022-07-29 00:02:29.997633 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6b906902-6ec0-422b-aa49-c406f239f767', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6d73c50>]}
2022-07-29 00:02:29.998638 (Thread-1): 00:02:29 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 7.77s]
2022-07-29 00:02:29.999245 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-29 00:02:30.093903 (MainThread): Using postgres connection "master".
2022-07-29 00:02:30.094364 (MainThread): On master: BEGIN
2022-07-29 00:02:30.095481 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:30.096097 (MainThread): On master: COMMIT
2022-07-29 00:02:30.096506 (MainThread): Using postgres connection "master".
2022-07-29 00:02:30.096981 (MainThread): On master: COMMIT
2022-07-29 00:02:30.098448 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-29 00:02:30.099865 (MainThread): 00:02:30 | 
2022-07-29 00:02:30.100551 (MainThread): 00:02:30 | Finished running 3 table models in 10.71s.
2022-07-29 00:02:30.101068 (MainThread): Connection 'master' was left open.
2022-07-29 00:02:30.101486 (MainThread): On master: Close
2022-07-29 00:02:30.102183 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-29 00:02:30.102737 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-29 00:02:30.169525 (MainThread): 
2022-07-29 00:02:30.170541 (MainThread): Completed successfully
2022-07-29 00:02:30.171698 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-29 00:02:30.172930 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6c394d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67e8c00550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f67f6cac390>]}
2022-07-29 00:02:30.173634 (MainThread): Flushing usage events
2022-07-29 00:02:47.714334 (MainThread): Running with dbt=0.16.1
2022-07-29 00:02:48.018428 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-29 00:02:48.019453 (MainThread): Tracking: tracking
2022-07-29 00:02:48.038732 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efefbc9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7eff011810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efeeda510>]}
2022-07-29 00:02:48.142728 (MainThread): Partial parsing not enabled
2022-07-29 00:02:48.152193 (MainThread): Parsing macros/core.sql
2022-07-29 00:02:48.174620 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:02:48.210080 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:02:48.218058 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:02:48.362897 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:02:48.446195 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:02:48.476855 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:02:48.486663 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:02:48.584298 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:02:48.647059 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:02:48.669982 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:02:48.697834 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:02:48.727391 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:02:48.912008 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:02:48.916905 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:02:48.931584 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:02:48.941665 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:02:48.946231 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:02:48.954252 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:02:48.959428 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:02:48.999277 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:02:49.003792 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:02:49.012998 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:02:49.018148 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:02:49.024499 (MainThread): Parsing macros/relations.sql
2022-07-29 00:02:49.031647 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:02:49.204220 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:02:49.222235 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:02:49.306769 (MainThread): Partial parsing not enabled
2022-07-29 00:02:49.440775 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:02:49.441244 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.512380 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:02:49.512833 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.542185 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:02:49.542641 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:49.678152 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:49.678612 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:50.332000 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:02:51.300266 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:02:51.313435 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:02:51.314815 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:02:51.327120 (MainThread): 
2022-07-29 00:02:51.329117 (MainThread): Acquiring new postgres connection "master".
2022-07-29 00:02:51.329759 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:02:51.511024 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.511942 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-29 00:02:51.896599 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.897246 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-29 00:02:51.910078 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-07-29 00:02:51.911077 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-29 00:02:51.911667 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-29 00:02:51.923979 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-07-29 00:02:51.946676 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-29 00:02:52.024066 (MainThread): Using postgres connection "master".
2022-07-29 00:02:52.024728 (MainThread): On master: BEGIN
2022-07-29 00:02:52.037177 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-29 00:02:52.037681 (MainThread): Using postgres connection "master".
2022-07-29 00:02:52.037985 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-29 00:02:52.063140 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-07-29 00:02:52.066637 (MainThread): On master: ROLLBACK
2022-07-29 00:02:52.068620 (MainThread): 00:02:52 | Concurrency: 1 threads (target='dev')
2022-07-29 00:02:52.069140 (MainThread): 00:02:52 | 
2022-07-29 00:02:52.084097 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.084896 (Thread-1): 00:02:52 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-29 00:02:52.086247 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.086791 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-29 00:02:52.087240 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.191880 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-29 00:02:52.225314 (Thread-1): finished collecting timing info
2022-07-29 00:02:52.226715 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.227234 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-29 00:02:52.229625 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:02:52.230285 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:02:52.230722 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-29 00:02:52.234623 (Thread-1): SQL status: SELECT 1 in 0.00 seconds
2022-07-29 00:02:52.240313 (Thread-1): finished collecting timing info
2022-07-29 00:02:52.241340 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-29 00:02:52.243070 (Thread-1): 00:02:52 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.16s]
2022-07-29 00:02:52.243858 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-29 00:02:52.292598 (MainThread): 00:02:52 | 
2022-07-29 00:02:52.293946 (MainThread): 00:02:52 | Finished running 1 test in 0.96s.
2022-07-29 00:02:52.295068 (MainThread): Connection 'master' was left open.
2022-07-29 00:02:52.303598 (MainThread): On master: Close
2022-07-29 00:02:52.310398 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-29 00:02:52.311076 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-29 00:02:52.325858 (MainThread): 
2022-07-29 00:02:52.327063 (MainThread): Completed successfully
2022-07-29 00:02:52.328185 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-29 00:02:52.329797 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efc5c7550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7efc593110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f7eee61f790>]}
2022-07-29 00:02:52.330743 (MainThread): Flushing usage events
2022-07-29 00:03:07.689328 (MainThread): Running with dbt=0.16.1
2022-07-29 00:03:07.971741 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-29 00:03:07.972737 (MainThread): Tracking: tracking
2022-07-29 00:03:07.989233 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10223650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10367e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c10317fd0>]}
2022-07-29 00:03:08.085775 (MainThread): Partial parsing not enabled
2022-07-29 00:03:08.092118 (MainThread): Parsing macros/core.sql
2022-07-29 00:03:08.115637 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-29 00:03:08.201102 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-29 00:03:08.220336 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-29 00:03:08.374855 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-29 00:03:08.467590 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-29 00:03:08.496882 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-29 00:03:08.506221 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-29 00:03:08.602320 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-29 00:03:08.659753 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-29 00:03:08.684423 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-29 00:03:08.713085 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-29 00:03:08.744359 (MainThread): Parsing macros/adapters/common.sql
2022-07-29 00:03:08.929853 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-29 00:03:08.935184 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-29 00:03:08.944753 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-29 00:03:08.955302 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-29 00:03:08.960016 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-29 00:03:08.968220 (MainThread): Parsing macros/etc/query.sql
2022-07-29 00:03:08.973525 (MainThread): Parsing macros/etc/datetime.sql
2022-07-29 00:03:09.015521 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-29 00:03:09.019861 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-29 00:03:09.029490 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-29 00:03:09.035018 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-29 00:03:09.041458 (MainThread): Parsing macros/relations.sql
2022-07-29 00:03:09.048388 (MainThread): Parsing macros/adapters.sql
2022-07-29 00:03:09.127099 (MainThread): Parsing macros/catalog.sql
2022-07-29 00:03:09.152833 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-29 00:03:09.249011 (MainThread): Partial parsing not enabled
2022-07-29 00:03:09.388050 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:03:09.388692 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.463266 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:03:09.464041 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.495851 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:03:09.496489 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:09.636140 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:03:09.636805 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:10.376042 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-29 00:03:11.307253 (MainThread): scipy not found, skipping conversion test.
2022-07-29 00:03:11.319718 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-29 00:03:11.320860 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-29 00:03:11.332850 (MainThread): 
2022-07-29 00:03:11.333521 (MainThread): 00:03:11 | Concurrency: 1 threads (target='dev')
2022-07-29 00:03:11.333919 (MainThread): 00:03:11 | 
2022-07-29 00:03:11.354895 (Thread-1): Began running node model.dbt_.dim_types
2022-07-29 00:03:11.356242 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-29 00:03:11.356682 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.357007 (Thread-1): Compiling model.dbt_.dim_types
2022-07-29 00:03:11.541270 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-29 00:03:11.542605 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.544072 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.546151 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-29 00:03:11.547663 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-29 00:03:11.548943 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-29 00:03:11.549541 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.549983 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-29 00:03:11.584158 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-29 00:03:11.585416 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.586735 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.588573 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-29 00:03:11.589345 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-29 00:03:11.590835 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-29 00:03:11.591688 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.592154 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-29 00:03:11.626086 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-29 00:03:11.627323 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.628881 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.630634 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-29 00:03:11.631467 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.632911 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-29 00:03:11.633497 (Thread-1): Opening a new connection, currently in state init
2022-07-29 00:03:11.633931 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.697301 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-29 00:03:11.698521 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.700107 (Thread-1): finished collecting timing info
2022-07-29 00:03:11.702209 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-29 00:03:11.792264 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-29 00:03:11.793892 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-29 00:03:11.841410 (MainThread): 00:03:11 | Done.
2022-07-29 00:03:11.889683 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-29 00:03:11.890305 (MainThread): Opening a new connection, currently in state init
2022-07-29 00:03:11.890735 (MainThread): 00:03:11 | Building catalog
2022-07-29 00:03:11.998921 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.000153 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-29 00:03:12.394014 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.394934 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-29 00:03:12.418947 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-07-29 00:03:12.420015 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:12.421544 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-29 00:03:14.253141 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 1.83 seconds
2022-07-29 00:03:14.353443 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-29 00:03:14.369601 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:14.370245 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-29 00:03:14.371793 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-29 00:03:14.372792 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-29 00:03:14.373334 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-29 00:03:14.378586 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-07-29 00:03:14.384037 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-29 00:03:14.613159 (MainThread): 00:03:14 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-29 00:03:14.614168 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d925b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d9250d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4c0d925950>]}
2022-07-29 00:03:14.615062 (MainThread): Flushing usage events
2022-07-29 00:03:15.604826 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-29 00:03:15.606829 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-29 00:03:15.608360 (MainThread): On warehouse.information_schema: Close
2022-07-29 00:03:20.088114 (MainThread): Running with dbt=0.16.1
2022-07-29 00:03:20.410738 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-29 00:03:20.411759 (MainThread): Tracking: tracking
2022-07-29 00:03:20.574106 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f69ebe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6ab5710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b1c350>]}
2022-07-29 00:03:20.584059 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-29 00:03:20.584639 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-29 00:03:20.584943 (MainThread): Press Ctrl+C to exit.


2022-07-29 00:03:20.587099 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b42e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b4c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff7f6b4c290>]}
2022-07-29 00:03:20.587937 (MainThread): Flushing usage events
2022-07-29 00:03:21.481458 (MainThread): Encountered an error:
2022-07-29 00:03:21.482956 (MainThread): [Errno 98] Address already in use
2022-07-29 00:03:21.534013 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/serve.py", line 31, in run
    SimpleHTTPRequestHandler  # type: ignore
  File "/usr/local/lib/python3.7/socketserver.py", line 452, in __init__
    self.server_bind()
  File "/usr/local/lib/python3.7/socketserver.py", line 466, in server_bind
    self.socket.bind(self.server_address)
OSError: [Errno 98] Address already in use

2022-07-30 19:16:20.330448 (MainThread): Running with dbt=0.16.1
2022-07-30 19:16:20.850472 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:16:20.895376 (MainThread): Tracking: tracking
2022-07-30 19:16:21.282043 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e0c05d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e219f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e0a50d0>]}
2022-07-30 19:16:21.554101 (MainThread): Partial parsing not enabled
2022-07-30 19:16:21.602407 (MainThread): Parsing macros/core.sql
2022-07-30 19:16:21.690980 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:16:21.799219 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:16:21.824958 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:16:22.027773 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:16:22.263548 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:16:22.381310 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:16:22.412253 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:16:22.644683 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:16:22.769141 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:16:22.807279 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:16:22.853745 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:16:22.903453 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:16:23.172744 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:16:23.190626 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:16:23.202616 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:16:23.220572 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:16:23.254246 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:16:23.285195 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:16:23.297823 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:16:23.385330 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:16:23.412531 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:16:23.446558 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:16:23.473312 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:16:23.575319 (MainThread): Parsing macros/relations.sql
2022-07-30 19:16:23.606457 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:16:23.899826 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:16:23.936929 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:16:24.373310 (MainThread): Partial parsing not enabled
2022-07-30 19:16:24.967746 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:24.970774 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.133836 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:16:25.134720 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.223207 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:16:25.223831 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:25.415356 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:16:25.416014 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:26.757440 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:16:28.245453 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:16:28.307626 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:16:28.308837 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:16:28.321584 (MainThread): 
2022-07-30 19:16:28.322905 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:16:28.323423 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:16:28.568379 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-30 19:16:28.569073 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:16:29.116342 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-30 19:16:29.116792 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-30 19:16:29.242806 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.13 seconds
2022-07-30 19:16:29.273767 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.274203 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-30 19:16:29.274460 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-07-30 19:16:29.283878 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.285161 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-07-30 19:16:29.287171 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.289834 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.290312 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-07-30 19:16:29.319269 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.03 seconds
2022-07-30 19:16:29.322186 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-30 19:16:29.322622 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-07-30 19:16:29.322904 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-07-30 19:16:29.341669 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.02 seconds
2022-07-30 19:16:29.417393 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.418200 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly create_warehouse_warehouse).
2022-07-30 19:16:29.436262 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.436951 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:16:29.438297 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.438916 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:16:29.439426 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:16:29.555972 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 0.12 seconds
2022-07-30 19:16:29.559868 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:16:29.709717 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.710185 (MainThread): On master: BEGIN
2022-07-30 19:16:29.724655 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:16:29.729004 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.735563 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:16:29.818782 (MainThread): SQL status: SELECT 0 in 0.08 seconds
2022-07-30 19:16:29.822968 (MainThread): On master: ROLLBACK
2022-07-30 19:16:29.824380 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.824984 (MainThread): On master: BEGIN
2022-07-30 19:16:29.826823 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:29.829815 (MainThread): On master: COMMIT
2022-07-30 19:16:29.838481 (MainThread): Using postgres connection "master".
2022-07-30 19:16:29.838932 (MainThread): On master: COMMIT
2022-07-30 19:16:29.839920 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:16:29.840997 (MainThread): 19:16:29 | Concurrency: 1 threads (target='dev')
2022-07-30 19:16:29.841499 (MainThread): 19:16:29 | 
2022-07-30 19:16:29.876219 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:16:29.877081 (Thread-1): 19:16:29 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-30 19:16:29.878329 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:29.878891 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:16:29.879453 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:16:29.962892 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:16:29.964441 (Thread-1): finished collecting timing info
2022-07-30 19:16:30.046734 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.047474 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-30 19:16:30.048912 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:16:30.057726 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.058391 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:16:30.059591 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:16:30.161477 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-30 19:16:30.163684 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.164282 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-30 19:16:30.166216 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:30.171566 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:16:30.172969 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-30 19:16:30.192097 (Thread-1): Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^

2022-07-30 19:16:30.192577 (Thread-1): On model.dbt_.dim_types: ROLLBACK
2022-07-30 19:16:30.195415 (Thread-1): finished collecting timing info
2022-07-30 19:16:30.198730 (Thread-1): Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-30 19:16:30.344638 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9668870b-2c0e-47d8-a3d3-a2ad496e8b91', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b7c07d0>]}
2022-07-30 19:16:30.345449 (Thread-1): 19:16:30 | 1 of 3 ERROR creating table model warehouse.dim_types................ [ERROR in 0.47s]
2022-07-30 19:16:30.345931 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:16:30.352552 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:16:30.353131 (Thread-1): 19:16:30 | 2 of 3 SKIP relation warehouse.fct_summary........................... [SKIP]
2022-07-30 19:16:30.353561 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:16:30.359474 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:16:30.360374 (Thread-1): 19:16:30 | 3 of 3 SKIP relation warehouse.fct_trajectory........................ [SKIP]
2022-07-30 19:16:30.360836 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:16:30.400347 (MainThread): Using postgres connection "master".
2022-07-30 19:16:30.400827 (MainThread): On master: BEGIN
2022-07-30 19:16:30.401632 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:16:30.402080 (MainThread): On master: COMMIT
2022-07-30 19:16:30.402356 (MainThread): Using postgres connection "master".
2022-07-30 19:16:30.402610 (MainThread): On master: COMMIT
2022-07-30 19:16:30.404928 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:16:30.406349 (MainThread): 19:16:30 | 
2022-07-30 19:16:30.407049 (MainThread): 19:16:30 | Finished running 3 table models in 2.08s.
2022-07-30 19:16:30.409257 (MainThread): Connection 'master' was left open.
2022-07-30 19:16:30.409864 (MainThread): On master: Close
2022-07-30 19:16:30.415245 (MainThread): Connection 'model.dbt_.dim_types' was left open.
2022-07-30 19:16:30.415928 (MainThread): On model.dbt_.dim_types: Close
2022-07-30 19:16:30.462283 (MainThread): 
2022-07-30 19:16:30.463005 (MainThread): Completed with 1 error and 0 warnings:
2022-07-30 19:16:30.463647 (MainThread): 
2022-07-30 19:16:30.464197 (MainThread): Database Error in model dim_types (models/traffic_models/dim_types.sql)
2022-07-30 19:16:30.464692 (MainThread):   relation "warehouse.source" does not exist
2022-07-30 19:16:30.465149 (MainThread):   LINE 8:     select * from warehouse.warehouse.source
2022-07-30 19:16:30.465699 (MainThread):                             ^
2022-07-30 19:16:30.466161 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-07-30 19:16:30.466642 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2022-07-30 19:16:30.467485 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5e156790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b8f2e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fed5b7d6350>]}
2022-07-30 19:16:30.468348 (MainThread): Flushing usage events
2022-07-30 19:23:12.511996 (MainThread): Running with dbt=0.16.1
2022-07-30 19:23:12.881312 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:23:12.901396 (MainThread): Tracking: tracking
2022-07-30 19:23:12.953446 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13e06390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13de0450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13de0b90>]}
2022-07-30 19:23:13.198791 (MainThread): Partial parsing not enabled
2022-07-30 19:23:13.299391 (MainThread): Parsing macros/core.sql
2022-07-30 19:23:13.426075 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:23:13.611772 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:23:13.647544 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:23:13.835638 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:23:14.109187 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:23:14.156738 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:23:14.170567 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:23:14.301924 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:23:14.460200 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:23:14.564122 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:23:14.677573 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:23:14.771342 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:23:15.107587 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:23:15.136282 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:23:15.159959 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:23:15.188210 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:23:15.210748 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:23:15.241851 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:23:15.254867 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:23:15.353310 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:23:15.367628 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:23:15.403518 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:23:15.429979 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:23:15.465382 (MainThread): Parsing macros/relations.sql
2022-07-30 19:23:15.496737 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:23:15.712420 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:23:15.755577 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:23:15.973670 (MainThread): Partial parsing not enabled
2022-07-30 19:23:16.241710 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:23:16.242150 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.336472 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:23:16.337005 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.386983 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:23:16.387668 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:16.553755 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:23:16.554531 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:23:17.133851 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f13f23810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f17be6b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8f115efcd0>]}
2022-07-30 19:23:17.134667 (MainThread): Flushing usage events
2022-07-30 19:23:18.508755 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:23:18.510326 (MainThread): Encountered an error:
2022-07-30 19:23:18.511752 (MainThread): Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found
2022-07-30 19:23:18.608951 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 301, in process_manifest
    process_sources(manifest, project_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 587, in process_sources
    _process_sources_for_node(manifest, current_project, node)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 575, in _process_sources_for_node
    table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/utils.py", line 348, in invalid_source_fail_unless_test
    target_table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 502, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found

2022-07-30 19:26:17.028777 (MainThread): Running with dbt=0.16.1
2022-07-30 19:26:17.313315 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:26:17.332345 (MainThread): Tracking: tracking
2022-07-30 19:26:17.353484 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d10bf890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d10caf10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d117d1d0>]}
2022-07-30 19:26:17.439627 (MainThread): Partial parsing not enabled
2022-07-30 19:26:17.445715 (MainThread): Parsing macros/core.sql
2022-07-30 19:26:17.466356 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:26:17.514202 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:26:17.523844 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:26:17.666178 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:26:17.749816 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:26:17.779003 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:26:17.788526 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:26:17.891293 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:26:17.951377 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:26:17.974261 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:26:18.002823 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:26:18.032599 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:26:18.230685 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:26:18.235951 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:26:18.246260 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:26:18.263448 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:26:18.268611 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:26:18.279524 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:26:18.302230 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:26:18.362247 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:26:18.366797 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:26:18.377101 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:26:18.383617 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:26:18.389922 (MainThread): Parsing macros/relations.sql
2022-07-30 19:26:18.397097 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:26:18.484732 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:26:18.498400 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:26:18.584193 (MainThread): Partial parsing not enabled
2022-07-30 19:26:18.735195 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:26:18.735663 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.810244 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:26:18.810705 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.840214 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:26:18.840646 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:18.975502 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:26:18.975941 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:26:19.362100 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3ce8c6590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3d1bc9b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff3ce8c1190>]}
2022-07-30 19:26:19.362915 (MainThread): Flushing usage events
2022-07-30 19:26:20.426206 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:26:20.428512 (MainThread): Encountered an error:
2022-07-30 19:26:20.429149 (MainThread): Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found
2022-07-30 19:26:20.509955 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 351, in run
    self._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 107, in _runtime_initialize
    super()._runtime_initialize()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 75, in _runtime_initialize
    self.load_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 63, in load_manifest
    self.manifest = get_full_manifest(self.config)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/perf_utils.py", line 23, in get_full_manifest
    return load_manifest(config, internal, set_header)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 646, in load_manifest
    return ManifestLoader.load_all(config, internal_manifest, macro_hook)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 338, in load_all
    manifest = loader.create_manifest()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 323, in create_manifest
    self.process_manifest(manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 301, in process_manifest
    process_sources(manifest, project_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 587, in process_sources
    _process_sources_for_node(manifest, current_project, node)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/parser/manifest.py", line 575, in _process_sources_for_node
    table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/utils.py", line 348, in invalid_source_fail_unless_test
    target_table_name)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 502, in source_target_not_found
    raise_compiler_error(msg, model)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/exceptions.py", line 363, in raise_compiler_error
    raise CompilationException(msg, node)
dbt.exceptions.CompilationException: Compilation Error in model fct_summary (models/traffic_models/fct_summary.sql)
  Model 'model.dbt_.fct_summary' (models/traffic_models/fct_summary.sql) depends on source 'traffic_source.source' which was not found

2022-07-30 19:29:47.695588 (MainThread): Running with dbt=0.16.1
2022-07-30 19:29:47.988063 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-07-30 19:29:47.989367 (MainThread): Tracking: tracking
2022-07-30 19:29:48.017122 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf430a210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf43edc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf43ed2d0>]}
2022-07-30 19:29:48.106989 (MainThread): Partial parsing not enabled
2022-07-30 19:29:48.113170 (MainThread): Parsing macros/core.sql
2022-07-30 19:29:48.134372 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:29:48.169395 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:29:48.177534 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:29:48.316474 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:29:48.395023 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:29:48.423645 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:29:48.432432 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:29:48.525286 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:29:48.594292 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:29:48.635669 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:29:48.664883 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:29:48.696130 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:29:48.912933 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:29:48.917947 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:29:48.927430 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:29:48.937600 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:29:48.942187 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:29:48.950566 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:29:48.955742 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:29:48.996349 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:29:49.001326 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:29:49.010764 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:29:49.019354 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:29:49.025640 (MainThread): Parsing macros/relations.sql
2022-07-30 19:29:49.032523 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:29:49.103360 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:29:49.114780 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:29:49.253171 (MainThread): Partial parsing not enabled
2022-07-30 19:29:49.401278 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:49.401952 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.479731 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:49.480351 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.509959 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:49.510589 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:49.675384 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:29:49.676021 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:50.358284 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:29:51.355933 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:29:51.368297 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:29:51.369377 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:29:51.381018 (MainThread): 
2022-07-30 19:29:51.382631 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:29:51.383069 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:29:51.567619 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-07-30 19:29:51.569057 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:29:51.962810 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-07-30 19:29:51.963492 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-07-30 19:29:51.983996 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-07-30 19:29:52.069706 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.071002 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-07-30 19:29:52.082533 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.083426 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:29:52.085743 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:52.088256 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:29:52.091564 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:29:52.100447 (ThreadPoolExecutor-1_0): SQL status: SELECT 1 in 0.01 seconds
2022-07-30 19:29:52.106749 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:29:52.212879 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.213605 (MainThread): On master: BEGIN
2022-07-30 19:29:52.232610 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-07-30 19:29:52.235882 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.236239 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:29:52.251365 (MainThread): SQL status: SELECT 0 in 0.01 seconds
2022-07-30 19:29:52.256365 (MainThread): On master: ROLLBACK
2022-07-30 19:29:52.258471 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.259065 (MainThread): On master: BEGIN
2022-07-30 19:29:52.263143 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:52.263677 (MainThread): On master: COMMIT
2022-07-30 19:29:52.263979 (MainThread): Using postgres connection "master".
2022-07-30 19:29:52.264247 (MainThread): On master: COMMIT
2022-07-30 19:29:52.266441 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:29:52.272404 (MainThread): 19:29:52 | Concurrency: 1 threads (target='dev')
2022-07-30 19:29:52.273495 (MainThread): 19:29:52 | 
2022-07-30 19:29:52.310478 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:29:52.311791 (Thread-1): 19:29:52 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-07-30 19:29:52.313566 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.314176 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:29:52.315828 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:29:52.411540 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:29:52.413297 (Thread-1): finished collecting timing info
2022-07-30 19:29:52.553791 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.554279 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-07-30 19:29:52.564149 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-07-30 19:29:52.577337 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.577783 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:29:52.578856 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:52.771788 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-07-30 19:29:52.778025 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.779823 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-07-30 19:29:52.789218 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:29:52.805015 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:52.807970 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-07-30 19:29:53.108911 (Thread-1): SQL status: SELECT 6 in 0.30 seconds
2022-07-30 19:29:53.129984 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.134410 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-07-30 19:29:53.139001 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:53.144151 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-30 19:29:53.144859 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.145324 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-07-30 19:29:53.248517 (Thread-1): SQL status: COMMIT in 0.10 seconds
2022-07-30 19:29:53.261147 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-07-30 19:29:53.262852 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-07-30 19:29:53.264518 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.276689 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.279320 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf19a6f90>]}
2022-07-30 19:29:53.280315 (Thread-1): 19:29:53 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.97s]
2022-07-30 19:29:53.280830 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:29:53.287539 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:29:53.288165 (Thread-1): 19:29:53 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-07-30 19:29:53.289131 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.289499 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-07-30 19:29:53.289792 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-30 19:29:53.424810 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-30 19:29:53.426296 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.469752 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.470380 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-07-30 19:29:53.473425 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.495233 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.498810 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-30 19:29:53.502807 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.522330 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-07-30 19:29:53.525875 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.526301 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-07-30 19:29:53.528200 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:53.528673 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.528943 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-30 19:29:53.635636 (Thread-1): SQL status: SELECT 922 in 0.10 seconds
2022-07-30 19:29:53.651580 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.653460 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-07-30 19:29:53.656504 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:53.665002 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-30 19:29:53.667012 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.669504 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-07-30 19:29:53.681486 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-07-30 19:29:53.690145 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-07-30 19:29:53.693073 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-07-30 19:29:53.694359 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.724277 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.732692 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b48f90>]}
2022-07-30 19:29:53.738058 (Thread-1): 19:29:53 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.44s]
2022-07-30 19:29:53.740905 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:29:53.743661 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:29:53.744811 (Thread-1): 19:29:53 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-07-30 19:29:53.751882 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.752979 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-07-30 19:29:53.754612 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-30 19:29:53.818810 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:29:53.820408 (Thread-1): finished collecting timing info
2022-07-30 19:29:53.862161 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.864311 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-07-30 19:29:53.865870 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.875574 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.876212 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-30 19:29:53.877445 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:53.890375 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:29:53.892497 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.893000 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-07-30 19:29:53.893861 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:53.894287 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:53.894547 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-07-30 19:29:55.905605 (Thread-1): SQL status: SELECT 922 in 2.01 seconds
2022-07-30 19:29:55.976091 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:55.976541 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-07-30 19:29:55.981311 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-07-30 19:29:55.985999 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-30 19:29:55.986455 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:55.986729 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-07-30 19:29:56.676070 (Thread-1): SQL status: COMMIT in 0.69 seconds
2022-07-30 19:29:56.682704 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:29:56.686862 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-07-30 19:29:56.688252 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-07-30 19:29:56.710695 (Thread-1): finished collecting timing info
2022-07-30 19:29:56.713483 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd056c54-79b3-4d37-8afe-317904b704d9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b57fd0>]}
2022-07-30 19:29:56.714533 (Thread-1): 19:29:56 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 2.97s]
2022-07-30 19:29:56.715256 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:29:56.785049 (MainThread): Using postgres connection "master".
2022-07-30 19:29:56.785484 (MainThread): On master: BEGIN
2022-07-30 19:29:56.786563 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:29:56.787051 (MainThread): On master: COMMIT
2022-07-30 19:29:56.787432 (MainThread): Using postgres connection "master".
2022-07-30 19:29:56.787696 (MainThread): On master: COMMIT
2022-07-30 19:29:56.788924 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-07-30 19:29:56.790521 (MainThread): 19:29:56 | 
2022-07-30 19:29:56.791566 (MainThread): 19:29:56 | Finished running 3 table models in 5.41s.
2022-07-30 19:29:56.792359 (MainThread): Connection 'master' was left open.
2022-07-30 19:29:56.792709 (MainThread): On master: Close
2022-07-30 19:29:56.793231 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-07-30 19:29:56.793705 (MainThread): On model.dbt_.fct_trajectory: Close
2022-07-30 19:29:56.834697 (MainThread): 
2022-07-30 19:29:56.835737 (MainThread): Completed successfully
2022-07-30 19:29:56.836600 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-07-30 19:29:56.837726 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bec01a090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0be3a2d550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bf1b95850>]}
2022-07-30 19:29:56.838466 (MainThread): Flushing usage events
2022-07-30 19:30:15.353058 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:15.769765 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-07-30 19:30:15.770716 (MainThread): Tracking: tracking
2022-07-30 19:30:15.788064 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9232e9490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa925eccd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa923137150>]}
2022-07-30 19:30:15.872789 (MainThread): Partial parsing not enabled
2022-07-30 19:30:15.878773 (MainThread): Parsing macros/core.sql
2022-07-30 19:30:15.900171 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:30:15.934843 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:30:15.942981 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:30:16.148426 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:30:16.250913 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:30:16.280293 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:30:16.289089 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:30:16.386332 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:30:16.448171 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:30:16.484143 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:30:16.554453 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:30:16.637112 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:30:16.888599 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:30:16.893532 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:30:16.903641 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:30:16.913894 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:30:16.918318 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:30:16.926217 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:30:16.931764 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:30:16.993137 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:30:16.998008 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:30:17.009386 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:30:17.014780 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:30:17.021185 (MainThread): Parsing macros/relations.sql
2022-07-30 19:30:17.028559 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:30:17.188942 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:30:17.203576 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:30:17.410227 (MainThread): Partial parsing not enabled
2022-07-30 19:30:17.672635 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:17.673074 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.749197 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:17.749651 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.778448 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:17.778887 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:17.926358 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:17.926884 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:18.701079 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:30:19.823971 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:30:19.835843 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:30:19.837185 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:30:19.848961 (MainThread): 
2022-07-30 19:30:19.850779 (MainThread): Acquiring new postgres connection "master".
2022-07-30 19:30:19.851410 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:20.030384 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.031280 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-07-30 19:30:20.858466 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.863345 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-07-30 19:30:20.916101 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.05 seconds
2022-07-30 19:30:20.916608 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-07-30 19:30:20.916936 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-07-30 19:30:20.943462 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.03 seconds
2022-07-30 19:30:21.006696 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-07-30 19:30:21.103575 (MainThread): Using postgres connection "master".
2022-07-30 19:30:21.104049 (MainThread): On master: BEGIN
2022-07-30 19:30:21.117642 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:30:21.118155 (MainThread): Using postgres connection "master".
2022-07-30 19:30:21.118971 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-07-30 19:30:21.170813 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-07-30 19:30:21.173901 (MainThread): On master: ROLLBACK
2022-07-30 19:30:21.184472 (MainThread): 19:30:21 | Concurrency: 1 threads (target='dev')
2022-07-30 19:30:21.185792 (MainThread): 19:30:21 | 
2022-07-30 19:30:21.224175 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.224829 (Thread-1): 19:30:21 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-07-30 19:30:21.225828 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.226782 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-07-30 19:30:21.227379 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.383676 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-30 19:30:21.405603 (Thread-1): finished collecting timing info
2022-07-30 19:30:21.406799 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.407220 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-07-30 19:30:21.408834 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:30:21.411070 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:21.411571 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-07-30 19:30:21.513291 (Thread-1): SQL status: SELECT 1 in 0.10 seconds
2022-07-30 19:30:21.514339 (Thread-1): finished collecting timing info
2022-07-30 19:30:21.516068 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-07-30 19:30:21.518498 (Thread-1): 19:30:21 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.29s]
2022-07-30 19:30:21.523221 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:21.533031 (MainThread): 19:30:21 | 
2022-07-30 19:30:21.534018 (MainThread): 19:30:21 | Finished running 1 test in 1.68s.
2022-07-30 19:30:21.540300 (MainThread): Connection 'master' was left open.
2022-07-30 19:30:21.540691 (MainThread): On master: Close
2022-07-30 19:30:21.551748 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-07-30 19:30:21.552232 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-07-30 19:30:21.586559 (MainThread): 
2022-07-30 19:30:21.587088 (MainThread): Completed successfully
2022-07-30 19:30:21.587569 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-07-30 19:30:21.588147 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9208421d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa920842550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa9208a93d0>]}
2022-07-30 19:30:21.588863 (MainThread): Flushing usage events
2022-07-30 19:30:35.590307 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:35.858644 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-07-30 19:30:35.859646 (MainThread): Tracking: tracking
2022-07-30 19:30:35.886044 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6a62bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6b177d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6a520d0>]}
2022-07-30 19:30:35.976539 (MainThread): Partial parsing not enabled
2022-07-30 19:30:35.982873 (MainThread): Parsing macros/core.sql
2022-07-30 19:30:36.004864 (MainThread): Parsing macros/materializations/helpers.sql
2022-07-30 19:30:36.039704 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-07-30 19:30:36.047854 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-07-30 19:30:36.191324 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-07-30 19:30:36.270875 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-07-30 19:30:36.299067 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-07-30 19:30:36.307906 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-07-30 19:30:36.400684 (MainThread): Parsing macros/materializations/common/merge.sql
2022-07-30 19:30:36.456402 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-07-30 19:30:36.479251 (MainThread): Parsing macros/materializations/view/view.sql
2022-07-30 19:30:36.506844 (MainThread): Parsing macros/materializations/table/table.sql
2022-07-30 19:30:36.536888 (MainThread): Parsing macros/adapters/common.sql
2022-07-30 19:30:36.886158 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-07-30 19:30:36.891090 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-07-30 19:30:36.900710 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-07-30 19:30:36.916512 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-07-30 19:30:36.920986 (MainThread): Parsing macros/etc/is_incremental.sql
2022-07-30 19:30:36.928908 (MainThread): Parsing macros/etc/query.sql
2022-07-30 19:30:36.935011 (MainThread): Parsing macros/etc/datetime.sql
2022-07-30 19:30:36.975510 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-07-30 19:30:36.979957 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-07-30 19:30:36.989147 (MainThread): Parsing macros/schema_tests/unique.sql
2022-07-30 19:30:36.994361 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-07-30 19:30:37.000615 (MainThread): Parsing macros/relations.sql
2022-07-30 19:30:37.007427 (MainThread): Parsing macros/adapters.sql
2022-07-30 19:30:37.082890 (MainThread): Parsing macros/catalog.sql
2022-07-30 19:30:37.094117 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-07-30 19:30:37.180138 (MainThread): Partial parsing not enabled
2022-07-30 19:30:37.314882 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:37.315433 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.389126 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:37.389596 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.418742 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:37.419279 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:37.554503 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:37.554960 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:38.351874 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-07-30 19:30:39.452185 (MainThread): scipy not found, skipping conversion test.
2022-07-30 19:30:39.466203 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-07-30 19:30:39.467581 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-07-30 19:30:39.480122 (MainThread): 
2022-07-30 19:30:39.481443 (MainThread): 19:30:39 | Concurrency: 1 threads (target='dev')
2022-07-30 19:30:39.482515 (MainThread): 19:30:39 | 
2022-07-30 19:30:39.505613 (Thread-1): Began running node model.dbt_.dim_types
2022-07-30 19:30:39.506858 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-07-30 19:30:39.507408 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.507747 (Thread-1): Compiling model.dbt_.dim_types
2022-07-30 19:30:39.703760 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-07-30 19:30:39.704835 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.705962 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.707564 (Thread-1): Finished running node model.dbt_.dim_types
2022-07-30 19:30:39.710309 (Thread-1): Began running node model.dbt_.fct_summary
2022-07-30 19:30:39.711691 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-07-30 19:30:39.712307 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.712778 (Thread-1): Compiling model.dbt_.fct_summary
2022-07-30 19:30:39.747311 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-07-30 19:30:39.748872 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.750105 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.751831 (Thread-1): Finished running node model.dbt_.fct_summary
2022-07-30 19:30:39.752577 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-07-30 19:30:39.753884 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-07-30 19:30:39.754445 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.754888 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-07-30 19:30:39.787927 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-07-30 19:30:39.789343 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.790582 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.792371 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-07-30 19:30:39.793145 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.794506 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-07-30 19:30:39.795086 (Thread-1): Opening a new connection, currently in state init
2022-07-30 19:30:39.795582 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.853904 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-07-30 19:30:39.855393 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.856661 (Thread-1): finished collecting timing info
2022-07-30 19:30:39.858461 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-07-30 19:30:39.955784 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:30:39.957263 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-07-30 19:30:40.006480 (MainThread): 19:30:40 | Done.
2022-07-30 19:30:40.068222 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-07-30 19:30:40.068823 (MainThread): Opening a new connection, currently in state init
2022-07-30 19:30:40.069229 (MainThread): 19:30:40 | Building catalog
2022-07-30 19:30:40.174512 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.176237 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-07-30 19:30:40.501583 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.502222 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-30 19:30:40.516177 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-07-30 19:30:40.518486 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:40.527329 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-30 19:30:40.952512 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.42 seconds
2022-07-30 19:30:40.994277 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-30 19:30:41.010241 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:41.010876 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-07-30 19:30:41.013479 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-07-30 19:30:41.014195 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-07-30 19:30:41.014827 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-07-30 19:30:41.024201 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-07-30 19:30:41.036002 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-07-30 19:30:41.547880 (MainThread): 19:30:41 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-07-30 19:30:41.549360 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baa790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baaa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10d6baa1d0>]}
2022-07-30 19:30:41.550070 (MainThread): Flushing usage events
2022-07-30 19:30:42.415008 (MainThread): Connection 'generate_catalog' was properly closed.
2022-07-30 19:30:42.420675 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-07-30 19:30:42.427508 (MainThread): On warehouse.information_schema: Close
2022-07-30 19:30:46.976326 (MainThread): Running with dbt=0.16.1
2022-07-30 19:30:47.251606 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-07-30 19:30:47.252508 (MainThread): Tracking: tracking
2022-07-30 19:30:47.285024 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f88682b71d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8868377d50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f886837ea10>]}
2022-07-30 19:30:47.296604 (MainThread): Serving docs at 0.0.0.0:7211
2022-07-30 19:30:47.297168 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-07-30 19:30:47.297474 (MainThread): Press Ctrl+C to exit.


2022-08-04 00:03:01.944183 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:02.328208 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 00:03:02.382009 (MainThread): Tracking: tracking
2022-08-04 00:03:02.429260 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb6c4cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb77e290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb81cf50>]}
2022-08-04 00:03:02.563951 (MainThread): Partial parsing not enabled
2022-08-04 00:03:02.616108 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:02.649188 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:02.690853 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:02.715553 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:02.844610 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:02.932877 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:02.980539 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:02.990528 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:03.104053 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:03.176162 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:03.221749 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:03.268598 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:03.329441 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:03.570640 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:03.594468 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:03.608898 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:03.635442 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:03.657954 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:03.866544 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:03.923557 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:04.056102 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:04.103737 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:04.151113 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:04.199744 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:04.268530 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:04.300049 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:04.382814 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:04.403645 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:04.534165 (MainThread): Partial parsing not enabled
2022-08-04 00:03:04.743070 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:04.743503 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:04.844029 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:04.844492 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:04.917629 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:04.918034 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:05.161299 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:05.161731 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:05.923399 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:06.813589 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:06.849097 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:06.850167 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:06.862083 (MainThread): 
2022-08-04 00:03:06.863208 (MainThread): Acquiring new postgres connection "master".
2022-08-04 00:03:06.863580 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:07.066971 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 00:03:07.068172 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 00:03:07.494186 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 00:03:07.494608 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 00:03:07.553122 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.06 seconds
2022-08-04 00:03:07.645953 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.646604 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 00:03:07.652191 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.652720 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 00:03:07.654340 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:07.654762 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:07.655025 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 00:03:07.801865 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.15 seconds
2022-08-04 00:03:07.824132 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 00:03:07.913305 (MainThread): Using postgres connection "master".
2022-08-04 00:03:07.913743 (MainThread): On master: BEGIN
2022-08-04 00:03:07.927992 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:07.928899 (MainThread): Using postgres connection "master".
2022-08-04 00:03:07.929596 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 00:03:08.052426 (MainThread): SQL status: SELECT 0 in 0.12 seconds
2022-08-04 00:03:08.059099 (MainThread): On master: ROLLBACK
2022-08-04 00:03:08.060629 (MainThread): Using postgres connection "master".
2022-08-04 00:03:08.061132 (MainThread): On master: BEGIN
2022-08-04 00:03:08.064300 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.064986 (MainThread): On master: COMMIT
2022-08-04 00:03:08.065424 (MainThread): Using postgres connection "master".
2022-08-04 00:03:08.065833 (MainThread): On master: COMMIT
2022-08-04 00:03:08.066858 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 00:03:08.067970 (MainThread): 00:03:08 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:08.069109 (MainThread): 00:03:08 | 
2022-08-04 00:03:08.110342 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 00:03:08.111076 (Thread-1): 00:03:08 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 00:03:08.112732 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.113172 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 00:03:08.113554 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 00:03:08.189503 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:08.209934 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.286875 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.287342 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 00:03:08.288710 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.296853 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.297274 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 00:03:08.298313 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.359987 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:08.379263 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.379682 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 00:03:08.380710 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.381209 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.381489 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 00:03:08.473811 (Thread-1): SQL status: SELECT 6 in 0.09 seconds
2022-08-04 00:03:08.488449 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.488873 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-04 00:03:08.492550 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.501767 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.502254 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-04 00:03:08.504889 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.511928 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 00:03:08.512736 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.513072 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 00:03:08.535935 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-08-04 00:03:08.542508 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:08.542964 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 00:03:08.558964 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-04 00:03:08.576840 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.579318 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8e3d150>]}
2022-08-04 00:03:08.580144 (Thread-1): 00:03:08 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.47s]
2022-08-04 00:03:08.592870 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 00:03:08.594024 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 00:03:08.594543 (Thread-1): 00:03:08 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-04 00:03:08.595587 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.595943 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-04 00:03:08.596221 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 00:03:08.636000 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:08.637140 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.662338 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.662794 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-04 00:03:08.664275 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.673156 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.673589 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 00:03:08.675021 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.680966 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:08.682229 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.682638 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-04 00:03:08.683501 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.683887 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.684160 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 00:03:08.755920 (Thread-1): SQL status: SELECT 922 in 0.07 seconds
2022-08-04 00:03:08.769959 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.770396 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-04 00:03:08.772193 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.781043 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.781475 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-04 00:03:08.782791 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:08.785929 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 00:03:08.786360 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.786624 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 00:03:08.792443 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-08-04 00:03:08.799295 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:08.799718 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 00:03:08.815455 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-04 00:03:08.824658 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.827065 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8e55210>]}
2022-08-04 00:03:08.827905 (Thread-1): 00:03:08 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.23s]
2022-08-04 00:03:08.828887 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 00:03:08.829413 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 00:03:08.830723 (Thread-1): 00:03:08 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-04 00:03:08.832453 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.832853 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-04 00:03:08.833145 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 00:03:08.866347 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:08.867406 (Thread-1): finished collecting timing info
2022-08-04 00:03:08.898144 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.898628 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-04 00:03:08.899822 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.908202 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.908742 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 00:03:08.909827 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 00:03:08.915005 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:08.916344 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.916771 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-04 00:03:08.917560 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:08.918167 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:08.918489 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 00:03:09.728561 (Thread-1): SQL status: SELECT 922 in 0.81 seconds
2022-08-04 00:03:09.742878 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.743300 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-04 00:03:09.746172 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:09.755271 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.755708 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-04 00:03:09.758490 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 00:03:09.761752 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 00:03:09.762187 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.762471 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 00:03:09.931206 (Thread-1): SQL status: COMMIT in 0.17 seconds
2022-08-04 00:03:09.937989 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:09.938409 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 00:03:09.973229 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-08-04 00:03:09.981935 (Thread-1): finished collecting timing info
2022-08-04 00:03:09.984272 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e7240227-6985-4167-be19-73547cc79397', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af9004e10>]}
2022-08-04 00:03:09.985188 (Thread-1): 00:03:09 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 1.15s]
2022-08-04 00:03:09.985625 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 00:03:10.034105 (MainThread): Using postgres connection "master".
2022-08-04 00:03:10.035710 (MainThread): On master: BEGIN
2022-08-04 00:03:10.038760 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:10.039232 (MainThread): On master: COMMIT
2022-08-04 00:03:10.039497 (MainThread): Using postgres connection "master".
2022-08-04 00:03:10.039740 (MainThread): On master: COMMIT
2022-08-04 00:03:10.041782 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 00:03:10.043226 (MainThread): 00:03:10 | 
2022-08-04 00:03:10.043720 (MainThread): 00:03:10 | Finished running 3 table models in 3.18s.
2022-08-04 00:03:10.044068 (MainThread): Connection 'master' was left open.
2022-08-04 00:03:10.044413 (MainThread): On master: Close
2022-08-04 00:03:10.044916 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-04 00:03:10.045236 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-04 00:03:10.086244 (MainThread): 
2022-08-04 00:03:10.087188 (MainThread): Completed successfully
2022-08-04 00:03:10.087972 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-04 00:03:10.089147 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8afb7ea450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8f47f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8af8f7b450>]}
2022-08-04 00:03:10.089822 (MainThread): Flushing usage events
2022-08-04 00:03:23.630301 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:23.915071 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-04 00:03:23.916000 (MainThread): Tracking: tracking
2022-08-04 00:03:23.933205 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d906c7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8fa3cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8fa3c10>]}
2022-08-04 00:03:24.018117 (MainThread): Partial parsing not enabled
2022-08-04 00:03:24.025272 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:24.046089 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:24.081920 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:24.090077 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:24.217661 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:24.296729 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:24.324833 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:24.333520 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:24.426159 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:24.481169 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:24.505944 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:24.537586 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:24.595352 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:24.811225 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:24.816076 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:24.825284 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:24.835488 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:24.840070 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:24.847763 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:24.852972 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:24.894457 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:24.898931 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:24.908107 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:24.913337 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:24.919496 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:24.926356 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:24.995623 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:25.005939 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:25.089902 (MainThread): Partial parsing not enabled
2022-08-04 00:03:25.223264 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:25.223696 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.296453 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:25.296938 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.326050 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:25.326459 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:25.474264 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:25.474687 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:26.249092 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:27.921233 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:27.939054 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:27.940087 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:27.960196 (MainThread): 
2022-08-04 00:03:27.979216 (MainThread): Acquiring new postgres connection "master".
2022-08-04 00:03:27.979630 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:28.241483 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.244195 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 00:03:28.743156 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.743596 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 00:03:28.766867 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.02 seconds
2022-08-04 00:03:28.767348 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 00:03:28.767619 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 00:03:28.776413 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-04 00:03:28.799316 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 00:03:28.878333 (MainThread): Using postgres connection "master".
2022-08-04 00:03:28.878777 (MainThread): On master: BEGIN
2022-08-04 00:03:28.890073 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:28.890563 (MainThread): Using postgres connection "master".
2022-08-04 00:03:28.890858 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 00:03:28.919934 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-08-04 00:03:28.922927 (MainThread): On master: ROLLBACK
2022-08-04 00:03:28.925122 (MainThread): 00:03:28 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:28.925627 (MainThread): 00:03:28 | 
2022-08-04 00:03:28.959618 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:28.961058 (Thread-1): 00:03:28 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-04 00:03:28.964072 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:28.965020 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 00:03:28.965847 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 00:03:29.130943 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 00:03:29.175378 (Thread-1): finished collecting timing info
2022-08-04 00:03:29.176727 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:29.177083 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-04 00:03:29.178428 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:29.178860 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:29.179121 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-04 00:03:29.239116 (Thread-1): SQL status: SELECT 1 in 0.06 seconds
2022-08-04 00:03:29.240090 (Thread-1): finished collecting timing info
2022-08-04 00:03:29.241132 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-04 00:03:29.242661 (Thread-1): 00:03:29 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.28s]
2022-08-04 00:03:29.243696 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:29.270418 (MainThread): 00:03:29 | 
2022-08-04 00:03:29.270932 (MainThread): 00:03:29 | Finished running 1 test in 1.29s.
2022-08-04 00:03:29.271270 (MainThread): Connection 'master' was left open.
2022-08-04 00:03:29.271534 (MainThread): On master: Close
2022-08-04 00:03:29.272234 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-04 00:03:29.272649 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-04 00:03:29.300107 (MainThread): 
2022-08-04 00:03:29.300670 (MainThread): Completed successfully
2022-08-04 00:03:29.301113 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-04 00:03:29.301680 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d668b190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8f84ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f37d8f84150>]}
2022-08-04 00:03:29.302311 (MainThread): Flushing usage events
2022-08-04 00:03:41.339147 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:41.599181 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-04 00:03:41.600077 (MainThread): Tracking: tracking
2022-08-04 00:03:41.629661 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9ffe0490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feea0043bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feea0043410>]}
2022-08-04 00:03:41.711655 (MainThread): Partial parsing not enabled
2022-08-04 00:03:41.717589 (MainThread): Parsing macros/core.sql
2022-08-04 00:03:41.738173 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 00:03:41.772808 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 00:03:41.781209 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 00:03:41.907423 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 00:03:41.984827 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 00:03:42.013036 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 00:03:42.021786 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 00:03:42.114197 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 00:03:42.168746 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 00:03:42.191603 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 00:03:42.219304 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 00:03:42.249064 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 00:03:42.425204 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 00:03:42.430102 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 00:03:42.439466 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 00:03:42.449820 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 00:03:42.454278 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 00:03:42.462039 (MainThread): Parsing macros/etc/query.sql
2022-08-04 00:03:42.467094 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 00:03:42.507273 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 00:03:42.511671 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 00:03:42.520894 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 00:03:42.526015 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 00:03:42.532109 (MainThread): Parsing macros/relations.sql
2022-08-04 00:03:42.538861 (MainThread): Parsing macros/adapters.sql
2022-08-04 00:03:42.618579 (MainThread): Parsing macros/catalog.sql
2022-08-04 00:03:42.639037 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 00:03:42.730314 (MainThread): Partial parsing not enabled
2022-08-04 00:03:42.863128 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:42.863589 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:42.933812 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:42.934224 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:42.963572 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:42.963992 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:43.099577 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:43.100002 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:43.740159 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 00:03:44.650372 (MainThread): scipy not found, skipping conversion test.
2022-08-04 00:03:44.662222 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 00:03:44.663323 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 00:03:44.675402 (MainThread): 
2022-08-04 00:03:44.676516 (MainThread): 00:03:44 | Concurrency: 1 threads (target='dev')
2022-08-04 00:03:44.677374 (MainThread): 00:03:44 | 
2022-08-04 00:03:44.703157 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 00:03:44.705440 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 00:03:44.706224 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.706938 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 00:03:44.881633 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 00:03:44.882668 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.883757 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.885336 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 00:03:44.887966 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 00:03:44.889275 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 00:03:44.889681 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.890000 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 00:03:44.923334 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 00:03:44.924589 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.925632 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.927083 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 00:03:44.927654 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 00:03:44.928854 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 00:03:44.929243 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.929532 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 00:03:44.961674 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 00:03:44.962671 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.963630 (Thread-1): finished collecting timing info
2022-08-04 00:03:44.965257 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 00:03:44.965866 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:44.967000 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 00:03:44.967378 (Thread-1): Opening a new connection, currently in state init
2022-08-04 00:03:44.967662 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 00:03:45.024061 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 00:03:45.025247 (Thread-1): finished collecting timing info
2022-08-04 00:03:45.026303 (Thread-1): finished collecting timing info
2022-08-04 00:03:45.027803 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 00:03:45.127581 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 00:03:45.128039 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 00:03:45.170008 (MainThread): 00:03:45 | Done.
2022-08-04 00:03:45.203914 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-04 00:03:45.204419 (MainThread): Opening a new connection, currently in state init
2022-08-04 00:03:45.204736 (MainThread): 00:03:45 | Building catalog
2022-08-04 00:03:45.302969 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.303702 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 00:03:45.605491 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.605917 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 00:03:45.618166 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 00:03:45.618627 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.618918 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 00:03:45.766372 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.15 seconds
2022-08-04 00:03:45.785346 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 00:03:45.810977 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.811397 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 00:03:45.813118 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 00:03:45.813555 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 00:03:45.814221 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 00:03:45.818518 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-08-04 00:03:45.825316 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 00:03:46.070469 (MainThread): 00:03:46 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-04 00:03:46.071761 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9ffd0810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9d71d090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fee9d72fd50>]}
2022-08-04 00:03:46.072867 (MainThread): Flushing usage events
2022-08-04 00:03:47.173964 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-04 00:03:47.175554 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-04 00:03:47.175875 (MainThread): On warehouse.information_schema: Close
2022-08-04 00:03:50.118338 (MainThread): Running with dbt=0.16.1
2022-08-04 00:03:50.382326 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-04 00:03:50.383206 (MainThread): Tracking: tracking
2022-08-04 00:03:50.401390 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1a98590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1b7c390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f63b1a97190>]}
2022-08-04 00:03:50.410986 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-04 00:03:50.411516 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-04 00:03:50.411801 (MainThread): Press Ctrl+C to exit.


2022-08-04 07:08:31.487621 (MainThread): Running with dbt=0.16.1
2022-08-04 07:08:31.944171 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 07:08:31.994977 (MainThread): Tracking: tracking
2022-08-04 07:08:32.063159 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feddcd0bc10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feddcdd0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7feddcebe490>]}
2022-08-04 07:08:32.187851 (MainThread): Partial parsing not enabled
2022-08-04 07:08:32.395831 (MainThread): Parsing macros/core.sql
2022-08-04 07:08:32.505497 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 07:08:32.725435 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 07:08:32.840070 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 07:08:33.132240 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 07:08:33.310127 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 07:08:33.371793 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 07:08:33.391812 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 07:08:33.570277 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 07:08:33.760833 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 07:08:33.847902 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 07:08:33.966937 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 07:08:34.083781 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 07:08:34.495880 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 07:08:34.614757 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 07:08:34.646224 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 07:08:34.700219 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 07:08:34.733574 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 07:08:34.764504 (MainThread): Parsing macros/etc/query.sql
2022-08-04 07:08:34.789411 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 07:08:34.943147 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 07:08:34.970269 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 07:08:35.005869 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 07:08:35.030412 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 07:08:35.121314 (MainThread): Parsing macros/relations.sql
2022-08-04 07:08:35.175670 (MainThread): Parsing macros/adapters.sql
2022-08-04 07:08:35.369405 (MainThread): Parsing macros/catalog.sql
2022-08-04 07:08:35.400490 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 07:08:35.581899 (MainThread): Partial parsing not enabled
2022-08-04 07:08:35.905671 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:35.908875 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:08:36.097813 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:36.098257 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:08:36.186930 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:36.191588 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:08:36.531916 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:08:36.532357 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:08:37.593917 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 07:08:38.714540 (MainThread): scipy not found, skipping conversion test.
2022-08-04 07:08:38.727333 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 07:08:38.728676 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 07:08:38.740979 (MainThread): 
2022-08-04 07:08:38.742416 (MainThread): Acquiring new postgres connection "master".
2022-08-04 07:08:38.743017 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:08:38.950451 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 07:08:38.950973 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 07:08:39.521319 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 07:08:39.522083 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 07:08:39.679303 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.16 seconds
2022-08-04 07:08:39.863207 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 07:08:39.864874 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 07:08:39.884164 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 07:08:39.888947 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 07:08:39.891762 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:39.893340 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 07:08:39.896179 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 07:08:40.201570 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.30 seconds
2022-08-04 07:08:40.226131 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 07:08:40.346678 (MainThread): Using postgres connection "master".
2022-08-04 07:08:40.347465 (MainThread): On master: BEGIN
2022-08-04 07:08:40.360068 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 07:08:40.360581 (MainThread): Using postgres connection "master".
2022-08-04 07:08:40.360875 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 07:08:40.577817 (MainThread): SQL status: SELECT 0 in 0.22 seconds
2022-08-04 07:08:40.585666 (MainThread): On master: ROLLBACK
2022-08-04 07:08:40.594919 (MainThread): Using postgres connection "master".
2022-08-04 07:08:40.595430 (MainThread): On master: BEGIN
2022-08-04 07:08:40.600252 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:40.600790 (MainThread): On master: COMMIT
2022-08-04 07:08:40.601121 (MainThread): Using postgres connection "master".
2022-08-04 07:08:40.601769 (MainThread): On master: COMMIT
2022-08-04 07:08:40.603938 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 07:08:40.611784 (MainThread): 07:08:40 | Concurrency: 1 threads (target='dev')
2022-08-04 07:08:40.612847 (MainThread): 07:08:40 | 
2022-08-04 07:08:40.697436 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 07:08:40.698112 (Thread-1): 07:08:40 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 07:08:40.699100 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:40.699533 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 07:08:40.699868 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 07:08:40.994073 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 07:08:41.105878 (Thread-1): finished collecting timing info
2022-08-04 07:08:41.280600 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:41.284476 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 07:08:41.288373 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:41.306052 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:41.306505 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 07:08:41.307681 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:41.452855 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 07:08:41.580097 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:41.580534 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 07:08:41.581641 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:41.582200 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:41.582480 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 07:08:41.937956 (Thread-1): SQL status: SELECT 6 in 0.36 seconds
2022-08-04 07:08:41.975804 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:41.980388 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-04 07:08:41.981925 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 07:08:42.004260 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:42.004728 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-04 07:08:42.006046 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 07:08:42.009236 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 07:08:42.009683 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:42.009958 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 07:08:42.030378 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-08-04 07:08:42.049285 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 07:08:42.049780 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 07:08:42.097668 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2022-08-04 07:08:42.120367 (Thread-1): finished collecting timing info
2022-08-04 07:08:42.125366 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b263f3-6546-40b2-a22d-82db312e5a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda56d8d0>]}
2022-08-04 07:08:42.126254 (Thread-1): 07:08:42 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.43s]
2022-08-04 07:08:42.126742 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 07:08:42.136304 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 07:08:42.138943 (Thread-1): 07:08:42 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-04 07:08:42.143095 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.145096 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-04 07:08:42.145483 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 07:08:42.230070 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 07:08:42.249141 (Thread-1): finished collecting timing info
2022-08-04 07:08:42.299386 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.299867 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-04 07:08:42.303713 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:42.317947 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.318411 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 07:08:42.323633 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:42.338617 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-04 07:08:42.367632 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.368055 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-04 07:08:42.368846 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:42.369284 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.369557 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 07:08:42.466350 (Thread-1): SQL status: SELECT 922 in 0.10 seconds
2022-08-04 07:08:42.481490 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.481936 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-04 07:08:42.485767 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 07:08:42.495950 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.496398 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-04 07:08:42.498097 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 07:08:42.501497 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 07:08:42.501951 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.502237 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 07:08:42.563011 (Thread-1): SQL status: COMMIT in 0.06 seconds
2022-08-04 07:08:42.572836 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 07:08:42.573342 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 07:08:42.609500 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-08-04 07:08:42.619576 (Thread-1): finished collecting timing info
2022-08-04 07:08:42.623791 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b263f3-6546-40b2-a22d-82db312e5a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda554510>]}
2022-08-04 07:08:42.624675 (Thread-1): 07:08:42 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.48s]
2022-08-04 07:08:42.625156 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 07:08:42.625806 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 07:08:42.627453 (Thread-1): 07:08:42 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-04 07:08:42.628606 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:42.628976 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-04 07:08:42.629261 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 07:08:42.711116 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 07:08:42.751723 (Thread-1): finished collecting timing info
2022-08-04 07:08:42.827099 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:42.830659 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-04 07:08:42.835491 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:42.846703 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:42.847357 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 07:08:42.851463 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 07:08:42.868806 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-04 07:08:42.887083 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:42.887573 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-04 07:08:42.890391 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:42.890896 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:42.891179 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 07:08:45.101858 (Thread-1): SQL status: SELECT 922 in 2.21 seconds
2022-08-04 07:08:45.188483 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:45.188945 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-04 07:08:45.199581 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-04 07:08:45.235860 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:45.236312 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-04 07:08:45.247139 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-04 07:08:45.252104 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 07:08:45.252555 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:45.252833 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 07:08:45.901977 (Thread-1): SQL status: COMMIT in 0.65 seconds
2022-08-04 07:08:45.913367 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:08:45.913821 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 07:08:45.987642 (Thread-1): SQL status: DROP TABLE in 0.07 seconds
2022-08-04 07:08:46.006883 (Thread-1): finished collecting timing info
2022-08-04 07:08:46.041881 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '99b263f3-6546-40b2-a22d-82db312e5a65', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda66a310>]}
2022-08-04 07:08:46.043689 (Thread-1): 07:08:46 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 3.41s]
2022-08-04 07:08:46.048148 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 07:08:46.107938 (MainThread): Using postgres connection "master".
2022-08-04 07:08:46.108456 (MainThread): On master: BEGIN
2022-08-04 07:08:46.109356 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:08:46.109800 (MainThread): On master: COMMIT
2022-08-04 07:08:46.110082 (MainThread): Using postgres connection "master".
2022-08-04 07:08:46.110336 (MainThread): On master: COMMIT
2022-08-04 07:08:46.111252 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 07:08:46.116151 (MainThread): 07:08:46 | 
2022-08-04 07:08:46.119145 (MainThread): 07:08:46 | Finished running 3 table models in 7.37s.
2022-08-04 07:08:46.120193 (MainThread): Connection 'master' was left open.
2022-08-04 07:08:46.120568 (MainThread): On master: Close
2022-08-04 07:08:46.121072 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-04 07:08:46.121409 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-04 07:08:46.230513 (MainThread): 
2022-08-04 07:08:46.231680 (MainThread): Completed successfully
2022-08-04 07:08:46.233031 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-04 07:08:46.234298 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda541450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda5410d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fedda5f1550>]}
2022-08-04 07:08:46.235013 (MainThread): Flushing usage events
2022-08-04 07:09:09.795226 (MainThread): Running with dbt=0.16.1
2022-08-04 07:09:10.097972 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-04 07:09:10.098918 (MainThread): Tracking: tracking
2022-08-04 07:09:10.115708 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056d30cfd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056d3392d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056d3394d0>]}
2022-08-04 07:09:10.202712 (MainThread): Partial parsing not enabled
2022-08-04 07:09:10.209011 (MainThread): Parsing macros/core.sql
2022-08-04 07:09:10.230023 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 07:09:10.272614 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 07:09:10.287017 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 07:09:10.424555 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 07:09:10.526214 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 07:09:10.579645 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 07:09:10.593528 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 07:09:10.729356 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 07:09:10.815792 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 07:09:10.844415 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 07:09:10.877415 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 07:09:10.917408 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 07:09:11.116536 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 07:09:11.121423 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 07:09:11.131970 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 07:09:11.146647 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 07:09:11.151071 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 07:09:11.158873 (MainThread): Parsing macros/etc/query.sql
2022-08-04 07:09:11.164260 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 07:09:11.204766 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 07:09:11.210037 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 07:09:11.219533 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 07:09:11.224956 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 07:09:11.231416 (MainThread): Parsing macros/relations.sql
2022-08-04 07:09:11.238518 (MainThread): Parsing macros/adapters.sql
2022-08-04 07:09:11.316441 (MainThread): Parsing macros/catalog.sql
2022-08-04 07:09:11.326956 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 07:09:11.414318 (MainThread): Partial parsing not enabled
2022-08-04 07:09:11.555762 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 07:09:11.556212 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:11.649739 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 07:09:11.650200 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:11.709834 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:09:11.710284 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:11.971514 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:11.972140 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:12.624526 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 07:09:14.459091 (MainThread): scipy not found, skipping conversion test.
2022-08-04 07:09:14.486121 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 07:09:14.489402 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 07:09:14.512795 (MainThread): 
2022-08-04 07:09:14.516144 (MainThread): Acquiring new postgres connection "master".
2022-08-04 07:09:14.516599 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:14.887923 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 07:09:14.888415 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 07:09:15.733251 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 07:09:15.733740 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 07:09:15.747633 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 07:09:15.748107 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 07:09:15.748379 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 07:09:15.762558 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-04 07:09:15.789973 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 07:09:15.924312 (MainThread): Using postgres connection "master".
2022-08-04 07:09:15.925535 (MainThread): On master: BEGIN
2022-08-04 07:09:15.959233 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-04 07:09:15.959870 (MainThread): Using postgres connection "master".
2022-08-04 07:09:15.960168 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 07:09:16.015893 (MainThread): SQL status: SELECT 0 in 0.06 seconds
2022-08-04 07:09:16.025016 (MainThread): On master: ROLLBACK
2022-08-04 07:09:16.028229 (MainThread): 07:09:16 | Concurrency: 1 threads (target='dev')
2022-08-04 07:09:16.029399 (MainThread): 07:09:16 | 
2022-08-04 07:09:16.131080 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 07:09:16.131884 (Thread-1): 07:09:16 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-04 07:09:16.133394 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:16.133797 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 07:09:16.134628 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 07:09:16.354101 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 07:09:16.374183 (Thread-1): finished collecting timing info
2022-08-04 07:09:16.375881 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:16.376306 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-04 07:09:16.378130 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 07:09:16.378621 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:16.378906 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-04 07:09:16.464258 (Thread-1): SQL status: SELECT 1 in 0.08 seconds
2022-08-04 07:09:16.465293 (Thread-1): finished collecting timing info
2022-08-04 07:09:16.466284 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-04 07:09:16.476746 (Thread-1): 07:09:16 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.34s]
2022-08-04 07:09:16.483790 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 07:09:16.530003 (MainThread): 07:09:16 | 
2022-08-04 07:09:16.530725 (MainThread): 07:09:16 | Finished running 1 test in 2.01s.
2022-08-04 07:09:16.531133 (MainThread): Connection 'master' was left open.
2022-08-04 07:09:16.531515 (MainThread): On master: Close
2022-08-04 07:09:16.532202 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-04 07:09:16.532639 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-04 07:09:16.565364 (MainThread): 
2022-08-04 07:09:16.566111 (MainThread): Completed successfully
2022-08-04 07:09:16.574478 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-04 07:09:16.575109 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056a964250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056a964d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f056a99aa90>]}
2022-08-04 07:09:16.575842 (MainThread): Flushing usage events
2022-08-04 07:09:34.793833 (MainThread): Running with dbt=0.16.1
2022-08-04 07:09:35.327977 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-04 07:09:35.329214 (MainThread): Tracking: tracking
2022-08-04 07:09:35.359522 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc044df73d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc044f1b250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc044e050d0>]}
2022-08-04 07:09:35.458837 (MainThread): Partial parsing not enabled
2022-08-04 07:09:35.466027 (MainThread): Parsing macros/core.sql
2022-08-04 07:09:35.498986 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 07:09:35.534931 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 07:09:35.543402 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 07:09:35.674328 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 07:09:35.755618 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 07:09:35.784369 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 07:09:35.793403 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 07:09:35.891600 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 07:09:35.989953 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 07:09:36.034130 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 07:09:36.082083 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 07:09:36.119014 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 07:09:36.299386 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 07:09:36.304715 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 07:09:36.314041 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 07:09:36.324478 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 07:09:36.329235 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 07:09:36.337599 (MainThread): Parsing macros/etc/query.sql
2022-08-04 07:09:36.342869 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 07:09:36.384295 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 07:09:36.389342 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 07:09:36.399066 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 07:09:36.404771 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 07:09:36.413521 (MainThread): Parsing macros/relations.sql
2022-08-04 07:09:36.420923 (MainThread): Parsing macros/adapters.sql
2022-08-04 07:09:36.513447 (MainThread): Parsing macros/catalog.sql
2022-08-04 07:09:36.524468 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 07:09:36.614544 (MainThread): Partial parsing not enabled
2022-08-04 07:09:36.753022 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 07:09:36.753684 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:36.834393 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 07:09:36.835023 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:36.864266 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:09:36.864887 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:37.015383 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:37.016004 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:37.683472 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 07:09:39.127745 (MainThread): scipy not found, skipping conversion test.
2022-08-04 07:09:39.142540 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 07:09:39.144389 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 07:09:39.157765 (MainThread): 
2022-08-04 07:09:39.158596 (MainThread): 07:09:39 | Concurrency: 1 threads (target='dev')
2022-08-04 07:09:39.159184 (MainThread): 07:09:39 | 
2022-08-04 07:09:39.194666 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 07:09:39.196043 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 07:09:39.196901 (Thread-1): Opening a new connection, currently in state init
2022-08-04 07:09:39.206552 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 07:09:39.524077 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 07:09:39.526059 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.527494 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.529256 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 07:09:39.541210 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 07:09:39.542383 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 07:09:39.542896 (Thread-1): Opening a new connection, currently in state init
2022-08-04 07:09:39.543242 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 07:09:39.640705 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 07:09:39.641809 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.655518 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.657322 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 07:09:39.657898 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 07:09:39.658843 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 07:09:39.659248 (Thread-1): Opening a new connection, currently in state init
2022-08-04 07:09:39.659870 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 07:09:39.727953 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 07:09:39.728993 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.730089 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.736043 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 07:09:39.736661 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 07:09:39.737615 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 07:09:39.737998 (Thread-1): Opening a new connection, currently in state init
2022-08-04 07:09:39.738295 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 07:09:39.855492 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 07:09:39.856887 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.858181 (Thread-1): finished collecting timing info
2022-08-04 07:09:39.859991 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 07:09:39.871017 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 07:09:39.871553 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 07:09:39.939911 (MainThread): 07:09:39 | Done.
2022-08-04 07:09:40.209638 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-04 07:09:40.210074 (MainThread): Opening a new connection, currently in state init
2022-08-04 07:09:40.215005 (MainThread): 07:09:40 | Building catalog
2022-08-04 07:09:40.464645 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-04 07:09:40.465468 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 07:09:41.356317 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 07:09:41.357773 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 07:09:41.386319 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.03 seconds
2022-08-04 07:09:41.386811 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 07:09:41.387096 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 07:09:42.203634 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.82 seconds
2022-08-04 07:09:42.262746 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 07:09:42.301419 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 07:09:42.301866 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 07:09:42.310355 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 07:09:42.310847 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 07:09:42.311138 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 07:09:42.317272 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-08-04 07:09:42.329751 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 07:09:42.943016 (MainThread): 07:09:42 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-04 07:09:42.952205 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc042493210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc044e510d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc0424ca8d0>]}
2022-08-04 07:09:42.952939 (MainThread): Flushing usage events
2022-08-04 07:09:43.913882 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-04 07:09:43.915968 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-04 07:09:43.917098 (MainThread): On warehouse.information_schema: Close
2022-08-04 07:09:50.195517 (MainThread): Running with dbt=0.16.1
2022-08-04 07:09:50.634938 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-04 07:09:50.640631 (MainThread): Tracking: tracking
2022-08-04 07:09:50.673716 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43fb801810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43fd4e0e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f43fb920910>]}
2022-08-04 07:09:50.695962 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-04 07:09:50.696977 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-04 07:09:50.697763 (MainThread): Press Ctrl+C to exit.


2022-08-04 12:21:12.274620 (MainThread): Running with dbt=0.16.1
2022-08-04 12:21:13.786238 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 12:21:13.846707 (MainThread): Tracking: tracking
2022-08-04 12:21:12.261468 (MainThread): Running with dbt=0.16.1
2022-08-04 12:21:13.933279 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 12:21:13.934320 (MainThread): Tracking: tracking
2022-08-04 12:21:14.530408 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645b5f6790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645b5ffe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645b5ff250>]}
2022-08-04 12:21:14.530385 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fa060990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fa04b910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39fa0468d0>]}
2022-08-04 12:21:14.795464 (MainThread): Partial parsing not enabled
2022-08-04 12:21:14.795510 (MainThread): Partial parsing not enabled
2022-08-04 12:21:14.972767 (MainThread): Parsing macros/core.sql
2022-08-04 12:21:14.972806 (MainThread): Parsing macros/core.sql
2022-08-04 12:21:15.116817 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 12:21:15.116876 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 12:21:15.225019 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 12:21:15.225170 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 12:21:15.253656 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 12:21:15.256779 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 12:21:15.704594 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 12:21:15.706510 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 12:21:15.910878 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 12:21:15.952239 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 12:21:16.014510 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 12:21:16.043731 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 12:21:16.045727 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 12:21:16.074724 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 12:21:16.347864 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 12:21:16.353195 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 12:21:16.506051 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 12:21:16.547426 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 12:21:16.606185 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 12:21:16.619177 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 12:21:16.701420 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 12:21:16.701592 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 12:21:16.806186 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 12:21:16.806638 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 12:21:17.420303 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 12:21:17.449056 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 12:21:17.466346 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 12:21:17.479880 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 12:21:17.489620 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 12:21:17.529803 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 12:21:17.536377 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 12:21:17.557425 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 12:21:17.581748 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 12:21:17.597376 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 12:21:17.609727 (MainThread): Parsing macros/etc/query.sql
2022-08-04 12:21:17.623656 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 12:21:17.634442 (MainThread): Parsing macros/etc/query.sql
2022-08-04 12:21:17.640363 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 12:21:17.754294 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 12:21:17.769294 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 12:21:17.780301 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 12:21:17.788103 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 12:21:17.817983 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 12:21:17.826540 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 12:21:17.853880 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 12:21:17.854194 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 12:21:17.911990 (MainThread): Parsing macros/relations.sql
2022-08-04 12:21:17.918720 (MainThread): Parsing macros/relations.sql
2022-08-04 12:21:17.986501 (MainThread): Parsing macros/adapters.sql
2022-08-04 12:21:17.993876 (MainThread): Parsing macros/adapters.sql
2022-08-04 12:21:18.091594 (MainThread): Parsing macros/catalog.sql
2022-08-04 12:21:18.112576 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 12:21:18.256090 (MainThread): Parsing macros/catalog.sql
2022-08-04 12:21:18.291358 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 12:21:18.383080 (MainThread): Partial parsing not enabled
2022-08-04 12:21:18.548934 (MainThread): Partial parsing not enabled
2022-08-04 12:21:18.897202 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:18.897662 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.154588 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:19.155060 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.194982 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:21:19.195467 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.311967 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:21:19.312427 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.326649 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:21:19.329757 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.411818 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:21:19.412279 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.840373 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:21:19.844813 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:19.909494 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:21:19.918253 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:21.680571 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 12:21:21.753310 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 12:21:24.492976 (MainThread): scipy not found, skipping conversion test.
2022-08-04 12:21:24.548876 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 12:21:24.550090 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 12:21:24.593457 (MainThread): 
2022-08-04 12:21:24.599695 (MainThread): Acquiring new postgres connection "master".
2022-08-04 12:21:24.600148 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:24.833271 (MainThread): scipy not found, skipping conversion test.
2022-08-04 12:21:24.868535 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 12:21:24.874668 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 12:21:24.909565 (MainThread): 
2022-08-04 12:21:24.918977 (MainThread): Acquiring new postgres connection "master".
2022-08-04 12:21:24.919423 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:21:25.163496 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 12:21:25.163962 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 12:21:25.362413 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 12:21:25.368270 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 12:21:26.365755 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 12:21:26.367054 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 12:21:26.445255 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 12:21:26.445760 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 12:21:26.464109 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.02 seconds
2022-08-04 12:21:26.466315 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.10 seconds
2022-08-04 12:21:26.504221 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.504697 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 12:21:26.504964 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-08-04 12:21:26.503205 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.506636 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 12:21:26.507001 (ThreadPoolExecutor-0_0): Creating schema "warehouse"."warehouse".
2022-08-04 12:21:26.517235 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.517694 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-08-04 12:21:26.522681 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:26.523172 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.523452 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-08-04 12:21:26.525105 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.525542 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: BEGIN
2022-08-04 12:21:26.527131 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:26.527612 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.527897 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "create_warehouse_warehouse"} */
create schema if not exists "warehouse"
2022-08-04 12:21:26.544018 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.02 seconds
2022-08-04 12:21:26.547107 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-08-04 12:21:26.547824 (ThreadPoolExecutor-0_0): Using postgres connection "create_warehouse_warehouse".
2022-08-04 12:21:26.548144 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: COMMIT
2022-08-04 12:21:26.570451 (ThreadPoolExecutor-0_0): Postgres error: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.

2022-08-04 12:21:26.570935 (ThreadPoolExecutor-0_0): On create_warehouse_warehouse: ROLLBACK
2022-08-04 12:21:26.572222 (ThreadPoolExecutor-0_0): Error running SQL: macro create_schema
2022-08-04 12:21:26.572644 (ThreadPoolExecutor-0_0): Rolling back transaction.
2022-08-04 12:21:26.574467 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.03 seconds
2022-08-04 12:21:26.574636 (MainThread): Connection 'master' was properly closed.
2022-08-04 12:21:26.575084 (MainThread): Connection 'create_warehouse_warehouse' was left open.
2022-08-04 12:21:26.575359 (MainThread): On create_warehouse_warehouse: Close
2022-08-04 12:21:26.576381 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39e9779290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39efad05d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f39f773c5d0>]}
2022-08-04 12:21:26.577138 (MainThread): Flushing usage events
2022-08-04 12:21:26.714535 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 12:21:26.716502 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly create_warehouse_warehouse).
2022-08-04 12:21:26.724980 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:21:26.734660 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 12:21:26.737773 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:26.738302 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:21:26.738651 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 12:21:27.216175 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 0.48 seconds
2022-08-04 12:21:27.221047 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 12:21:27.380383 (MainThread): Using postgres connection "master".
2022-08-04 12:21:27.380836 (MainThread): On master: BEGIN
2022-08-04 12:21:27.399691 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-04 12:21:27.400192 (MainThread): Using postgres connection "master".
2022-08-04 12:21:27.400509 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 12:21:27.575916 (MainThread): SQL status: SELECT 0 in 0.18 seconds
2022-08-04 12:21:27.578764 (MainThread): On master: ROLLBACK
2022-08-04 12:21:27.579724 (MainThread): Using postgres connection "master".
2022-08-04 12:21:27.580128 (MainThread): On master: BEGIN
2022-08-04 12:21:27.584018 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:27.584539 (MainThread): On master: COMMIT
2022-08-04 12:21:27.584832 (MainThread): Using postgres connection "master".
2022-08-04 12:21:27.585100 (MainThread): On master: COMMIT
2022-08-04 12:21:27.591702 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-04 12:21:27.592909 (MainThread): 12:21:27 | Concurrency: 1 threads (target='dev')
2022-08-04 12:21:27.594117 (MainThread): 12:21:27 | 
2022-08-04 12:21:27.660557 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 12:21:27.661281 (Thread-1): 12:21:27 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 12:21:27.662405 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:27.662782 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 12:21:27.663107 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 12:21:27.840656 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 12:21:27.914843 (Thread-1): finished collecting timing info
2022-08-04 12:21:28.092146 (MainThread): Encountered an error:
2022-08-04 12:21:28.110693 (MainThread): Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.
2022-08-04 12:21:28.105067 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:28.112603 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 12:21:28.113904 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:21:28.129054 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:28.137590 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 12:21:28.138824 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:21:28.255802 (MainThread): Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UniqueViolation: duplicate key value violates unique constraint "pg_namespace_nspname_index"
DETAIL:  Key (nspname)=(warehouse) already exists.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 81, in main
    results, succeeded = handle_and_check(args)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 159, in handle_and_check
    task, res = run_from_args(parsed)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/main.py", line 212, in run_from_args
    results = task.run()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 371, in run
    result = self.execute_with_hooks(selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 331, in execute_with_hooks
    self.before_run(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/run.py", line 199, in before_run
    self.create_schemas(adapter, selected_uids)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 472, in create_schemas
    create_future.result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/usr/local/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.7/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/task/runnable.py", line 440, in create_schema
    adapter.create_schema(db, schema)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/impl.py", line 183, in create_schema
    self.execute_macro(CREATE_SCHEMA_MACRO_NAME, kwargs=kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 952, in execute_macro
    result = macro_function(**kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 163, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 97, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 73, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error
  duplicate key value violates unique constraint "pg_namespace_nspname_index"
  DETAIL:  Key (nspname)=(warehouse) already exists.

2022-08-04 12:21:28.329174 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 12:21:28.368244 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:28.368682 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 12:21:28.372157 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:28.372662 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:21:28.372961 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 12:21:28.374810 (Thread-1): Postgres error: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^

2022-08-04 12:21:28.375262 (Thread-1): On model.dbt_.dim_types: ROLLBACK
2022-08-04 12:21:28.379820 (Thread-1): finished collecting timing info
2022-08-04 12:21:28.390711 (Thread-1): Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "warehouse.source" does not exist
LINE 8:     select * from warehouse.warehouse.source
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model dim_types (models/traffic_models/dim_types.sql)
  relation "warehouse.source" does not exist
  LINE 8:     select * from warehouse.warehouse.source
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-08-04 12:21:28.424694 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1db3f016-2488-45de-b496-5166f8fdaea7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6458d692d0>]}
2022-08-04 12:21:28.425568 (Thread-1): 12:21:28 | 1 of 3 ERROR creating table model warehouse.dim_types................ [ERROR in 0.76s]
2022-08-04 12:21:28.426572 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 12:21:28.434731 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 12:21:28.435336 (Thread-1): 12:21:28 | 2 of 3 SKIP relation warehouse.fct_summary........................... [SKIP]
2022-08-04 12:21:28.436222 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 12:21:28.436772 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 12:21:28.437213 (Thread-1): 12:21:28 | 3 of 3 SKIP relation warehouse.fct_trajectory........................ [SKIP]
2022-08-04 12:21:28.441107 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 12:21:28.539435 (MainThread): Using postgres connection "master".
2022-08-04 12:21:28.539913 (MainThread): On master: BEGIN
2022-08-04 12:21:28.540727 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:21:28.541201 (MainThread): On master: COMMIT
2022-08-04 12:21:28.541494 (MainThread): Using postgres connection "master".
2022-08-04 12:21:28.541761 (MainThread): On master: COMMIT
2022-08-04 12:21:28.545206 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 12:21:28.546533 (MainThread): 12:21:28 | 
2022-08-04 12:21:28.547492 (MainThread): 12:21:28 | Finished running 3 table models in 3.95s.
2022-08-04 12:21:28.553715 (MainThread): Connection 'master' was left open.
2022-08-04 12:21:28.554239 (MainThread): On master: Close
2022-08-04 12:21:28.562908 (MainThread): Connection 'model.dbt_.dim_types' was left open.
2022-08-04 12:21:28.567236 (MainThread): On model.dbt_.dim_types: Close
2022-08-04 12:21:28.677244 (MainThread): 
2022-08-04 12:21:28.686322 (MainThread): Completed with 1 error and 0 warnings:
2022-08-04 12:21:28.687443 (MainThread): 
2022-08-04 12:21:28.688308 (MainThread): Database Error in model dim_types (models/traffic_models/dim_types.sql)
2022-08-04 12:21:28.689085 (MainThread):   relation "warehouse.source" does not exist
2022-08-04 12:21:28.689806 (MainThread):   LINE 8:     select * from warehouse.warehouse.source
2022-08-04 12:21:28.690593 (MainThread):                             ^
2022-08-04 12:21:28.698628 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/dim_types.sql
2022-08-04 12:21:28.699495 (MainThread): 
Done. PASS=2 WARN=0 ERROR=1 SKIP=0 TOTAL=3
2022-08-04 12:21:28.702059 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6458e5e190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f645b708690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6458de35d0>]}
2022-08-04 12:21:28.702793 (MainThread): Flushing usage events
2022-08-04 12:24:28.616491 (MainThread): Running with dbt=0.16.1
2022-08-04 12:24:28.907332 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 12:24:28.908554 (MainThread): Tracking: tracking
2022-08-04 12:24:28.929669 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fc0306e90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fc03099d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fc0440610>]}
2022-08-04 12:24:29.016283 (MainThread): Partial parsing not enabled
2022-08-04 12:24:29.022602 (MainThread): Parsing macros/core.sql
2022-08-04 12:24:29.043815 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 12:24:29.079184 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 12:24:29.087420 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 12:24:29.227639 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 12:24:29.328164 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 12:24:29.357090 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 12:24:29.366415 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 12:24:29.470782 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 12:24:29.525266 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 12:24:29.548540 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 12:24:29.582807 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 12:24:29.614103 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 12:24:29.882340 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 12:24:29.887956 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 12:24:29.897451 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 12:24:29.908415 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 12:24:29.913127 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 12:24:29.923218 (MainThread): Parsing macros/etc/query.sql
2022-08-04 12:24:29.928613 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 12:24:29.974192 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 12:24:29.979078 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 12:24:29.989001 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 12:24:29.994449 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 12:24:30.000850 (MainThread): Parsing macros/relations.sql
2022-08-04 12:24:30.008405 (MainThread): Parsing macros/adapters.sql
2022-08-04 12:24:30.086747 (MainThread): Parsing macros/catalog.sql
2022-08-04 12:24:30.097827 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 12:24:30.189344 (MainThread): Partial parsing not enabled
2022-08-04 12:24:30.339632 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:30.340268 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:30.421655 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:30.422333 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:30.452508 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:30.453131 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:30.630349 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:24:30.630799 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:31.284835 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 12:24:32.955254 (MainThread): scipy not found, skipping conversion test.
2022-08-04 12:24:32.983932 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 12:24:32.991376 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 12:24:33.023099 (MainThread): 
2022-08-04 12:24:33.025495 (MainThread): Acquiring new postgres connection "master".
2022-08-04 12:24:33.026327 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:33.391357 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 12:24:33.391859 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 12:24:34.027993 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 12:24:34.028625 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 12:24:34.047870 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-08-04 12:24:34.138193 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 12:24:34.138836 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 12:24:34.144538 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:24:34.144992 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 12:24:34.146972 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:24:34.147453 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:24:34.147742 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 12:24:34.156717 (ThreadPoolExecutor-1_0): SQL status: SELECT 1 in 0.01 seconds
2022-08-04 12:24:34.162530 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 12:24:34.245248 (MainThread): Using postgres connection "master".
2022-08-04 12:24:34.245708 (MainThread): On master: BEGIN
2022-08-04 12:24:34.256845 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 12:24:34.257364 (MainThread): Using postgres connection "master".
2022-08-04 12:24:34.257685 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 12:24:34.272558 (MainThread): SQL status: SELECT 0 in 0.01 seconds
2022-08-04 12:24:34.275624 (MainThread): On master: ROLLBACK
2022-08-04 12:24:34.276848 (MainThread): Using postgres connection "master".
2022-08-04 12:24:34.277346 (MainThread): On master: BEGIN
2022-08-04 12:24:34.278798 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:24:34.279286 (MainThread): On master: COMMIT
2022-08-04 12:24:34.279597 (MainThread): Using postgres connection "master".
2022-08-04 12:24:34.279868 (MainThread): On master: COMMIT
2022-08-04 12:24:34.281219 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 12:24:34.282381 (MainThread): 12:24:34 | Concurrency: 1 threads (target='dev')
2022-08-04 12:24:34.284628 (MainThread): 12:24:34 | 
2022-08-04 12:24:34.310054 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 12:24:34.310701 (Thread-1): 12:24:34 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 12:24:34.311699 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.312069 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 12:24:34.312407 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 12:24:34.413827 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 12:24:34.414968 (Thread-1): finished collecting timing info
2022-08-04 12:24:34.510825 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.511760 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 12:24:34.513206 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:34.521532 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.522270 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 12:24:34.523732 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:34.587387 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 12:24:34.588729 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.589146 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 12:24:34.594069 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:24:34.594624 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.594925 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 12:24:34.743743 (Thread-1): SQL status: SELECT 6 in 0.15 seconds
2022-08-04 12:24:34.768783 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.769254 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-04 12:24:34.770596 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 12:24:34.783402 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 12:24:34.783859 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.784143 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 12:24:34.808998 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-08-04 12:24:34.826766 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:34.827250 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 12:24:34.835039 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-04 12:24:34.862803 (Thread-1): finished collecting timing info
2022-08-04 12:24:34.865310 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd17c78a-8835-4617-96e0-f56ca57dbcd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fafa42f90>]}
2022-08-04 12:24:34.874994 (Thread-1): 12:24:34 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.55s]
2022-08-04 12:24:34.875566 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 12:24:34.878627 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 12:24:34.879291 (Thread-1): 12:24:34 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-04 12:24:34.880252 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:34.880611 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-04 12:24:34.880917 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 12:24:34.995257 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 12:24:34.998218 (Thread-1): finished collecting timing info
2022-08-04 12:24:35.091169 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.091672 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-04 12:24:35.092720 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:35.119495 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.119961 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 12:24:35.120943 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:35.139164 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-04 12:24:35.142394 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.142887 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-04 12:24:35.147852 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:24:35.148492 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.154772 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 12:24:35.216136 (Thread-1): SQL status: SELECT 922 in 0.06 seconds
2022-08-04 12:24:35.228886 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.229361 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-04 12:24:35.234261 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 12:24:35.237353 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 12:24:35.237795 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.238194 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 12:24:35.253336 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-08-04 12:24:35.259881 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:35.260333 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 12:24:35.266353 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-04 12:24:35.285793 (Thread-1): finished collecting timing info
2022-08-04 12:24:35.292482 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd17c78a-8835-4617-96e0-f56ca57dbcd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fbdb00950>]}
2022-08-04 12:24:35.293341 (Thread-1): 12:24:35 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.41s]
2022-08-04 12:24:35.293810 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 12:24:35.294318 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 12:24:35.294769 (Thread-1): 12:24:35 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-04 12:24:35.295705 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:35.296070 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-04 12:24:35.314662 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 12:24:35.400461 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 12:24:35.401738 (Thread-1): finished collecting timing info
2022-08-04 12:24:35.468214 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:35.468697 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-04 12:24:35.478378 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-04 12:24:35.499154 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:35.499612 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 12:24:35.502417 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:35.508101 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-04 12:24:35.518455 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:35.518952 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-04 12:24:35.526533 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-04 12:24:35.527123 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:35.527431 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 12:24:37.209651 (Thread-1): SQL status: SELECT 922 in 1.68 seconds
2022-08-04 12:24:37.223810 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:37.224270 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-04 12:24:37.226113 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 12:24:37.229165 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 12:24:37.229602 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:37.229881 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 12:24:37.632019 (Thread-1): SQL status: COMMIT in 0.40 seconds
2022-08-04 12:24:37.650141 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:37.650606 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 12:24:37.651671 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 12:24:37.665921 (Thread-1): finished collecting timing info
2022-08-04 12:24:37.672788 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bd17c78a-8835-4617-96e0-f56ca57dbcd8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fc0385450>]}
2022-08-04 12:24:37.675862 (Thread-1): 12:24:37 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 2.38s]
2022-08-04 12:24:37.676419 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 12:24:37.782468 (MainThread): Using postgres connection "master".
2022-08-04 12:24:37.783105 (MainThread): On master: BEGIN
2022-08-04 12:24:37.786958 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:24:37.787479 (MainThread): On master: COMMIT
2022-08-04 12:24:37.787840 (MainThread): Using postgres connection "master".
2022-08-04 12:24:37.788149 (MainThread): On master: COMMIT
2022-08-04 12:24:37.794396 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-04 12:24:37.795924 (MainThread): 12:24:37 | 
2022-08-04 12:24:37.796414 (MainThread): 12:24:37 | Finished running 3 table models in 4.77s.
2022-08-04 12:24:37.796763 (MainThread): Connection 'master' was left open.
2022-08-04 12:24:37.797043 (MainThread): On master: Close
2022-08-04 12:24:37.822532 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-04 12:24:37.830753 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-04 12:24:37.968732 (MainThread): 
2022-08-04 12:24:37.974281 (MainThread): Completed successfully
2022-08-04 12:24:37.977052 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-04 12:24:37.981503 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fc03db150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fbdbf83d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5fbdae4310>]}
2022-08-04 12:24:37.982808 (MainThread): Flushing usage events
2022-08-04 12:24:56.453482 (MainThread): Running with dbt=0.16.1
2022-08-04 12:24:56.721517 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-04 12:24:56.722554 (MainThread): Tracking: tracking
2022-08-04 12:24:56.740647 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc4550110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc455ad10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc455a790>]}
2022-08-04 12:24:56.849588 (MainThread): Partial parsing not enabled
2022-08-04 12:24:56.858098 (MainThread): Parsing macros/core.sql
2022-08-04 12:24:56.879618 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 12:24:56.948851 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 12:24:56.959731 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 12:24:57.101526 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 12:24:57.182270 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 12:24:57.210724 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 12:24:57.219480 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 12:24:57.317840 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 12:24:57.373766 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 12:24:57.398708 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 12:24:57.427055 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 12:24:57.456815 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 12:24:57.636205 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 12:24:57.641077 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 12:24:57.650348 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 12:24:57.660465 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 12:24:57.664914 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 12:24:57.672626 (MainThread): Parsing macros/etc/query.sql
2022-08-04 12:24:57.677711 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 12:24:57.718058 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 12:24:57.722423 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 12:24:57.731745 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 12:24:57.736901 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 12:24:57.743185 (MainThread): Parsing macros/relations.sql
2022-08-04 12:24:57.750024 (MainThread): Parsing macros/adapters.sql
2022-08-04 12:24:57.823625 (MainThread): Parsing macros/catalog.sql
2022-08-04 12:24:57.836323 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 12:24:57.924850 (MainThread): Partial parsing not enabled
2022-08-04 12:24:58.084131 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:24:58.084583 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:58.235898 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:24:58.236359 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:58.266039 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:24:58.266566 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:58.432243 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:24:58.432702 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:24:59.116367 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 12:25:00.065486 (MainThread): scipy not found, skipping conversion test.
2022-08-04 12:25:00.077426 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 12:25:00.078693 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 12:25:00.090049 (MainThread): 
2022-08-04 12:25:00.091284 (MainThread): Acquiring new postgres connection "master".
2022-08-04 12:25:00.091697 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:00.270859 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 12:25:00.271602 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 12:25:00.643443 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:25:00.643896 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 12:25:00.656912 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 12:25:00.657432 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 12:25:00.657743 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 12:25:00.668356 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-04 12:25:00.690098 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 12:25:00.766432 (MainThread): Using postgres connection "master".
2022-08-04 12:25:00.766895 (MainThread): On master: BEGIN
2022-08-04 12:25:00.778443 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 12:25:00.779173 (MainThread): Using postgres connection "master".
2022-08-04 12:25:00.779516 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 12:25:00.799425 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-04 12:25:00.803056 (MainThread): On master: ROLLBACK
2022-08-04 12:25:00.804610 (MainThread): 12:25:00 | Concurrency: 1 threads (target='dev')
2022-08-04 12:25:00.805139 (MainThread): 12:25:00 | 
2022-08-04 12:25:00.821156 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 12:25:00.821793 (Thread-1): 12:25:00 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-04 12:25:00.823395 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:25:00.823797 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-04 12:25:00.824127 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 12:25:00.901062 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 12:25:00.934709 (Thread-1): finished collecting timing info
2022-08-04 12:25:00.935949 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:25:00.936293 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-04 12:25:00.938755 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:25:00.939246 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:25:00.939534 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-04 12:25:01.050345 (Thread-1): SQL status: SELECT 1 in 0.11 seconds
2022-08-04 12:25:01.076581 (Thread-1): finished collecting timing info
2022-08-04 12:25:01.077628 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-04 12:25:01.079702 (Thread-1): 12:25:01 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.26s]
2022-08-04 12:25:01.080246 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 12:25:01.123472 (MainThread): 12:25:01 | 
2022-08-04 12:25:01.124071 (MainThread): 12:25:01 | Finished running 1 test in 1.03s.
2022-08-04 12:25:01.124493 (MainThread): Connection 'master' was left open.
2022-08-04 12:25:01.124839 (MainThread): On master: Close
2022-08-04 12:25:01.130801 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-04 12:25:01.131378 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-04 12:25:01.144764 (MainThread): 
2022-08-04 12:25:01.145306 (MainThread): Completed successfully
2022-08-04 12:25:01.145690 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-04 12:25:01.146358 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc1c81d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc1c57850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7febc1c57c90>]}
2022-08-04 12:25:01.147042 (MainThread): Flushing usage events
2022-08-04 12:25:15.476940 (MainThread): Running with dbt=0.16.1
2022-08-04 12:25:15.880987 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-04 12:25:15.882434 (MainThread): Tracking: tracking
2022-08-04 12:25:15.904101 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2256d82f90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2256c46090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2256c59510>]}
2022-08-04 12:25:16.006436 (MainThread): Partial parsing not enabled
2022-08-04 12:25:16.012667 (MainThread): Parsing macros/core.sql
2022-08-04 12:25:16.034040 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 12:25:16.070996 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 12:25:16.079827 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 12:25:16.214892 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 12:25:16.302605 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 12:25:16.334732 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 12:25:16.343689 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 12:25:16.454991 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 12:25:16.511532 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 12:25:16.534620 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 12:25:16.563018 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 12:25:16.594210 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 12:25:16.773650 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 12:25:16.778689 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 12:25:16.788278 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 12:25:16.798635 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 12:25:16.803481 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 12:25:16.811388 (MainThread): Parsing macros/etc/query.sql
2022-08-04 12:25:16.816689 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 12:25:16.857356 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 12:25:16.861837 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 12:25:16.872956 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 12:25:16.878216 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 12:25:16.884581 (MainThread): Parsing macros/relations.sql
2022-08-04 12:25:16.891822 (MainThread): Parsing macros/adapters.sql
2022-08-04 12:25:16.980448 (MainThread): Parsing macros/catalog.sql
2022-08-04 12:25:16.991041 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 12:25:17.076263 (MainThread): Partial parsing not enabled
2022-08-04 12:25:17.225523 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:25:17.229389 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:17.305396 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:25:17.305841 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:17.338426 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:25:17.338871 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:17.489021 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:25:17.489459 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:18.135854 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 12:25:19.194501 (MainThread): scipy not found, skipping conversion test.
2022-08-04 12:25:19.206922 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 12:25:19.208319 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 12:25:19.220390 (MainThread): 
2022-08-04 12:25:19.221731 (MainThread): 12:25:19 | Concurrency: 1 threads (target='dev')
2022-08-04 12:25:19.222880 (MainThread): 12:25:19 | 
2022-08-04 12:25:19.244616 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 12:25:19.246168 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 12:25:19.246782 (Thread-1): Opening a new connection, currently in state init
2022-08-04 12:25:19.247250 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 12:25:19.475886 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 12:25:19.478278 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.480065 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.482125 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 12:25:19.485048 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 12:25:19.486651 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 12:25:19.487272 (Thread-1): Opening a new connection, currently in state init
2022-08-04 12:25:19.487768 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 12:25:19.522835 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 12:25:19.524230 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.525481 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.527231 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 12:25:19.527966 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 12:25:19.529300 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 12:25:19.529856 (Thread-1): Opening a new connection, currently in state init
2022-08-04 12:25:19.530358 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 12:25:19.563853 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 12:25:19.565222 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.566966 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.568847 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 12:25:19.569782 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 12:25:19.571420 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 12:25:19.572017 (Thread-1): Opening a new connection, currently in state init
2022-08-04 12:25:19.572479 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 12:25:19.639479 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 12:25:19.640562 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.641612 (Thread-1): finished collecting timing info
2022-08-04 12:25:19.643171 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 12:25:19.717497 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 12:25:19.718481 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 12:25:19.765267 (MainThread): 12:25:19 | Done.
2022-08-04 12:25:19.832246 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-04 12:25:19.832672 (MainThread): Opening a new connection, currently in state init
2022-08-04 12:25:19.832965 (MainThread): 12:25:19 | Building catalog
2022-08-04 12:25:19.941397 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-04 12:25:19.941890 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 12:25:20.262603 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 12:25:20.263048 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 12:25:20.279712 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-08-04 12:25:20.280314 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 12:25:20.280633 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 12:25:20.399469 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.12 seconds
2022-08-04 12:25:20.425926 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 12:25:20.455357 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 12:25:20.455990 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 12:25:20.457717 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 12:25:20.458499 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 12:25:20.459077 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 12:25:20.463087 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-08-04 12:25:20.469675 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 12:25:20.703160 (MainThread): 12:25:20 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-04 12:25:20.704214 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2254368350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2254300c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2254300d90>]}
2022-08-04 12:25:20.705179 (MainThread): Flushing usage events
2022-08-04 12:25:21.576028 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-04 12:25:21.578177 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-04 12:25:21.578794 (MainThread): On warehouse.information_schema: Close
2022-08-04 12:25:27.361741 (MainThread): Running with dbt=0.16.1
2022-08-04 12:25:28.176563 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-04 12:25:28.183180 (MainThread): Tracking: tracking
2022-08-04 12:25:28.235318 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04824e5710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0482f734d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04825bcb90>]}
2022-08-04 12:25:28.263778 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-04 12:25:28.269102 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-04 12:25:28.275000 (MainThread): Press Ctrl+C to exit.


2022-08-04 13:32:44.292063 (MainThread): Running with dbt=0.16.1
2022-08-04 13:32:45.445740 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-04 13:32:45.476818 (MainThread): Tracking: tracking
2022-08-04 13:32:46.126955 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f566b33c9d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f566b2def50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f566b3603d0>]}
2022-08-04 13:32:46.401326 (MainThread): Partial parsing not enabled
2022-08-04 13:32:46.479051 (MainThread): Parsing macros/core.sql
2022-08-04 13:32:46.512224 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 13:32:46.599824 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 13:32:46.629176 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 13:32:46.886439 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 13:32:47.028129 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 13:32:47.076213 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 13:32:47.110404 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 13:32:47.310330 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 13:32:47.456785 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 13:32:47.649823 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 13:32:47.774406 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 13:32:47.901804 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 13:32:48.170109 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 13:32:48.244510 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 13:32:48.255627 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 13:32:48.340724 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 13:32:48.385530 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 13:32:48.405883 (MainThread): Parsing macros/etc/query.sql
2022-08-04 13:32:48.473624 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 13:32:48.539175 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 13:32:48.575609 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 13:32:48.611428 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 13:32:48.671217 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 13:32:48.817623 (MainThread): Parsing macros/relations.sql
2022-08-04 13:32:48.838367 (MainThread): Parsing macros/adapters.sql
2022-08-04 13:32:48.987194 (MainThread): Parsing macros/catalog.sql
2022-08-04 13:32:49.240979 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 13:32:49.378168 (MainThread): Partial parsing not enabled
2022-08-04 13:32:49.643843 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:49.645785 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:32:49.767290 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:49.767744 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:32:49.827368 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:49.827802 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:32:50.191225 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:32:50.191667 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:32:51.641340 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 13:32:53.103796 (MainThread): scipy not found, skipping conversion test.
2022-08-04 13:32:53.116310 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 13:32:53.117371 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 13:32:53.130271 (MainThread): 
2022-08-04 13:32:53.131814 (MainThread): Acquiring new postgres connection "master".
2022-08-04 13:32:53.132264 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:32:53.456196 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-04 13:32:53.457089 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 13:32:53.868025 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-04 13:32:53.868654 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-04 13:32:53.958826 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.09 seconds
2022-08-04 13:32:54.090831 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 13:32:54.091527 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 13:32:54.097894 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 13:32:54.098535 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 13:32:54.111196 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 13:32:54.111859 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 13:32:54.112281 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 13:32:54.295086 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.18 seconds
2022-08-04 13:32:54.312200 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 13:32:54.410468 (MainThread): Using postgres connection "master".
2022-08-04 13:32:54.411111 (MainThread): On master: BEGIN
2022-08-04 13:32:54.427437 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-04 13:32:54.428123 (MainThread): Using postgres connection "master".
2022-08-04 13:32:54.428556 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 13:32:55.514336 (MainThread): SQL status: SELECT 0 in 1.09 seconds
2022-08-04 13:32:55.523087 (MainThread): On master: ROLLBACK
2022-08-04 13:32:55.525187 (MainThread): Using postgres connection "master".
2022-08-04 13:32:55.525733 (MainThread): On master: BEGIN
2022-08-04 13:32:55.527704 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:32:55.530027 (MainThread): On master: COMMIT
2022-08-04 13:32:55.530517 (MainThread): Using postgres connection "master".
2022-08-04 13:32:55.530809 (MainThread): On master: COMMIT
2022-08-04 13:32:55.531531 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 13:32:55.532610 (MainThread): 13:32:55 | Concurrency: 1 threads (target='dev')
2022-08-04 13:32:55.533156 (MainThread): 13:32:55 | 
2022-08-04 13:32:55.604233 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 13:32:55.605074 (Thread-1): 13:32:55 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-04 13:32:55.606742 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:55.607313 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-04 13:32:55.608368 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 13:32:55.698173 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 13:32:55.759599 (Thread-1): finished collecting timing info
2022-08-04 13:32:55.865717 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:55.866414 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-04 13:32:55.912576 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2022-08-04 13:32:55.921503 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:55.922164 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 13:32:55.923303 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 13:32:55.997243 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-04 13:32:56.035368 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.035936 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-04 13:32:56.036932 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:32:56.037379 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.037682 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-04 13:32:56.422710 (Thread-1): SQL status: SELECT 6 in 0.38 seconds
2022-08-04 13:32:56.443471 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.444126 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-04 13:32:56.445643 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 13:32:56.455607 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.456244 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-04 13:32:56.458445 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 13:32:56.461780 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 13:32:56.462503 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.463031 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-04 13:32:56.492071 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-08-04 13:32:56.504477 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-04 13:32:56.522368 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-04 13:32:56.603247 (Thread-1): SQL status: DROP TABLE in 0.08 seconds
2022-08-04 13:32:56.627873 (Thread-1): finished collecting timing info
2022-08-04 13:32:56.631207 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bfa22f6-8316-49b8-86f3-d8cb038fd326', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56689a01d0>]}
2022-08-04 13:32:56.632399 (Thread-1): 13:32:56 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 1.02s]
2022-08-04 13:32:56.633112 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 13:32:56.635128 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 13:32:56.635964 (Thread-1): 13:32:56 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-04 13:32:56.637184 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:56.637742 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-04 13:32:56.638240 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 13:32:56.674793 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 13:32:56.676277 (Thread-1): finished collecting timing info
2022-08-04 13:32:56.711356 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:56.712083 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-04 13:32:56.713400 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 13:32:56.731159 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:56.731839 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 13:32:56.733065 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 13:32:56.748934 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-04 13:32:56.756433 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:56.757312 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-04 13:32:56.759089 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:32:56.759724 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:56.760168 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 13:32:57.276939 (Thread-1): SQL status: SELECT 922 in 0.52 seconds
2022-08-04 13:32:57.291364 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:57.291811 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-04 13:32:57.293197 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 13:32:57.301813 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:57.302349 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-04 13:32:57.303651 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 13:32:57.306959 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 13:32:57.307394 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:57.307663 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-04 13:32:57.535817 (Thread-1): SQL status: COMMIT in 0.23 seconds
2022-08-04 13:32:57.546971 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-04 13:32:57.547422 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-04 13:32:57.591784 (Thread-1): SQL status: DROP TABLE in 0.04 seconds
2022-08-04 13:32:57.608126 (Thread-1): finished collecting timing info
2022-08-04 13:32:57.617478 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bfa22f6-8316-49b8-86f3-d8cb038fd326', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f565b875150>]}
2022-08-04 13:32:57.623980 (Thread-1): 13:32:57 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.98s]
2022-08-04 13:32:57.624506 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 13:32:57.624970 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 13:32:57.627299 (Thread-1): 13:32:57 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-04 13:32:57.629009 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:57.629751 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-04 13:32:57.630359 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 13:32:57.690659 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 13:32:57.691786 (Thread-1): finished collecting timing info
2022-08-04 13:32:57.727833 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:57.728620 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-04 13:32:57.730361 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 13:32:57.738986 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:57.739431 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 13:32:57.740685 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-04 13:32:57.746476 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-04 13:32:57.747754 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:57.748151 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-04 13:32:57.750353 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:32:57.750879 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:32:57.751162 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-04 13:33:02.452322 (Thread-1): SQL status: SELECT 922 in 4.70 seconds
2022-08-04 13:33:02.622764 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:33:02.623378 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-04 13:33:03.042314 (Thread-1): SQL status: ALTER TABLE in 0.42 seconds
2022-08-04 13:33:03.051463 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:33:03.051916 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-04 13:33:03.053140 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-04 13:33:03.056564 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 13:33:03.057006 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:33:03.057284 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-04 13:33:03.497321 (Thread-1): SQL status: COMMIT in 0.44 seconds
2022-08-04 13:33:03.505511 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:33:03.506047 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-04 13:33:03.849139 (Thread-1): SQL status: DROP TABLE in 0.34 seconds
2022-08-04 13:33:03.859369 (Thread-1): finished collecting timing info
2022-08-04 13:33:03.864520 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7bfa22f6-8316-49b8-86f3-d8cb038fd326', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f566892fa50>]}
2022-08-04 13:33:03.865704 (Thread-1): 13:33:03 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 6.24s]
2022-08-04 13:33:03.867000 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 13:33:03.944916 (MainThread): Using postgres connection "master".
2022-08-04 13:33:03.945372 (MainThread): On master: BEGIN
2022-08-04 13:33:03.946250 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:33:03.946711 (MainThread): On master: COMMIT
2022-08-04 13:33:03.946998 (MainThread): Using postgres connection "master".
2022-08-04 13:33:03.947519 (MainThread): On master: COMMIT
2022-08-04 13:33:03.948341 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-04 13:33:03.949426 (MainThread): 13:33:03 | 
2022-08-04 13:33:03.949895 (MainThread): 13:33:03 | Finished running 3 table models in 10.82s.
2022-08-04 13:33:03.950609 (MainThread): Connection 'master' was left open.
2022-08-04 13:33:03.951199 (MainThread): On master: Close
2022-08-04 13:33:03.951904 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-04 13:33:03.952472 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-04 13:33:03.958775 (MainThread): Connection 'list_warehouse_warehouse' was left open.
2022-08-04 13:33:03.959437 (MainThread): On list_warehouse_warehouse: Close
2022-08-04 13:33:04.470855 (MainThread): 
2022-08-04 13:33:04.471430 (MainThread): Completed successfully
2022-08-04 13:33:04.471832 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-04 13:33:04.472400 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f565a9c1790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56689d5150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5668ada550>]}
2022-08-04 13:33:04.473064 (MainThread): Flushing usage events
2022-08-04 13:34:38.540254 (MainThread): Running with dbt=0.16.1
2022-08-04 13:34:54.335538 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-04 13:34:55.840312 (MainThread): Tracking: tracking
2022-08-04 13:35:01.027243 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70b6f38310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70b7d68c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70b6ffa650>]}
2022-08-04 13:35:01.239836 (MainThread): Partial parsing not enabled
2022-08-04 13:35:01.669222 (MainThread): Parsing macros/core.sql
2022-08-04 13:35:02.090711 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 13:35:02.154493 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 13:35:02.190252 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 13:35:02.386722 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 13:35:02.584619 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 13:35:02.765992 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 13:35:02.800066 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 13:35:03.044576 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 13:35:03.135534 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 13:35:03.306489 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 13:35:03.386351 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 13:35:03.491297 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 13:35:03.859755 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 13:35:03.923163 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 13:35:03.948556 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 13:35:03.986224 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 13:35:04.019786 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 13:35:04.073328 (MainThread): Parsing macros/etc/query.sql
2022-08-04 13:35:04.185280 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 13:35:04.241454 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 13:35:04.309765 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 13:35:04.434366 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 13:35:04.538949 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 13:35:04.740337 (MainThread): Parsing macros/relations.sql
2022-08-04 13:35:04.849494 (MainThread): Parsing macros/adapters.sql
2022-08-04 13:35:05.010435 (MainThread): Parsing macros/catalog.sql
2022-08-04 13:35:05.052795 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 13:35:05.167892 (MainThread): Partial parsing not enabled
2022-08-04 13:35:05.632694 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 13:35:05.633172 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:35:05.945507 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 13:35:05.946183 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:35:06.106213 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:35:06.106821 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:35:06.422071 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:35:06.422687 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:35:07.789084 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 13:35:28.584862 (MainThread): scipy not found, skipping conversion test.
2022-08-04 13:35:28.601257 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 13:35:28.604893 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 13:35:28.769067 (MainThread): 
2022-08-04 13:35:28.772285 (MainThread): Acquiring new postgres connection "master".
2022-08-04 13:35:28.772917 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:35:29.138433 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-04 13:35:29.139700 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-04 13:35:29.551510 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 13:35:29.551967 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-04 13:35:30.029547 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.48 seconds
2022-08-04 13:35:30.030087 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-04 13:35:30.030387 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-04 13:35:30.223501 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.19 seconds
2022-08-04 13:35:30.959045 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-04 13:35:31.046150 (MainThread): Using postgres connection "master".
2022-08-04 13:35:31.046791 (MainThread): On master: BEGIN
2022-08-04 13:35:31.058891 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-04 13:35:31.059564 (MainThread): Using postgres connection "master".
2022-08-04 13:35:31.060001 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-04 13:35:31.084922 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-04 13:35:31.088149 (MainThread): On master: ROLLBACK
2022-08-04 13:35:31.090461 (MainThread): 13:35:31 | Concurrency: 1 threads (target='dev')
2022-08-04 13:35:31.091829 (MainThread): 13:35:31 | 
2022-08-04 13:35:31.190378 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 13:35:31.191248 (Thread-1): 13:35:31 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-04 13:35:31.194923 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:35:31.195360 (Thread-1): Opening a new connection, currently in state init
2022-08-04 13:35:31.195684 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 13:35:31.280634 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 13:35:31.346975 (Thread-1): finished collecting timing info
2022-08-04 13:35:31.348223 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:35:31.348566 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-04 13:35:31.358524 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-04 13:35:31.359257 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:35:31.359633 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-04 13:35:31.366317 (Thread-1): SQL status: SELECT 1 in 0.01 seconds
2022-08-04 13:35:31.367568 (Thread-1): finished collecting timing info
2022-08-04 13:35:31.368526 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-04 13:35:31.371289 (Thread-1): 13:35:31 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.18s]
2022-08-04 13:35:31.372549 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 13:35:31.390983 (MainThread): 13:35:31 | 
2022-08-04 13:35:31.391511 (MainThread): 13:35:31 | Finished running 1 test in 2.62s.
2022-08-04 13:35:31.391867 (MainThread): Connection 'master' was left open.
2022-08-04 13:35:31.392142 (MainThread): On master: Close
2022-08-04 13:35:31.393347 (MainThread): Connection 'list_warehouse_warehouse' was left open.
2022-08-04 13:35:31.393807 (MainThread): On list_warehouse_warehouse: Close
2022-08-04 13:35:31.398894 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-04 13:35:31.399581 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-04 13:35:31.413159 (MainThread): 
2022-08-04 13:35:31.414993 (MainThread): Completed successfully
2022-08-04 13:35:31.416152 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-04 13:35:31.417520 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70b4601a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70b4625fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f70a6684f50>]}
2022-08-04 13:35:31.418574 (MainThread): Flushing usage events
2022-08-04 13:36:22.487874 (MainThread): Running with dbt=0.16.1
2022-08-04 13:36:22.784784 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-04 13:36:22.785725 (MainThread): Tracking: tracking
2022-08-04 13:36:22.816014 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa73113b490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa73113be50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7311ee790>]}
2022-08-04 13:36:22.933247 (MainThread): Partial parsing not enabled
2022-08-04 13:36:22.941276 (MainThread): Parsing macros/core.sql
2022-08-04 13:36:22.968974 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-04 13:36:23.008464 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-04 13:36:23.016621 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-04 13:36:23.151145 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-04 13:36:23.233867 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-04 13:36:23.262589 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-04 13:36:23.271712 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-04 13:36:23.417547 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-04 13:36:23.544144 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-04 13:36:23.573388 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-04 13:36:23.669096 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-04 13:36:23.753430 (MainThread): Parsing macros/adapters/common.sql
2022-08-04 13:36:24.124054 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-04 13:36:24.135261 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-04 13:36:24.169649 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-04 13:36:24.193569 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-04 13:36:24.208890 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-04 13:36:24.225102 (MainThread): Parsing macros/etc/query.sql
2022-08-04 13:36:24.241394 (MainThread): Parsing macros/etc/datetime.sql
2022-08-04 13:36:24.343138 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-04 13:36:24.351705 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-04 13:36:24.377414 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-04 13:36:24.395374 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-04 13:36:24.411407 (MainThread): Parsing macros/relations.sql
2022-08-04 13:36:24.426945 (MainThread): Parsing macros/adapters.sql
2022-08-04 13:36:24.595277 (MainThread): Parsing macros/catalog.sql
2022-08-04 13:36:24.622337 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-04 13:36:24.823474 (MainThread): Partial parsing not enabled
2022-08-04 13:36:25.117481 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 13:36:25.119751 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:36:25.218237 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 13:36:25.223592 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:36:25.257916 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:36:25.258593 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:36:25.504662 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:36:25.506967 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:36:26.356353 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-04 13:36:27.419391 (MainThread): scipy not found, skipping conversion test.
2022-08-04 13:36:27.431841 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-04 13:36:27.433149 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-04 13:36:27.444758 (MainThread): 
2022-08-04 13:36:27.445578 (MainThread): 13:36:27 | Concurrency: 1 threads (target='dev')
2022-08-04 13:36:27.446243 (MainThread): 13:36:27 | 
2022-08-04 13:36:27.468095 (Thread-1): Began running node model.dbt_.dim_types
2022-08-04 13:36:27.469586 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-04 13:36:27.470268 (Thread-1): Opening a new connection, currently in state init
2022-08-04 13:36:27.470767 (Thread-1): Compiling model.dbt_.dim_types
2022-08-04 13:36:27.658089 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-04 13:36:27.711076 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.712481 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.714348 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-04 13:36:27.716636 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-04 13:36:27.717908 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-04 13:36:27.718564 (Thread-1): Opening a new connection, currently in state init
2022-08-04 13:36:27.719022 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-04 13:36:27.754876 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-04 13:36:27.756401 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.757705 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.759555 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-04 13:36:27.760337 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-04 13:36:27.761751 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-04 13:36:27.762401 (Thread-1): Opening a new connection, currently in state init
2022-08-04 13:36:27.762857 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-04 13:36:27.803220 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-04 13:36:27.804714 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.806040 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.807799 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-04 13:36:27.808582 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-04 13:36:27.810036 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-04 13:36:27.810637 (Thread-1): Opening a new connection, currently in state init
2022-08-04 13:36:27.811097 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-04 13:36:27.877310 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-04 13:36:27.878932 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.880272 (Thread-1): finished collecting timing info
2022-08-04 13:36:27.882129 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-04 13:36:27.903369 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 13:36:27.904343 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-04 13:36:27.954544 (MainThread): 13:36:27 | Done.
2022-08-04 13:36:28.093882 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-04 13:36:28.094576 (MainThread): Opening a new connection, currently in state init
2022-08-04 13:36:28.095034 (MainThread): 13:36:28 | Building catalog
2022-08-04 13:36:28.197040 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-04 13:36:28.198060 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-04 13:36:28.546465 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 13:36:28.547098 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 13:36:28.559561 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-08-04 13:36:28.560249 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 13:36:28.560701 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 13:36:28.876913 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.32 seconds
2022-08-04 13:36:28.898807 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 13:36:28.914201 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 13:36:28.914668 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-04 13:36:28.915968 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-04 13:36:28.916487 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-04 13:36:28.916803 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-04 13:36:28.920807 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-08-04 13:36:28.926101 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-04 13:36:29.149189 (MainThread): 13:36:29 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-04 13:36:29.150088 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa731135590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa72e81f050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa72e81f0d0>]}
2022-08-04 13:36:29.150895 (MainThread): Flushing usage events
2022-08-04 13:36:30.042803 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-04 13:36:30.044517 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-04 13:36:30.044994 (MainThread): On warehouse.information_schema: Close
2022-08-04 13:36:34.333584 (MainThread): Running with dbt=0.16.1
2022-08-04 13:36:34.662383 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-04 13:36:34.663300 (MainThread): Tracking: tracking
2022-08-04 13:36:34.707683 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a72a164d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a72b53290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1a72a1bb10>]}
2022-08-04 13:36:34.730440 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-04 13:36:34.731618 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-04 13:36:34.732904 (MainThread): Press Ctrl+C to exit.


2022-08-05 01:54:36.942618 (MainThread): Running with dbt=0.16.1
2022-08-05 01:54:37.562572 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-05 01:54:37.622412 (MainThread): Tracking: tracking
2022-08-05 01:54:37.719058 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5c9f790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5e52e10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5da18d0>]}
2022-08-05 01:54:37.919002 (MainThread): Partial parsing not enabled
2022-08-05 01:54:37.961796 (MainThread): Parsing macros/core.sql
2022-08-05 01:54:38.017139 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-05 01:54:38.103081 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-05 01:54:38.127709 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-05 01:54:38.393666 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-05 01:54:38.555277 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-05 01:54:38.614279 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-05 01:54:38.631947 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-05 01:54:38.882046 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-05 01:54:39.018952 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-05 01:54:39.088397 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-05 01:54:39.161451 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-05 01:54:39.229131 (MainThread): Parsing macros/adapters/common.sql
2022-08-05 01:54:39.487363 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-05 01:54:39.505596 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-05 01:54:39.517501 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-05 01:54:39.535195 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-05 01:54:39.557838 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-05 01:54:39.577685 (MainThread): Parsing macros/etc/query.sql
2022-08-05 01:54:39.590078 (MainThread): Parsing macros/etc/datetime.sql
2022-08-05 01:54:39.667044 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-05 01:54:39.681253 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-05 01:54:39.706076 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-05 01:54:39.732628 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-05 01:54:39.790218 (MainThread): Parsing macros/relations.sql
2022-08-05 01:54:39.821593 (MainThread): Parsing macros/adapters.sql
2022-08-05 01:54:39.926350 (MainThread): Parsing macros/catalog.sql
2022-08-05 01:54:39.958360 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-05 01:54:40.073370 (MainThread): Partial parsing not enabled
2022-08-05 01:54:40.362032 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:40.362460 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:54:40.517616 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:40.518026 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:54:40.577990 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:40.578418 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:54:40.941531 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:54:40.941954 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:54:42.343456 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-05 01:54:43.647219 (MainThread): scipy not found, skipping conversion test.
2022-08-05 01:54:43.674569 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-05 01:54:43.675610 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-05 01:54:43.696126 (MainThread): 
2022-08-05 01:54:43.705506 (MainThread): Acquiring new postgres connection "master".
2022-08-05 01:54:43.705983 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:54:44.046487 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-05 01:54:44.047254 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-05 01:54:44.851470 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-05 01:54:44.851923 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-05 01:54:44.998619 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.15 seconds
2022-08-05 01:54:45.242094 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-05 01:54:45.255144 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-05 01:54:45.297965 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-05 01:54:45.298463 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-05 01:54:45.299748 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:45.305695 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-05 01:54:45.306166 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-05 01:54:45.683941 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.38 seconds
2022-08-05 01:54:45.703163 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-05 01:54:45.862138 (MainThread): Using postgres connection "master".
2022-08-05 01:54:45.862571 (MainThread): On master: BEGIN
2022-08-05 01:54:45.885524 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-05 01:54:45.885967 (MainThread): Using postgres connection "master".
2022-08-05 01:54:45.886374 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-05 01:54:46.667824 (MainThread): SQL status: SELECT 0 in 0.78 seconds
2022-08-05 01:54:46.670836 (MainThread): On master: ROLLBACK
2022-08-05 01:54:46.672350 (MainThread): Using postgres connection "master".
2022-08-05 01:54:46.672775 (MainThread): On master: BEGIN
2022-08-05 01:54:46.677999 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:46.678509 (MainThread): On master: COMMIT
2022-08-05 01:54:46.678815 (MainThread): Using postgres connection "master".
2022-08-05 01:54:46.679082 (MainThread): On master: COMMIT
2022-08-05 01:54:46.679756 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-05 01:54:46.680800 (MainThread): 01:54:46 | Concurrency: 1 threads (target='dev')
2022-08-05 01:54:46.681831 (MainThread): 01:54:46 | 
2022-08-05 01:54:47.053921 (Thread-1): Began running node model.dbt_.dim_types
2022-08-05 01:54:47.054532 (Thread-1): 01:54:47 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-05 01:54:47.055961 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:47.056336 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-05 01:54:47.056665 (Thread-1): Compiling model.dbt_.dim_types
2022-08-05 01:54:47.138750 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-05 01:54:47.487247 (Thread-1): finished collecting timing info
2022-08-05 01:54:47.577354 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:47.577840 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-05 01:54:47.579334 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:47.587545 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:47.587978 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-05 01:54:47.588946 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:47.656870 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-05 01:54:48.506177 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:48.506602 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-05 01:54:48.507774 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:48.508199 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:48.508468 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-05 01:54:48.873400 (Thread-1): SQL status: SELECT 6 in 0.36 seconds
2022-08-05 01:54:48.895082 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:48.895518 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-05 01:54:48.898188 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:48.908262 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:48.908699 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-05 01:54:48.910246 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:48.914497 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-05 01:54:48.914928 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:48.915200 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-05 01:54:49.147570 (Thread-1): SQL status: COMMIT in 0.23 seconds
2022-08-05 01:54:49.162754 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-05 01:54:49.163227 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-05 01:54:49.230897 (Thread-1): SQL status: DROP TABLE in 0.07 seconds
2022-08-05 01:54:49.282009 (Thread-1): finished collecting timing info
2022-08-05 01:54:49.290420 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5859332-e1b1-4b12-ad13-fe931ea8979b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5db0c50>]}
2022-08-05 01:54:49.291453 (Thread-1): 01:54:49 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 2.23s]
2022-08-05 01:54:49.295976 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-05 01:54:49.298172 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-05 01:54:49.298752 (Thread-1): 01:54:49 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-05 01:54:49.302731 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.303145 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-05 01:54:49.303458 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-05 01:54:49.370687 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-05 01:54:49.371768 (Thread-1): finished collecting timing info
2022-08-05 01:54:49.399925 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.400370 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-05 01:54:49.401647 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:49.410705 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.411138 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-05 01:54:49.412501 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:49.418183 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-05 01:54:49.419699 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.420091 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-05 01:54:49.420822 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:49.421294 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.421575 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-05 01:54:49.740009 (Thread-1): SQL status: SELECT 922 in 0.32 seconds
2022-08-05 01:54:49.765165 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.765591 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-05 01:54:49.766945 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:49.782611 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.783042 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-05 01:54:49.784465 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:49.788327 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-05 01:54:49.788909 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.789282 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-05 01:54:49.919253 (Thread-1): SQL status: COMMIT in 0.13 seconds
2022-08-05 01:54:49.928343 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-05 01:54:49.928767 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-05 01:54:50.047225 (Thread-1): SQL status: DROP TABLE in 0.12 seconds
2022-08-05 01:54:50.056120 (Thread-1): finished collecting timing info
2022-08-05 01:54:50.058862 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5859332-e1b1-4b12-ad13-fe931ea8979b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5db0c50>]}
2022-08-05 01:54:50.059694 (Thread-1): 01:54:50 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.76s]
2022-08-05 01:54:50.061184 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-05 01:54:50.061799 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-05 01:54:50.062858 (Thread-1): 01:54:50 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-05 01:54:50.063909 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:50.064255 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-05 01:54:50.064545 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-05 01:54:50.099921 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-05 01:54:50.100943 (Thread-1): finished collecting timing info
2022-08-05 01:54:50.134250 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:50.134699 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-05 01:54:50.135767 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:50.144061 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:50.144487 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-05 01:54:50.145540 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-05 01:54:50.151360 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-05 01:54:50.152578 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:50.152977 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-05 01:54:50.153784 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:50.154173 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:50.154446 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-05 01:54:54.103777 (Thread-1): SQL status: SELECT 922 in 3.95 seconds
2022-08-05 01:54:54.120555 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:54.121012 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-05 01:54:54.122490 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:54.133756 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:54.134193 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-05 01:54:54.135667 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-05 01:54:54.138875 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-05 01:54:54.139314 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:54.139595 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-05 01:54:54.275901 (Thread-1): SQL status: COMMIT in 0.14 seconds
2022-08-05 01:54:54.282453 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:54:54.282874 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-05 01:54:54.330840 (Thread-1): SQL status: DROP TABLE in 0.05 seconds
2022-08-05 01:54:54.340485 (Thread-1): finished collecting timing info
2022-08-05 01:54:54.342954 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5859332-e1b1-4b12-ad13-fe931ea8979b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e33f9d90>]}
2022-08-05 01:54:54.343767 (Thread-1): 01:54:54 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 4.28s]
2022-08-05 01:54:54.344220 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-05 01:54:54.416156 (MainThread): Using postgres connection "master".
2022-08-05 01:54:54.416593 (MainThread): On master: BEGIN
2022-08-05 01:54:54.417511 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:54:54.417974 (MainThread): On master: COMMIT
2022-08-05 01:54:54.418258 (MainThread): Using postgres connection "master".
2022-08-05 01:54:54.418515 (MainThread): On master: COMMIT
2022-08-05 01:54:54.419216 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-05 01:54:54.420347 (MainThread): 01:54:54 | 
2022-08-05 01:54:54.421403 (MainThread): 01:54:54 | Finished running 3 table models in 10.72s.
2022-08-05 01:54:54.422302 (MainThread): Connection 'master' was left open.
2022-08-05 01:54:54.422666 (MainThread): On master: Close
2022-08-05 01:54:54.428167 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-05 01:54:54.428657 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-05 01:54:54.474548 (MainThread): 
2022-08-05 01:54:54.480865 (MainThread): Completed successfully
2022-08-05 01:54:54.481839 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-05 01:54:54.483075 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5d70390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5d70a50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79e5d70c10>]}
2022-08-05 01:54:54.483799 (MainThread): Flushing usage events
2022-08-05 01:55:25.021992 (MainThread): Running with dbt=0.16.1
2022-08-05 01:55:25.837757 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-05 01:55:25.838701 (MainThread): Tracking: tracking
2022-08-05 01:55:25.977709 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3b05fa6b90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3b06091750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3b05fc62d0>]}
2022-08-05 01:55:26.531087 (MainThread): Partial parsing not enabled
2022-08-05 01:55:26.540772 (MainThread): Parsing macros/core.sql
2022-08-05 01:55:26.599955 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-05 01:55:26.722181 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-05 01:55:26.735244 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-05 01:55:27.019171 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-05 01:55:27.207852 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-05 01:55:27.266258 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-05 01:55:27.290343 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-05 01:55:27.512772 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-05 01:55:27.643632 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-05 01:55:27.718068 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-05 01:55:27.802344 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-05 01:55:27.902243 (MainThread): Parsing macros/adapters/common.sql
2022-08-05 01:55:28.245738 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-05 01:55:28.254960 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-05 01:55:28.277554 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-05 01:55:28.320741 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-05 01:55:28.341519 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-05 01:55:28.373878 (MainThread): Parsing macros/etc/query.sql
2022-08-05 01:55:28.385931 (MainThread): Parsing macros/etc/datetime.sql
2022-08-05 01:55:28.505224 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-05 01:55:28.513875 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-05 01:55:28.527347 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-05 01:55:28.539567 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-05 01:55:28.554226 (MainThread): Parsing macros/relations.sql
2022-08-05 01:55:28.573453 (MainThread): Parsing macros/adapters.sql
2022-08-05 01:55:28.716054 (MainThread): Parsing macros/catalog.sql
2022-08-05 01:55:28.738285 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-05 01:55:28.918932 (MainThread): Partial parsing not enabled
2022-08-05 01:55:29.187062 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-05 01:55:29.187477 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:29.358833 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-05 01:55:29.359301 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:29.451975 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:55:29.452471 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:29.766153 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:55:29.766564 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:30.904264 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-05 01:55:32.667300 (MainThread): scipy not found, skipping conversion test.
2022-08-05 01:55:32.691353 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-05 01:55:32.692393 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-05 01:55:32.721981 (MainThread): 
2022-08-05 01:55:32.723717 (MainThread): Acquiring new postgres connection "master".
2022-08-05 01:55:32.724130 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:33.169774 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-05 01:55:33.171299 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-05 01:55:33.802540 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-05 01:55:33.802975 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-05 01:55:33.825117 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.02 seconds
2022-08-05 01:55:33.825564 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-05 01:55:33.825828 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-05 01:55:33.835382 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-05 01:55:33.857526 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-05 01:55:33.938427 (MainThread): Using postgres connection "master".
2022-08-05 01:55:33.938857 (MainThread): On master: BEGIN
2022-08-05 01:55:33.949518 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-05 01:55:33.950034 (MainThread): Using postgres connection "master".
2022-08-05 01:55:33.950325 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-05 01:55:33.969709 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-05 01:55:33.972816 (MainThread): On master: ROLLBACK
2022-08-05 01:55:33.974353 (MainThread): 01:55:33 | Concurrency: 1 threads (target='dev')
2022-08-05 01:55:33.974857 (MainThread): 01:55:33 | 
2022-08-05 01:55:34.000877 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-05 01:55:34.001533 (Thread-1): 01:55:33 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-05 01:55:34.002499 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:55:34.002841 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-05 01:55:34.003150 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-05 01:55:34.100033 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-05 01:55:34.164113 (Thread-1): finished collecting timing info
2022-08-05 01:55:34.165519 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:55:34.165911 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-05 01:55:34.169409 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:55:34.169883 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:55:34.170161 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-05 01:55:34.268175 (Thread-1): SQL status: SELECT 1 in 0.10 seconds
2022-08-05 01:55:34.269337 (Thread-1): finished collecting timing info
2022-08-05 01:55:34.270313 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-05 01:55:34.271814 (Thread-1): 01:55:34 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.27s]
2022-08-05 01:55:34.272336 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-05 01:55:34.299944 (MainThread): 01:55:34 | 
2022-08-05 01:55:34.300465 (MainThread): 01:55:34 | Finished running 1 test in 1.58s.
2022-08-05 01:55:34.300808 (MainThread): Connection 'master' was left open.
2022-08-05 01:55:34.305291 (MainThread): On master: Close
2022-08-05 01:55:34.310983 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-05 01:55:34.311468 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-05 01:55:34.349444 (MainThread): 
2022-08-05 01:55:34.350029 (MainThread): Completed successfully
2022-08-05 01:55:34.350569 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-05 01:55:34.351257 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3b036e4590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af56f3c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3af609df10>]}
2022-08-05 01:55:34.351916 (MainThread): Flushing usage events
2022-08-05 01:55:55.637608 (MainThread): Running with dbt=0.16.1
2022-08-05 01:55:56.103613 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-05 01:55:56.104727 (MainThread): Tracking: tracking
2022-08-05 01:55:56.121978 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32911b6c90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32912a2e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32911c0a10>]}
2022-08-05 01:55:56.207776 (MainThread): Partial parsing not enabled
2022-08-05 01:55:56.213936 (MainThread): Parsing macros/core.sql
2022-08-05 01:55:56.235239 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-05 01:55:56.286813 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-05 01:55:56.294918 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-05 01:55:56.432877 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-05 01:55:56.515220 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-05 01:55:56.543889 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-05 01:55:56.553566 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-05 01:55:56.731514 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-05 01:55:56.797204 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-05 01:55:56.820898 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-05 01:55:56.852177 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-05 01:55:56.889772 (MainThread): Parsing macros/adapters/common.sql
2022-08-05 01:55:57.123621 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-05 01:55:57.128734 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-05 01:55:57.138690 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-05 01:55:57.150561 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-05 01:55:57.165432 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-05 01:55:57.184106 (MainThread): Parsing macros/etc/query.sql
2022-08-05 01:55:57.189569 (MainThread): Parsing macros/etc/datetime.sql
2022-08-05 01:55:57.285643 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-05 01:55:57.290377 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-05 01:55:57.303805 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-05 01:55:57.316042 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-05 01:55:57.324438 (MainThread): Parsing macros/relations.sql
2022-08-05 01:55:57.343972 (MainThread): Parsing macros/adapters.sql
2022-08-05 01:55:57.555834 (MainThread): Parsing macros/catalog.sql
2022-08-05 01:55:57.585830 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-05 01:55:57.827276 (MainThread): Partial parsing not enabled
2022-08-05 01:55:58.205324 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-05 01:55:58.205749 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:58.295683 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-05 01:55:58.296100 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:58.338935 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:55:58.339348 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:58.679038 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:55:58.681385 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:55:59.953407 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-05 01:56:01.868359 (MainThread): scipy not found, skipping conversion test.
2022-08-05 01:56:01.896706 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-05 01:56:01.902148 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-05 01:56:01.927231 (MainThread): 
2022-08-05 01:56:01.927873 (MainThread): 01:56:01 | Concurrency: 1 threads (target='dev')
2022-08-05 01:56:01.928255 (MainThread): 01:56:01 | 
2022-08-05 01:56:01.997564 (Thread-1): Began running node model.dbt_.dim_types
2022-08-05 01:56:01.998806 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-05 01:56:01.999218 (Thread-1): Opening a new connection, currently in state init
2022-08-05 01:56:01.999521 (Thread-1): Compiling model.dbt_.dim_types
2022-08-05 01:56:02.396000 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-05 01:56:02.408117 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.409394 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.410937 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-05 01:56:02.413597 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-05 01:56:02.414717 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-05 01:56:02.415102 (Thread-1): Opening a new connection, currently in state init
2022-08-05 01:56:02.415394 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-05 01:56:02.483760 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-05 01:56:02.484790 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.490214 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.502012 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-05 01:56:02.509741 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-05 01:56:02.510870 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-05 01:56:02.511251 (Thread-1): Opening a new connection, currently in state init
2022-08-05 01:56:02.511546 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-05 01:56:02.575520 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-05 01:56:02.576541 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.581744 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.585613 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-05 01:56:02.586862 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-05 01:56:02.588082 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-05 01:56:02.588474 (Thread-1): Opening a new connection, currently in state init
2022-08-05 01:56:02.588773 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-05 01:56:02.685768 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-05 01:56:02.686934 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.688052 (Thread-1): finished collecting timing info
2022-08-05 01:56:02.689585 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-05 01:56:02.787488 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-05 01:56:02.787950 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-05 01:56:02.884346 (MainThread): 01:56:02 | Done.
2022-08-05 01:56:02.935684 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-05 01:56:02.936096 (MainThread): Opening a new connection, currently in state init
2022-08-05 01:56:02.936388 (MainThread): 01:56:02 | Building catalog
2022-08-05 01:56:03.069693 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-05 01:56:03.070144 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-05 01:56:03.438346 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-05 01:56:03.438767 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-05 01:56:03.483099 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.04 seconds
2022-08-05 01:56:03.483570 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-05 01:56:03.483843 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-05 01:56:03.628545 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.14 seconds
2022-08-05 01:56:03.667908 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-05 01:56:03.696827 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-05 01:56:03.701473 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-05 01:56:03.702633 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-05 01:56:03.703046 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-05 01:56:03.703308 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-05 01:56:03.706555 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.00 seconds
2022-08-05 01:56:03.716328 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-05 01:56:04.260832 (MainThread): 01:56:04 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-05 01:56:04.272644 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f32912a2e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3291259050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f329122bcd0>]}
2022-08-05 01:56:04.273768 (MainThread): Flushing usage events
2022-08-05 01:56:10.049305 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-05 01:56:10.051186 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-05 01:56:10.051501 (MainThread): On warehouse.information_schema: Close
2022-08-05 01:56:18.531730 (MainThread): Running with dbt=0.16.1
2022-08-05 01:56:19.299061 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-05 01:56:19.299976 (MainThread): Tracking: tracking
2022-08-05 01:56:19.356535 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85fb8772d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85fb85d590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f85fb9c2910>]}
2022-08-05 01:56:19.393860 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-05 01:56:19.394402 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-05 01:56:19.394714 (MainThread): Press Ctrl+C to exit.


2022-08-06 16:15:17.783297 (MainThread): Running with dbt=0.16.1
2022-08-06 16:15:18.407621 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-06 16:15:18.446571 (MainThread): Tracking: tracking
2022-08-06 16:15:18.503905 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379f2a8a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379f36c290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379f2c8310>]}
2022-08-06 16:15:18.723047 (MainThread): Partial parsing not enabled
2022-08-06 16:15:18.790996 (MainThread): Parsing macros/core.sql
2022-08-06 16:15:18.846240 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-06 16:15:18.932293 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-06 16:15:18.967921 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-06 16:15:19.269724 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-06 16:15:19.462395 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-06 16:15:19.543247 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-06 16:15:19.560961 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-06 16:15:19.799919 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-06 16:15:19.969476 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-06 16:15:20.050662 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-06 16:15:20.097414 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-06 16:15:20.180594 (MainThread): Parsing macros/adapters/common.sql
2022-08-06 16:15:20.649278 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-06 16:15:20.667655 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-06 16:15:20.692962 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-06 16:15:20.719344 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-06 16:15:20.741943 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-06 16:15:20.772928 (MainThread): Parsing macros/etc/query.sql
2022-08-06 16:15:20.796565 (MainThread): Parsing macros/etc/datetime.sql
2022-08-06 16:15:20.928876 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-06 16:15:20.954239 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-06 16:15:20.978992 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-06 16:15:21.005480 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-06 16:15:21.063111 (MainThread): Parsing macros/relations.sql
2022-08-06 16:15:21.094405 (MainThread): Parsing macros/adapters.sql
2022-08-06 16:15:21.316722 (MainThread): Parsing macros/catalog.sql
2022-08-06 16:15:21.375451 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-06 16:15:21.523521 (MainThread): Partial parsing not enabled
2022-08-06 16:15:21.861543 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:21.861958 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:15:22.089776 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:22.090190 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:15:22.221655 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:22.222074 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:15:22.546885 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:15:22.547301 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:15:24.009899 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-06 16:15:26.068876 (MainThread): scipy not found, skipping conversion test.
2022-08-06 16:15:26.091403 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-06 16:15:26.096457 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-06 16:15:26.113619 (MainThread): 
2022-08-06 16:15:26.114716 (MainThread): Acquiring new postgres connection "master".
2022-08-06 16:15:26.115082 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:15:26.502294 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_warehouse".
2022-08-06 16:15:26.504832 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-06 16:15:27.551394 (ThreadPoolExecutor-0_0): Using postgres connection "list_warehouse".
2022-08-06 16:15:27.551815 (ThreadPoolExecutor-0_0): On list_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse"} */

    select distinct nspname from pg_namespace
  
2022-08-06 16:15:27.578670 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-08-06 16:15:27.833782 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-06 16:15:27.834622 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_warehouse).
2022-08-06 16:15:27.847729 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-06 16:15:27.848170 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-06 16:15:27.858475 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-06 16:15:27.858920 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-06 16:15:27.859209 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-06 16:15:27.924648 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.07 seconds
2022-08-06 16:15:27.992299 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-06 16:15:28.312121 (MainThread): Using postgres connection "master".
2022-08-06 16:15:28.312592 (MainThread): On master: BEGIN
2022-08-06 16:15:28.334030 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-06 16:15:28.334498 (MainThread): Using postgres connection "master".
2022-08-06 16:15:28.334765 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-06 16:15:28.426287 (MainThread): SQL status: SELECT 0 in 0.09 seconds
2022-08-06 16:15:28.430100 (MainThread): On master: ROLLBACK
2022-08-06 16:15:28.437956 (MainThread): Using postgres connection "master".
2022-08-06 16:15:28.438378 (MainThread): On master: BEGIN
2022-08-06 16:15:28.451129 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-06 16:15:28.453733 (MainThread): On master: COMMIT
2022-08-06 16:15:28.457154 (MainThread): Using postgres connection "master".
2022-08-06 16:15:28.457471 (MainThread): On master: COMMIT
2022-08-06 16:15:28.459247 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-06 16:15:28.460253 (MainThread): 16:15:28 | Concurrency: 1 threads (target='dev')
2022-08-06 16:15:28.463758 (MainThread): 16:15:28 | 
2022-08-06 16:15:28.496909 (Thread-1): Began running node model.dbt_.dim_types
2022-08-06 16:15:28.497516 (Thread-1): 16:15:28 | 1 of 3 START table model warehouse.dim_types......................... [RUN]
2022-08-06 16:15:28.498459 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:28.498910 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-06 16:15:28.499222 (Thread-1): Compiling model.dbt_.dim_types
2022-08-06 16:15:28.643017 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-06 16:15:28.645358 (Thread-1): finished collecting timing info
2022-08-06 16:15:28.774957 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:28.775512 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_tmp" cascade
2022-08-06 16:15:28.776874 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-06 16:15:28.792009 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:28.792492 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-06 16:15:28.795037 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-06 16:15:28.885139 (Thread-1): Writing runtime SQL for node "model.dbt_.dim_types"
2022-08-06 16:15:28.886466 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:28.886874 (Thread-1): On model.dbt_.dim_types: BEGIN
2022-08-06 16:15:28.888179 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-06 16:15:28.888718 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:28.888991 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */


  create  table "warehouse"."warehouse"."dim_types__dbt_tmp"
  as (
    
with source_data as (
    select * from warehouse.warehouse.source
),

final as (
    SELECT distinct
    md5(types) as Id,
    types FROM source_data
)

select * from final
  );
2022-08-06 16:15:29.045637 (Thread-1): SQL status: SELECT 6 in 0.16 seconds
2022-08-06 16:15:29.087661 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:29.088087 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types" rename to "dim_types__dbt_backup"
2022-08-06 16:15:29.093353 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-06 16:15:29.108048 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:29.111969 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
alter table "warehouse"."warehouse"."dim_types__dbt_tmp" rename to "dim_types"
2022-08-06 16:15:29.120648 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-06 16:15:29.125410 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-06 16:15:29.125838 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:29.126102 (Thread-1): On model.dbt_.dim_types: COMMIT
2022-08-06 16:15:29.147129 (Thread-1): SQL status: COMMIT in 0.02 seconds
2022-08-06 16:15:29.163791 (Thread-1): Using postgres connection "model.dbt_.dim_types".
2022-08-06 16:15:29.164255 (Thread-1): On model.dbt_.dim_types: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.dim_types"} */
drop table if exists "warehouse"."warehouse"."dim_types__dbt_backup" cascade
2022-08-06 16:15:29.190464 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-06 16:15:29.238668 (Thread-1): finished collecting timing info
2022-08-06 16:15:29.242921 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb571d3a-c924-4db1-b707-17659a988c01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379c95fd10>]}
2022-08-06 16:15:29.249591 (Thread-1): 16:15:29 | 1 of 3 OK created table model warehouse.dim_types.................... [SELECT 6 in 0.74s]
2022-08-06 16:15:29.250103 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-06 16:15:29.253652 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-06 16:15:29.254235 (Thread-1): 16:15:29 | 2 of 3 START table model warehouse.fct_summary....................... [RUN]
2022-08-06 16:15:29.257947 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.265009 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.dim_types).
2022-08-06 16:15:29.265431 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-06 16:15:29.438128 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-06 16:15:29.439146 (Thread-1): finished collecting timing info
2022-08-06 16:15:29.576351 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.576922 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_tmp" cascade
2022-08-06 16:15:29.577963 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-06 16:15:29.630133 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.630551 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-06 16:15:29.646334 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-06 16:15:29.659165 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_summary"
2022-08-06 16:15:29.681355 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.681867 (Thread-1): On model.dbt_.fct_summary: BEGIN
2022-08-06 16:15:29.682638 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-06 16:15:29.683038 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.683297 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */


  create  table "warehouse"."warehouse"."fct_summary__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, traveled_d, avg_speed from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.traveled_d, sel.avg_speed
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-06 16:15:29.753122 (Thread-1): SQL status: SELECT 922 in 0.07 seconds
2022-08-06 16:15:29.793640 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.794067 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary" rename to "fct_summary__dbt_backup"
2022-08-06 16:15:29.800747 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-06 16:15:29.843423 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.843847 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
alter table "warehouse"."warehouse"."fct_summary__dbt_tmp" rename to "fct_summary"
2022-08-06 16:15:29.849371 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-06 16:15:29.868665 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-06 16:15:29.869121 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.869392 (Thread-1): On model.dbt_.fct_summary: COMMIT
2022-08-06 16:15:29.901664 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-08-06 16:15:29.949239 (Thread-1): Using postgres connection "model.dbt_.fct_summary".
2022-08-06 16:15:29.949682 (Thread-1): On model.dbt_.fct_summary: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_summary"} */
drop table if exists "warehouse"."warehouse"."fct_summary__dbt_backup" cascade
2022-08-06 16:15:29.983186 (Thread-1): SQL status: DROP TABLE in 0.03 seconds
2022-08-06 16:15:30.035530 (Thread-1): finished collecting timing info
2022-08-06 16:15:30.043770 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb571d3a-c924-4db1-b707-17659a988c01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379c95fd10>]}
2022-08-06 16:15:30.070847 (Thread-1): 16:15:30 | 2 of 3 OK created table model warehouse.fct_summary.................. [SELECT 922 in 0.79s]
2022-08-06 16:15:30.071364 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-06 16:15:30.077064 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-06 16:15:30.082510 (Thread-1): 16:15:30 | 3 of 3 START table model warehouse.fct_trajectory.................... [RUN]
2022-08-06 16:15:30.083546 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:30.083893 (Thread-1): Re-using an available connection from the pool (formerly model.dbt_.fct_summary).
2022-08-06 16:15:30.084173 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-06 16:15:30.324250 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-06 16:15:30.327138 (Thread-1): finished collecting timing info
2022-08-06 16:15:30.438392 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:30.439288 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_tmp" cascade
2022-08-06 16:15:30.452967 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-06 16:15:30.469515 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:30.469932 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-06 16:15:30.482928 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-06 16:15:30.488034 (Thread-1): Writing runtime SQL for node "model.dbt_.fct_trajectory"
2022-08-06 16:15:30.501917 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:30.502339 (Thread-1): On model.dbt_.fct_trajectory: BEGIN
2022-08-06 16:15:30.512071 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-06 16:15:30.512852 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:30.513279 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */


  create  table "warehouse"."warehouse"."fct_trajectory__dbt_tmp"
  as (
    

with source_data as (
    select * from warehouse.warehouse.source
),

selection as (
    select track_id, md5(types) as type_id, trajectory from source_data
),

dim_types as (
    select * from "warehouse"."warehouse"."dim_types"
), 

final as (
    select sel.track_id, dim_types.types, sel.trajectory as paths
    from selection as sel 
    LEFT JOIN dim_types on sel.type_id = dim_types.Id
)

select * from final
  );
2022-08-06 16:15:32.586844 (Thread-1): SQL status: SELECT 922 in 2.07 seconds
2022-08-06 16:15:32.647954 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:32.648437 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory" rename to "fct_trajectory__dbt_backup"
2022-08-06 16:15:32.657707 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-06 16:15:32.680172 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:32.686323 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
alter table "warehouse"."warehouse"."fct_trajectory__dbt_tmp" rename to "fct_trajectory"
2022-08-06 16:15:32.687624 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-06 16:15:32.695409 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-06 16:15:32.695824 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:32.696084 (Thread-1): On model.dbt_.fct_trajectory: COMMIT
2022-08-06 16:15:32.738576 (Thread-1): SQL status: COMMIT in 0.04 seconds
2022-08-06 16:15:32.759934 (Thread-1): Using postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:15:32.760395 (Thread-1): On model.dbt_.fct_trajectory: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "model.dbt_.fct_trajectory"} */
drop table if exists "warehouse"."warehouse"."fct_trajectory__dbt_backup" cascade
2022-08-06 16:15:32.785380 (Thread-1): SQL status: DROP TABLE in 0.02 seconds
2022-08-06 16:15:32.805611 (Thread-1): finished collecting timing info
2022-08-06 16:15:32.815649 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'bb571d3a-c924-4db1-b707-17659a988c01', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379ca1ce10>]}
2022-08-06 16:15:32.816620 (Thread-1): 16:15:32 | 3 of 3 OK created table model warehouse.fct_trajectory............... [SELECT 922 in 2.73s]
2022-08-06 16:15:32.818301 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-06 16:15:32.838534 (MainThread): Using postgres connection "master".
2022-08-06 16:15:32.838976 (MainThread): On master: BEGIN
2022-08-06 16:15:32.841233 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-06 16:15:32.841720 (MainThread): On master: COMMIT
2022-08-06 16:15:32.841992 (MainThread): Using postgres connection "master".
2022-08-06 16:15:32.842336 (MainThread): On master: COMMIT
2022-08-06 16:15:32.843508 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-06 16:15:32.847696 (MainThread): 16:15:32 | 
2022-08-06 16:15:32.848204 (MainThread): 16:15:32 | Finished running 3 table models in 6.73s.
2022-08-06 16:15:32.848984 (MainThread): Connection 'master' was left open.
2022-08-06 16:15:32.849333 (MainThread): On master: Close
2022-08-06 16:15:32.850341 (MainThread): Connection 'model.dbt_.fct_trajectory' was left open.
2022-08-06 16:15:32.850754 (MainThread): On model.dbt_.fct_trajectory: Close
2022-08-06 16:15:32.934678 (MainThread): 
2022-08-06 16:15:32.935203 (MainThread): Completed successfully
2022-08-06 16:15:32.935581 (MainThread): 
Done. PASS=3 WARN=0 ERROR=0 SKIP=0 TOTAL=3
2022-08-06 16:15:32.936185 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379cbfbd50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379cadec90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f379cb8e610>]}
2022-08-06 16:15:32.940952 (MainThread): Flushing usage events
2022-08-06 16:16:23.795317 (MainThread): Running with dbt=0.16.1
2022-08-06 16:16:24.881253 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-06 16:16:24.882241 (MainThread): Tracking: tracking
2022-08-06 16:16:24.941913 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a74768490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a74788690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a7484f310>]}
2022-08-06 16:16:25.411128 (MainThread): Partial parsing not enabled
2022-08-06 16:16:25.440419 (MainThread): Parsing macros/core.sql
2022-08-06 16:16:25.563911 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-06 16:16:25.878907 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-06 16:16:25.931633 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-06 16:16:26.879770 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-06 16:16:27.337273 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-06 16:16:27.482623 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-06 16:16:27.557020 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-06 16:16:27.970660 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-06 16:16:28.080191 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-06 16:16:28.106670 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-06 16:16:28.151991 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-06 16:16:28.256855 (MainThread): Parsing macros/adapters/common.sql
2022-08-06 16:16:28.475638 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-06 16:16:28.481961 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-06 16:16:28.491335 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-06 16:16:28.502860 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-06 16:16:28.507422 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-06 16:16:28.516645 (MainThread): Parsing macros/etc/query.sql
2022-08-06 16:16:28.521728 (MainThread): Parsing macros/etc/datetime.sql
2022-08-06 16:16:28.569279 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-06 16:16:28.573760 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-06 16:16:28.584291 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-06 16:16:28.589633 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-06 16:16:28.597176 (MainThread): Parsing macros/relations.sql
2022-08-06 16:16:28.603957 (MainThread): Parsing macros/adapters.sql
2022-08-06 16:16:28.879623 (MainThread): Parsing macros/catalog.sql
2022-08-06 16:16:28.906810 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-06 16:16:29.038975 (MainThread): Partial parsing not enabled
2022-08-06 16:16:29.368742 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-06 16:16:29.372516 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:16:29.500064 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-06 16:16:29.504781 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:16:29.573910 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:16:29.574314 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:16:30.164030 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:16:30.188683 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:16:32.327088 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-06 16:16:34.883074 (MainThread): scipy not found, skipping conversion test.
2022-08-06 16:16:34.898376 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-06 16:16:34.899402 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-06 16:16:34.921790 (MainThread): 
2022-08-06 16:16:34.924254 (MainThread): Acquiring new postgres connection "master".
2022-08-06 16:16:34.928888 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:16:35.563762 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_warehouse_warehouse".
2022-08-06 16:16:35.564212 (ThreadPoolExecutor-1_0): Opening a new connection, currently in state init
2022-08-06 16:16:37.198980 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-06 16:16:37.202332 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: BEGIN
2022-08-06 16:16:37.222860 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.02 seconds
2022-08-06 16:16:37.223317 (ThreadPoolExecutor-1_0): Using postgres connection "list_warehouse_warehouse".
2022-08-06 16:16:37.223586 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "list_warehouse_warehouse"} */
select
      'warehouse' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'warehouse'
    union all
    select
      'warehouse' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'warehouse'
  
2022-08-06 16:16:37.232212 (ThreadPoolExecutor-1_0): SQL status: SELECT 4 in 0.01 seconds
2022-08-06 16:16:37.296232 (ThreadPoolExecutor-1_0): On list_warehouse_warehouse: ROLLBACK
2022-08-06 16:16:37.841662 (MainThread): Using postgres connection "master".
2022-08-06 16:16:37.842101 (MainThread): On master: BEGIN
2022-08-06 16:16:37.866845 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-06 16:16:37.867295 (MainThread): Using postgres connection "master".
2022-08-06 16:16:37.867561 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-06 16:16:37.932690 (MainThread): SQL status: SELECT 0 in 0.06 seconds
2022-08-06 16:16:37.935788 (MainThread): On master: ROLLBACK
2022-08-06 16:16:37.956725 (MainThread): 16:16:37 | Concurrency: 1 threads (target='dev')
2022-08-06 16:16:37.957279 (MainThread): 16:16:37 | 
2022-08-06 16:16:38.116847 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-06 16:16:38.117472 (Thread-1): 16:16:38 | 1 of 1 START test unique_dim_types_Id................................ [RUN]
2022-08-06 16:16:38.118466 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:16:38.118922 (Thread-1): Re-using an available connection from the pool (formerly list_warehouse_warehouse).
2022-08-06 16:16:38.125262 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-06 16:16:38.615418 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-06 16:16:38.636914 (Thread-1): finished collecting timing info
2022-08-06 16:16:38.656725 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:16:38.657193 (Thread-1): On test.dbt_.unique_dim_types_Id: BEGIN
2022-08-06 16:16:38.664690 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-06 16:16:38.665154 (Thread-1): Using postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:16:38.665472 (Thread-1): On test.dbt_.unique_dim_types_Id: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "node_id": "test.dbt_.unique_dim_types_Id"} */




select count(*)
from (

    select
        Id

    from "warehouse"."warehouse"."dim_types"
    where Id is not null
    group by Id
    having count(*) > 1

) validation_errors


2022-08-06 16:16:38.784668 (Thread-1): SQL status: SELECT 1 in 0.12 seconds
2022-08-06 16:16:38.785688 (Thread-1): finished collecting timing info
2022-08-06 16:16:38.786618 (Thread-1): On test.dbt_.unique_dim_types_Id: ROLLBACK
2022-08-06 16:16:38.804085 (Thread-1): 16:16:38 | 1 of 1 PASS unique_dim_types_Id...................................... [PASS in 0.69s]
2022-08-06 16:16:38.810870 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-06 16:16:38.914055 (MainThread): 16:16:38 | 
2022-08-06 16:16:38.914572 (MainThread): 16:16:38 | Finished running 1 test in 3.99s.
2022-08-06 16:16:38.920104 (MainThread): Connection 'master' was left open.
2022-08-06 16:16:38.920557 (MainThread): On master: Close
2022-08-06 16:16:38.921115 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was left open.
2022-08-06 16:16:38.921505 (MainThread): On test.dbt_.unique_dim_types_Id: Close
2022-08-06 16:16:38.992305 (MainThread): 
2022-08-06 16:16:39.022120 (MainThread): Completed successfully
2022-08-06 16:16:39.023016 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-06 16:16:39.024181 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a71f50790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a71eb6f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8a71eb2c50>]}
2022-08-06 16:16:39.024956 (MainThread): Flushing usage events
2022-08-06 16:17:02.114118 (MainThread): Running with dbt=0.16.1
2022-08-06 16:17:02.779525 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.generate.GenerateTask'>, compile=True, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='docs.generate', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='generate', write_json=True)
2022-08-06 16:17:02.788722 (MainThread): Tracking: tracking
2022-08-06 16:17:02.829836 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75b870d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75b963b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75b953a50>]}
2022-08-06 16:17:03.061896 (MainThread): Partial parsing not enabled
2022-08-06 16:17:03.071396 (MainThread): Parsing macros/core.sql
2022-08-06 16:17:03.121976 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-06 16:17:03.207252 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-06 16:17:03.215300 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-06 16:17:03.458183 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-06 16:17:03.604624 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-06 16:17:03.668789 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-06 16:17:03.677657 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-06 16:17:03.869763 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-06 16:17:03.985841 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-06 16:17:04.033890 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-06 16:17:04.093906 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-06 16:17:04.154701 (MainThread): Parsing macros/adapters/common.sql
2022-08-06 16:17:04.585531 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-06 16:17:04.593365 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-06 16:17:04.607078 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-06 16:17:04.630859 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-06 16:17:04.635420 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-06 16:17:04.647474 (MainThread): Parsing macros/etc/query.sql
2022-08-06 16:17:04.668968 (MainThread): Parsing macros/etc/datetime.sql
2022-08-06 16:17:04.745645 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-06 16:17:04.754782 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-06 16:17:04.774619 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-06 16:17:04.789065 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-06 16:17:04.796388 (MainThread): Parsing macros/relations.sql
2022-08-06 16:17:04.811303 (MainThread): Parsing macros/adapters.sql
2022-08-06 16:17:04.951892 (MainThread): Parsing macros/catalog.sql
2022-08-06 16:17:04.977703 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-06 16:17:05.152283 (MainThread): Partial parsing not enabled
2022-08-06 16:17:05.430184 (MainThread): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-06 16:17:05.430585 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:17:05.622391 (MainThread): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-06 16:17:05.622799 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:17:05.685921 (MainThread): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:17:05.686332 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:17:05.847357 (MainThread): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:17:05.847765 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:17:07.113752 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-06 16:17:10.184489 (MainThread): scipy not found, skipping conversion test.
2022-08-06 16:17:10.196290 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-06 16:17:10.197479 (MainThread): Found 3 models, 1 test, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 1 source
2022-08-06 16:17:10.209471 (MainThread): 
2022-08-06 16:17:10.210710 (MainThread): 16:17:10 | Concurrency: 1 threads (target='dev')
2022-08-06 16:17:10.211548 (MainThread): 16:17:10 | 
2022-08-06 16:17:10.232176 (Thread-1): Began running node model.dbt_.dim_types
2022-08-06 16:17:10.233420 (Thread-1): Acquiring new postgres connection "model.dbt_.dim_types".
2022-08-06 16:17:10.233817 (Thread-1): Opening a new connection, currently in state init
2022-08-06 16:17:10.234116 (Thread-1): Compiling model.dbt_.dim_types
2022-08-06 16:17:10.489502 (Thread-1): Writing injected SQL for node "model.dbt_.dim_types"
2022-08-06 16:17:10.490521 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.491603 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.497337 (Thread-1): Finished running node model.dbt_.dim_types
2022-08-06 16:17:10.498626 (Thread-1): Began running node model.dbt_.fct_summary
2022-08-06 16:17:10.499636 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_summary".
2022-08-06 16:17:10.500005 (Thread-1): Opening a new connection, currently in state init
2022-08-06 16:17:10.500289 (Thread-1): Compiling model.dbt_.fct_summary
2022-08-06 16:17:10.559086 (Thread-1): Writing injected SQL for node "model.dbt_.fct_summary"
2022-08-06 16:17:10.560100 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.561245 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.562748 (Thread-1): Finished running node model.dbt_.fct_summary
2022-08-06 16:17:10.563341 (Thread-1): Began running node model.dbt_.fct_trajectory
2022-08-06 16:17:10.564666 (Thread-1): Acquiring new postgres connection "model.dbt_.fct_trajectory".
2022-08-06 16:17:10.565074 (Thread-1): Opening a new connection, currently in state init
2022-08-06 16:17:10.565360 (Thread-1): Compiling model.dbt_.fct_trajectory
2022-08-06 16:17:10.682643 (Thread-1): Writing injected SQL for node "model.dbt_.fct_trajectory"
2022-08-06 16:17:10.683648 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.684741 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.688065 (Thread-1): Finished running node model.dbt_.fct_trajectory
2022-08-06 16:17:10.695050 (Thread-1): Began running node test.dbt_.unique_dim_types_Id
2022-08-06 16:17:10.696432 (Thread-1): Acquiring new postgres connection "test.dbt_.unique_dim_types_Id".
2022-08-06 16:17:10.696845 (Thread-1): Opening a new connection, currently in state init
2022-08-06 16:17:10.697136 (Thread-1): Compiling test.dbt_.unique_dim_types_Id
2022-08-06 16:17:10.821538 (Thread-1): Writing injected SQL for node "test.dbt_.unique_dim_types_Id"
2022-08-06 16:17:10.822546 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.823589 (Thread-1): finished collecting timing info
2022-08-06 16:17:10.837295 (Thread-1): Finished running node test.dbt_.unique_dim_types_Id
2022-08-06 16:17:10.905921 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-06 16:17:10.906365 (MainThread): Connection 'test.dbt_.unique_dim_types_Id' was properly closed.
2022-08-06 16:17:10.990302 (MainThread): 16:17:10 | Done.
2022-08-06 16:17:11.023567 (MainThread): Acquiring new postgres connection "generate_catalog".
2022-08-06 16:17:11.023986 (MainThread): Opening a new connection, currently in state init
2022-08-06 16:17:11.024268 (MainThread): 16:17:11 | Building catalog
2022-08-06 16:17:11.243468 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "warehouse.information_schema".
2022-08-06 16:17:11.244199 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-06 16:17:11.957212 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-06 16:17:11.957647 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-06 16:17:11.977736 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.02 seconds
2022-08-06 16:17:11.978178 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-06 16:17:11.978437 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-06 16:17:12.041250 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.06 seconds
2022-08-06 16:17:12.075610 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-06 16:17:12.106187 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-06 16:17:12.106622 (ThreadPoolExecutor-0_0): On warehouse.information_schema: BEGIN
2022-08-06 16:17:12.113292 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.01 seconds
2022-08-06 16:17:12.113819 (ThreadPoolExecutor-0_0): Using postgres connection "warehouse.information_schema".
2022-08-06 16:17:12.114085 (ThreadPoolExecutor-0_0): On warehouse.information_schema: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "dbt_", "target_name": "dev", "connection_name": "warehouse.information_schema"} */

    
    

    select
        'warehouse' as table_database,
        sch.nspname as table_schema,
        tbl.relname as table_name,
        case tbl.relkind
            when 'v' then 'VIEW'
            else 'BASE TABLE'
        end as table_type,
        null::text as table_comment,
        col.attname as column_name,
        col.attnum as column_index,
        pg_catalog.format_type(col.atttypid, col.atttypmod) as column_type,
        null::text as column_comment,
        pg_get_userbyid(tbl.relowner) as table_owner

    from pg_catalog.pg_namespace sch
    join pg_catalog.pg_class tbl on tbl.relnamespace = sch.oid
    join pg_catalog.pg_attribute col on col.attrelid = tbl.oid

    where (upper(sch.nspname) = upper('warehouse'))
      and not pg_is_other_temp_schema(sch.oid) -- not a temporary schema belonging to another session
      and tbl.relpersistence = 'p' -- [p]ermanent table. Other values are [u]nlogged table, [t]emporary table
      and tbl.relkind in ('r', 'v', 'f', 'p') -- o[r]dinary table, [v]iew, [f]oreign table, [p]artitioned table. Other values are [i]ndex, [S]equence, [c]omposite type, [t]OAST table, [m]aterialized view
      and col.attnum > 0 -- negative numbers are used for system columns such as oid
      and not col.attisdropped -- column as not been dropped

    order by
        sch.nspname,
        tbl.relname,
        col.attnum
2022-08-06 16:17:12.120866 (ThreadPoolExecutor-0_0): SQL status: SELECT 14 in 0.01 seconds
2022-08-06 16:17:12.126177 (ThreadPoolExecutor-0_0): On warehouse.information_schema: ROLLBACK
2022-08-06 16:17:12.914256 (MainThread): 16:17:12 | Catalog written to /usr/local/airflow/dbt_/dbt_/target/catalog.json
2022-08-06 16:17:12.915082 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa758f73c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7590aa090>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75901e610>]}
2022-08-06 16:17:12.916424 (MainThread): Flushing usage events
2022-08-06 16:17:13.755331 (MainThread): Connection 'generate_catalog' was properly closed.
2022-08-06 16:17:13.761686 (MainThread): Connection 'warehouse.information_schema' was left open.
2022-08-06 16:17:13.762074 (MainThread): On warehouse.information_schema: Close
2022-08-06 16:17:21.379233 (MainThread): Running with dbt=0.16.1
2022-08-06 16:17:22.148060 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.serve.ServeTask'>, debug=False, log_cache_events=False, log_format='default', partial_parse=None, port=7211, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method=None, single_threaded=False, strict=False, target=None, test_new_parser=False, use_cache=True, vars='{}', warn_error=False, which='serve', write_json=True)
2022-08-06 16:17:22.157218 (MainThread): Tracking: tracking
2022-08-06 16:17:22.201868 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f561157de10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56114cdc50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f56114cd190>]}
2022-08-06 16:17:22.225512 (MainThread): Serving docs at 0.0.0.0:7211
2022-08-06 16:17:22.226017 (MainThread): To access from your browser, navigate to:  http://localhost:7211
2022-08-06 16:17:22.226295 (MainThread): Press Ctrl+C to exit.


2022-08-07 18:54:19.909345 (MainThread): Running with dbt=0.16.1
2022-08-07 18:54:20.636803 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 18:54:20.643725 (MainThread): Tracking: tracking
2022-08-07 18:54:20.733216 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16d0927b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16d0a33ed0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16d0a25d50>]}
2022-08-07 18:54:20.919160 (MainThread): Partial parsing not enabled
2022-08-07 18:54:20.975888 (MainThread): Parsing macros/core.sql
2022-08-07 18:54:21.075604 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 18:54:21.194987 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 18:54:21.219423 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 18:54:21.455757 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 18:54:21.569949 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 18:54:21.761586 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 18:54:21.877869 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 18:54:22.406639 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 18:54:22.564247 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 18:54:22.768379 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 18:54:22.926133 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 18:54:23.075709 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 18:54:23.477126 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 18:54:23.496051 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 18:54:23.512735 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 18:54:23.559418 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 18:54:23.581921 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 18:54:23.612786 (MainThread): Parsing macros/etc/query.sql
2022-08-07 18:54:23.625007 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 18:54:23.703086 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 18:54:23.716227 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 18:54:23.752088 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 18:54:23.779008 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 18:54:23.836241 (MainThread): Parsing macros/relations.sql
2022-08-07 18:54:23.867731 (MainThread): Parsing macros/adapters.sql
2022-08-07 18:54:24.028348 (MainThread): Parsing macros/catalog.sql
2022-08-07 18:54:24.061733 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 18:54:24.325500 (MainThread): Partial parsing not enabled
2022-08-07 18:54:24.795682 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:24.796110 (MainThread): Opening a new connection, currently in state init
2022-08-07 18:54:27.178637 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 18:54:30.130087 (MainThread): scipy not found, skipping conversion test.
2022-08-07 18:54:30.162681 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 18:54:30.169484 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 18:54:30.197629 (MainThread): 
2022-08-07 18:54:30.198998 (MainThread): Acquiring new postgres connection "master".
2022-08-07 18:54:30.208507 (MainThread): Opening a new connection, currently in state init
2022-08-07 18:54:30.311011 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 18:54:30.311860 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 18:54:31.644800 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 18:54:31.645257 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 18:54:31.689544 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.03 seconds
2022-08-07 18:54:31.717652 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_staging_staging".
2022-08-07 18:54:31.722906 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 18:54:31.723342 (ThreadPoolExecutor-0_0): Creating schema "staging"."staging".
2022-08-07 18:54:31.960279 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-07 18:54:31.960742 (ThreadPoolExecutor-0_0): On create_staging_staging: BEGIN
2022-08-07 18:54:31.964716 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:54:31.965203 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-07 18:54:31.967985 (ThreadPoolExecutor-0_0): On create_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "create_staging_staging"} */
create schema if not exists "staging"
2022-08-07 18:54:32.009678 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.04 seconds
2022-08-07 18:54:32.012837 (ThreadPoolExecutor-0_0): On create_staging_staging: COMMIT
2022-08-07 18:54:32.013272 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-07 18:54:32.014127 (ThreadPoolExecutor-0_0): On create_staging_staging: COMMIT
2022-08-07 18:54:32.035788 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.02 seconds
2022-08-07 18:54:32.103235 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 18:54:32.104034 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly create_staging_staging).
2022-08-07 18:54:32.112484 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 18:54:32.114163 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 18:54:32.115958 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:54:32.116439 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 18:54:32.116710 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 18:54:32.146337 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 0.03 seconds
2022-08-07 18:54:32.149141 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 18:54:32.229693 (MainThread): Using postgres connection "master".
2022-08-07 18:54:32.230148 (MainThread): On master: BEGIN
2022-08-07 18:54:32.259803 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-07 18:54:32.260279 (MainThread): Using postgres connection "master".
2022-08-07 18:54:32.260555 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 18:54:32.389503 (MainThread): SQL status: SELECT 0 in 0.13 seconds
2022-08-07 18:54:32.401057 (MainThread): On master: ROLLBACK
2022-08-07 18:54:32.407511 (MainThread): Using postgres connection "master".
2022-08-07 18:54:32.407995 (MainThread): On master: BEGIN
2022-08-07 18:54:32.409272 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:54:32.409809 (MainThread): On master: COMMIT
2022-08-07 18:54:32.410098 (MainThread): Using postgres connection "master".
2022-08-07 18:54:32.410357 (MainThread): On master: COMMIT
2022-08-07 18:54:32.414544 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 18:54:32.417997 (MainThread): 18:54:32 | Concurrency: 1 threads (target='dev')
2022-08-07 18:54:32.418527 (MainThread): 18:54:32 | 
2022-08-07 18:54:32.486252 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 18:54:32.487147 (Thread-1): 18:54:32 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 18:54:32.488825 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:32.489466 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 18:54:32.489967 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 18:54:32.664093 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 18:54:32.671411 (Thread-1): finished collecting timing info
2022-08-07 18:54:32.896027 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:32.901943 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 18:54:32.903382 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 18:54:32.911701 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:32.920861 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 18:54:32.922150 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 18:54:33.055996 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 18:54:33.062650 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:33.063041 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 18:54:33.064967 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:54:33.066149 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:54:33.066661 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
with source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT *
    FROM
        source_camp
    INNER JOIN source_brief ON source_camp.campaign_id = brief.campaign_id;
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 18:54:33.073764 (Thread-1): Postgres error: syntax error at or near "with"
LINE 11: with source_brief as (
         ^

2022-08-07 18:54:33.074252 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 18:54:33.077730 (Thread-1): finished collecting timing info
2022-08-07 18:54:33.079373 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near "with"
  LINE 11: with source_brief as (
           ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "with"
LINE 11: with source_brief as (
         ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near "with"
  LINE 11: with source_brief as (
           ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 18:54:33.156562 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '2fe5fc94-9313-480a-a1d6-6b5debff40f2', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16ce14b390>]}
2022-08-07 18:54:33.158740 (Thread-1): 18:54:33 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.67s]
2022-08-07 18:54:33.159682 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 18:54:33.213633 (MainThread): Using postgres connection "master".
2022-08-07 18:54:33.214107 (MainThread): On master: BEGIN
2022-08-07 18:54:33.216054 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:54:33.216545 (MainThread): On master: COMMIT
2022-08-07 18:54:33.216820 (MainThread): Using postgres connection "master".
2022-08-07 18:54:33.217109 (MainThread): On master: COMMIT
2022-08-07 18:54:33.222880 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-07 18:54:33.224210 (MainThread): 18:54:33 | 
2022-08-07 18:54:33.224694 (MainThread): 18:54:33 | Finished running 1 table model in 3.03s.
2022-08-07 18:54:33.229970 (MainThread): Connection 'master' was left open.
2022-08-07 18:54:33.230352 (MainThread): On master: Close
2022-08-07 18:54:33.230846 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 18:54:33.232743 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 18:54:33.275377 (MainThread): 
2022-08-07 18:54:33.275937 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 18:54:33.286156 (MainThread): 
2022-08-07 18:54:33.286553 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 18:54:33.286890 (MainThread):   syntax error at or near "with"
2022-08-07 18:54:33.287192 (MainThread):   LINE 11: with source_brief as (
2022-08-07 18:54:33.287739 (MainThread):            ^
2022-08-07 18:54:33.288131 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 18:54:33.288464 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 18:54:33.289020 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16bf790690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16bf790810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f16bf790610>]}
2022-08-07 18:54:33.289753 (MainThread): Flushing usage events
2022-08-07 18:57:29.282301 (MainThread): Running with dbt=0.16.1
2022-08-07 18:57:30.455474 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 18:57:30.476341 (MainThread): Tracking: tracking
2022-08-07 18:57:30.561032 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1874c791d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1874db0210>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1874c650d0>]}
2022-08-07 18:57:30.812310 (MainThread): Partial parsing not enabled
2022-08-07 18:57:30.826968 (MainThread): Parsing macros/core.sql
2022-08-07 18:57:30.911274 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 18:57:31.044809 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 18:57:31.076813 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 18:57:31.361122 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 18:57:31.445206 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 18:57:31.475338 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 18:57:31.484736 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 18:57:31.601324 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 18:57:31.674653 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 18:57:31.707088 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 18:57:31.745180 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 18:57:31.784094 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 18:57:32.009857 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 18:57:32.014668 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 18:57:32.025262 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 18:57:32.037081 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 18:57:32.041546 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 18:57:32.069023 (MainThread): Parsing macros/etc/query.sql
2022-08-07 18:57:32.075441 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 18:57:32.140188 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 18:57:32.146792 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 18:57:32.159687 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 18:57:32.165250 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 18:57:32.194887 (MainThread): Parsing macros/relations.sql
2022-08-07 18:57:32.210340 (MainThread): Parsing macros/adapters.sql
2022-08-07 18:57:32.346238 (MainThread): Parsing macros/catalog.sql
2022-08-07 18:57:32.362501 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 18:57:32.550176 (MainThread): Partial parsing not enabled
2022-08-07 18:57:32.911280 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:32.911950 (MainThread): Opening a new connection, currently in state init
2022-08-07 18:57:34.564947 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 18:57:36.286007 (MainThread): scipy not found, skipping conversion test.
2022-08-07 18:57:36.295544 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 18:57:36.296914 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 18:57:36.313991 (MainThread): 
2022-08-07 18:57:36.315345 (MainThread): Acquiring new postgres connection "master".
2022-08-07 18:57:36.315924 (MainThread): Opening a new connection, currently in state init
2022-08-07 18:57:36.353839 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 18:57:36.354755 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 18:57:36.987225 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 18:57:36.987886 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 18:57:37.040359 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.05 seconds
2022-08-07 18:57:37.292225 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 18:57:37.292862 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 18:57:37.312433 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 18:57:37.314147 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 18:57:37.320179 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:57:37.320774 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 18:57:37.321055 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 18:57:37.334130 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.01 seconds
2022-08-07 18:57:37.362101 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 18:57:37.481825 (MainThread): Using postgres connection "master".
2022-08-07 18:57:37.485685 (MainThread): On master: BEGIN
2022-08-07 18:57:37.529934 (MainThread): SQL status: BEGIN in 0.04 seconds
2022-08-07 18:57:37.534216 (MainThread): Using postgres connection "master".
2022-08-07 18:57:37.534627 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 18:57:37.574455 (MainThread): SQL status: SELECT 0 in 0.04 seconds
2022-08-07 18:57:37.582857 (MainThread): On master: ROLLBACK
2022-08-07 18:57:37.586734 (MainThread): Using postgres connection "master".
2022-08-07 18:57:37.587184 (MainThread): On master: BEGIN
2022-08-07 18:57:37.594849 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 18:57:37.595380 (MainThread): On master: COMMIT
2022-08-07 18:57:37.595706 (MainThread): Using postgres connection "master".
2022-08-07 18:57:37.600393 (MainThread): On master: COMMIT
2022-08-07 18:57:37.604249 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 18:57:37.611248 (MainThread): 18:57:37 | Concurrency: 1 threads (target='dev')
2022-08-07 18:57:37.611863 (MainThread): 18:57:37 | 
2022-08-07 18:57:37.669726 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 18:57:37.670923 (Thread-1): 18:57:37 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 18:57:37.678913 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:37.679360 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 18:57:37.680155 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 18:57:37.847521 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 18:57:37.856219 (Thread-1): finished collecting timing info
2022-08-07 18:57:38.186640 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:38.187171 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 18:57:38.196458 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 18:57:38.234723 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:38.238771 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 18:57:38.242713 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 18:57:38.486174 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 18:57:38.489134 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:38.491325 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 18:57:38.494346 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:57:38.497973 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 18:57:38.500715 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT *
    FROM
        source_camp
    INNER JOIN source_brief ON source_camp.campaign_id = brief.campaign_id;
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 18:57:38.510443 (Thread-1): Postgres error: syntax error at or near ";"
LINE 18: ...source_brief ON source_camp.campaign_id = brief.campaign_id;
                                                                       ^

2022-08-07 18:57:38.510958 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 18:57:38.513020 (Thread-1): finished collecting timing info
2022-08-07 18:57:38.514636 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near ";"
  LINE 18: ...source_brief ON source_camp.campaign_id = brief.campaign_id;
                                                                         ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near ";"
LINE 18: ...source_brief ON source_camp.campaign_id = brief.campaign_id;
                                                                       ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near ";"
  LINE 18: ...source_brief ON source_camp.campaign_id = brief.campaign_id;
                                                                         ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 18:57:38.537516 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ba196dec-3c0d-4d8d-83b2-b68c929d3c86', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18724de110>]}
2022-08-07 18:57:38.539329 (Thread-1): 18:57:38 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.87s]
2022-08-07 18:57:38.541738 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 18:57:38.638753 (MainThread): Using postgres connection "master".
2022-08-07 18:57:38.639916 (MainThread): On master: BEGIN
2022-08-07 18:57:38.640987 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 18:57:38.642261 (MainThread): On master: COMMIT
2022-08-07 18:57:38.642833 (MainThread): Using postgres connection "master".
2022-08-07 18:57:38.643270 (MainThread): On master: COMMIT
2022-08-07 18:57:38.644824 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 18:57:38.646282 (MainThread): 18:57:38 | 
2022-08-07 18:57:38.648335 (MainThread): 18:57:38 | Finished running 1 table model in 2.33s.
2022-08-07 18:57:38.650851 (MainThread): Connection 'master' was left open.
2022-08-07 18:57:38.651375 (MainThread): On master: Close
2022-08-07 18:57:38.658592 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 18:57:38.660960 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 18:57:38.739996 (MainThread): 
2022-08-07 18:57:38.740542 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 18:57:38.740912 (MainThread): 
2022-08-07 18:57:38.741264 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 18:57:38.743715 (MainThread):   syntax error at or near ";"
2022-08-07 18:57:38.746184 (MainThread):   LINE 18: ...source_brief ON source_camp.campaign_id = brief.campaign_id;
2022-08-07 18:57:38.754565 (MainThread):                                                                          ^
2022-08-07 18:57:38.755098 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 18:57:38.755449 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 18:57:38.756122 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1863a666d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1863a66f50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1863a661d0>]}
2022-08-07 18:57:38.757268 (MainThread): Flushing usage events
2022-08-07 19:01:11.794788 (MainThread): Running with dbt=0.16.1
2022-08-07 19:01:12.325266 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:01:12.334748 (MainThread): Tracking: tracking
2022-08-07 19:01:12.373642 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae5e7bcf90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae5e973a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae5e7c0ed0>]}
2022-08-07 19:01:12.609553 (MainThread): Partial parsing not enabled
2022-08-07 19:01:12.624154 (MainThread): Parsing macros/core.sql
2022-08-07 19:01:12.669799 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:01:12.747747 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:01:12.766578 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:01:13.046565 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:01:13.209055 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:01:13.282873 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:01:13.291788 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:01:13.489620 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:01:13.615922 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:01:13.676677 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:01:13.756445 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:01:13.833134 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:01:14.197352 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:01:14.223169 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:01:14.264884 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:01:14.310193 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:01:14.314646 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:01:14.340446 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:01:14.353953 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:01:14.433902 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:01:14.449259 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:01:14.492117 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:01:14.505563 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:01:14.512080 (MainThread): Parsing macros/relations.sql
2022-08-07 19:01:14.556115 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:01:14.728389 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:01:14.747761 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:01:14.932384 (MainThread): Partial parsing not enabled
2022-08-07 19:01:15.211215 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:15.221657 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:01:16.340381 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:01:17.851256 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:01:17.868575 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:01:17.874562 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:01:17.891164 (MainThread): 
2022-08-07 19:01:17.892957 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:01:17.893619 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:01:17.944516 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:01:17.945001 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:01:18.816223 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:01:18.819046 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:01:18.840641 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-08-07 19:01:19.117657 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:01:19.118086 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:01:19.133320 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:01:19.137707 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:01:19.139652 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:01:19.140301 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:01:19.141562 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:01:19.159075 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.02 seconds
2022-08-07 19:01:19.172092 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:01:19.248510 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.253721 (MainThread): On master: BEGIN
2022-08-07 19:01:19.267123 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:01:19.267785 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.269175 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:01:19.291041 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-07 19:01:19.294467 (MainThread): On master: ROLLBACK
2022-08-07 19:01:19.295596 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.296161 (MainThread): On master: BEGIN
2022-08-07 19:01:19.298397 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:01:19.299100 (MainThread): On master: COMMIT
2022-08-07 19:01:19.300530 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.305783 (MainThread): On master: COMMIT
2022-08-07 19:01:19.306706 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:01:19.307734 (MainThread): 19:01:19 | Concurrency: 1 threads (target='dev')
2022-08-07 19:01:19.308677 (MainThread): 19:01:19 | 
2022-08-07 19:01:19.330151 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:01:19.330995 (Thread-1): 19:01:19 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:01:19.332577 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:19.333689 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:01:19.334283 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:01:19.410757 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:01:19.412278 (Thread-1): finished collecting timing info
2022-08-07 19:01:19.512223 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:19.512900 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:01:19.514392 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:01:19.531637 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:19.538684 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:01:19.543179 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:01:19.676247 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:01:19.682507 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:19.683191 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:01:19.688700 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:01:19.689359 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:01:19.689849 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT *
    FROM
        source_camp
    INNER JOIN source_brief ON source_camp.campaign_id = brief.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:01:19.696667 (Thread-1): Postgres error: missing FROM-clause entry for table "brief"
LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                              ^

2022-08-07 19:01:19.697498 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 19:01:19.707989 (Thread-1): finished collecting timing info
2022-08-07 19:01:19.709945 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  missing FROM-clause entry for table "brief"
  LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: missing FROM-clause entry for table "brief"
LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  missing FROM-clause entry for table "brief"
  LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:01:19.727985 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cd6fa56f-c65e-4b43-ad75-385f5a63fb10', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae4d6249d0>]}
2022-08-07 19:01:19.728970 (Thread-1): 19:01:19 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.40s]
2022-08-07 19:01:19.730129 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:01:19.746999 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.747634 (MainThread): On master: BEGIN
2022-08-07 19:01:19.748575 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:01:19.749173 (MainThread): On master: COMMIT
2022-08-07 19:01:19.749668 (MainThread): Using postgres connection "master".
2022-08-07 19:01:19.750105 (MainThread): On master: COMMIT
2022-08-07 19:01:19.750914 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:01:19.752212 (MainThread): 19:01:19 | 
2022-08-07 19:01:19.753304 (MainThread): 19:01:19 | Finished running 1 table model in 1.86s.
2022-08-07 19:01:19.754432 (MainThread): Connection 'master' was left open.
2022-08-07 19:01:19.757745 (MainThread): On master: Close
2022-08-07 19:01:19.762922 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:01:19.763608 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:01:19.806486 (MainThread): 
2022-08-07 19:01:19.807592 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 19:01:19.808614 (MainThread): 
2022-08-07 19:01:19.809644 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 19:01:19.810613 (MainThread):   missing FROM-clause entry for table "brief"
2022-08-07 19:01:19.811512 (MainThread):   LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
2022-08-07 19:01:19.812407 (MainThread):                                                                 ^
2022-08-07 19:01:19.813369 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:01:19.814438 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 19:01:19.815763 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae4d5dfdd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae4d5df990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae4d5dfad0>]}
2022-08-07 19:01:19.816649 (MainThread): Flushing usage events
2022-08-07 19:04:51.250419 (MainThread): Running with dbt=0.16.1
2022-08-07 19:04:51.527288 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:04:51.528250 (MainThread): Tracking: tracking
2022-08-07 19:04:51.547979 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14261e4890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14261d9290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f14261d9210>]}
2022-08-07 19:04:51.656966 (MainThread): Partial parsing not enabled
2022-08-07 19:04:51.663423 (MainThread): Parsing macros/core.sql
2022-08-07 19:04:51.688240 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:04:51.773577 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:04:51.786260 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:04:52.075827 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:04:52.261794 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:04:52.327865 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:04:52.346570 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:04:52.460441 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:04:52.550626 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:04:52.574495 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:04:52.616361 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:04:52.675446 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:04:53.105151 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:04:53.110245 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:04:53.120004 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:04:53.130796 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:04:53.135424 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:04:53.150445 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:04:53.160729 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:04:53.269206 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:04:53.283167 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:04:53.302300 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:04:53.308217 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:04:53.314883 (MainThread): Parsing macros/relations.sql
2022-08-07 19:04:53.321909 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:04:53.410780 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:04:53.425605 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:04:53.620794 (MainThread): Partial parsing not enabled
2022-08-07 19:04:53.822109 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:53.822743 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:04:54.594995 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:04:56.439147 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:04:56.451292 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:04:56.452521 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:04:56.462704 (MainThread): 
2022-08-07 19:04:56.464561 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:04:56.465178 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:04:56.528939 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:04:56.529876 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:04:57.144537 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:04:57.145011 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:04:57.163802 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.02 seconds
2022-08-07 19:04:57.290555 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:04:57.290979 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:04:57.306991 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:04:57.307452 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:04:57.309893 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:04:57.310361 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:04:57.310638 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:04:57.326057 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.02 seconds
2022-08-07 19:04:57.340146 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:04:57.385730 (MainThread): Using postgres connection "master".
2022-08-07 19:04:57.386182 (MainThread): On master: BEGIN
2022-08-07 19:04:57.403510 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-07 19:04:57.404199 (MainThread): Using postgres connection "master".
2022-08-07 19:04:57.405367 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:04:57.431891 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-08-07 19:04:57.435100 (MainThread): On master: ROLLBACK
2022-08-07 19:04:57.436081 (MainThread): Using postgres connection "master".
2022-08-07 19:04:57.436476 (MainThread): On master: BEGIN
2022-08-07 19:04:57.439942 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:04:57.440454 (MainThread): On master: COMMIT
2022-08-07 19:04:57.440764 (MainThread): Using postgres connection "master".
2022-08-07 19:04:57.441032 (MainThread): On master: COMMIT
2022-08-07 19:04:57.442511 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:04:57.450012 (MainThread): 19:04:57 | Concurrency: 1 threads (target='dev')
2022-08-07 19:04:57.450989 (MainThread): 19:04:57 | 
2022-08-07 19:04:57.475202 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:04:57.476027 (Thread-1): 19:04:57 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:04:57.479358 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:57.480064 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:04:57.480554 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:04:57.567836 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:04:57.610206 (Thread-1): finished collecting timing info
2022-08-07 19:04:57.698839 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:57.699526 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:04:57.700935 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:04:57.710277 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:57.710913 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:04:57.712011 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:04:57.811736 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:04:57.907421 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:57.907883 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:04:57.908896 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:04:57.909343 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:04:57.909735 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT *
    FROM
        source_camp
    INNER JOIN source_brief ON source_camp.campaign_id = brief.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:04:57.911408 (Thread-1): Postgres error: missing FROM-clause entry for table "brief"
LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                              ^

2022-08-07 19:04:57.911945 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 19:04:57.913500 (Thread-1): finished collecting timing info
2022-08-07 19:04:57.915423 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  missing FROM-clause entry for table "brief"
  LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: missing FROM-clause entry for table "brief"
LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  missing FROM-clause entry for table "brief"
  LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
                                                                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:04:57.926786 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '16d8f55a-52aa-407b-97f5-66c54544285d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1423a16f50>]}
2022-08-07 19:04:57.930067 (Thread-1): 19:04:57 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.45s]
2022-08-07 19:04:57.930627 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:04:58.002487 (MainThread): Using postgres connection "master".
2022-08-07 19:04:58.002985 (MainThread): On master: BEGIN
2022-08-07 19:04:58.005746 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:04:58.006269 (MainThread): On master: COMMIT
2022-08-07 19:04:58.006564 (MainThread): Using postgres connection "master".
2022-08-07 19:04:58.006826 (MainThread): On master: COMMIT
2022-08-07 19:04:58.008743 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:04:58.010048 (MainThread): 19:04:58 | 
2022-08-07 19:04:58.018821 (MainThread): 19:04:58 | Finished running 1 table model in 1.55s.
2022-08-07 19:04:58.019971 (MainThread): Connection 'master' was left open.
2022-08-07 19:04:58.020413 (MainThread): On master: Close
2022-08-07 19:04:58.021042 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:04:58.026104 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:04:58.095144 (MainThread): 
2022-08-07 19:04:58.096155 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 19:04:58.096955 (MainThread): 
2022-08-07 19:04:58.097762 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 19:04:58.098512 (MainThread):   missing FROM-clause entry for table "brief"
2022-08-07 19:04:58.099195 (MainThread):   LINE 18: ...ER JOIN source_brief ON source_camp.campaign_id = brief.camp...
2022-08-07 19:04:58.099892 (MainThread):                                                                 ^
2022-08-07 19:04:58.100554 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:04:58.101628 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 19:04:58.103071 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1414fec510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1414fec1d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1414fec390>]}
2022-08-07 19:04:58.103932 (MainThread): Flushing usage events
2022-08-07 19:07:27.004494 (MainThread): Running with dbt=0.16.1
2022-08-07 19:07:27.793687 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:07:27.794680 (MainThread): Tracking: tracking
2022-08-07 19:07:27.862715 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2a4d2d990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2a4edbf50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff2a4d23c90>]}
2022-08-07 19:07:28.198684 (MainThread): Partial parsing not enabled
2022-08-07 19:07:28.229767 (MainThread): Parsing macros/core.sql
2022-08-07 19:07:28.316519 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:07:28.454837 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:07:28.480904 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:07:29.020738 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:07:29.456156 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:07:29.583080 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:07:29.614962 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:07:29.971013 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:07:30.079235 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:07:30.131493 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:07:30.208292 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:07:30.321193 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:07:31.036703 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:07:31.045029 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:07:31.076126 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:07:31.091993 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:07:31.110721 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:07:31.127274 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:07:31.133070 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:07:31.237965 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:07:31.259050 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:07:31.296541 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:07:31.315140 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:07:31.343935 (MainThread): Parsing macros/relations.sql
2022-08-07 19:07:31.361540 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:07:31.576164 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:07:31.598767 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:07:31.914842 (MainThread): Partial parsing not enabled
2022-08-07 19:07:32.121034 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:32.128248 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:07:33.604572 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:07:35.061708 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:07:35.082748 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:07:35.084165 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:07:35.101363 (MainThread): 
2022-08-07 19:07:35.103201 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:07:35.103803 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:07:35.168215 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:07:35.172873 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:07:36.010942 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:07:36.011574 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:07:36.136262 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.12 seconds
2022-08-07 19:07:36.519162 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:07:36.519819 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:07:36.530616 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:07:36.531094 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:07:36.533639 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:07:36.536139 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:07:36.539523 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:07:36.549785 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.01 seconds
2022-08-07 19:07:36.568945 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:07:36.626497 (MainThread): Using postgres connection "master".
2022-08-07 19:07:36.628530 (MainThread): On master: BEGIN
2022-08-07 19:07:36.642757 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:07:36.643292 (MainThread): Using postgres connection "master".
2022-08-07 19:07:36.643947 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:07:36.663813 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-07 19:07:36.667166 (MainThread): On master: ROLLBACK
2022-08-07 19:07:36.668757 (MainThread): Using postgres connection "master".
2022-08-07 19:07:36.669379 (MainThread): On master: BEGIN
2022-08-07 19:07:36.672118 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:07:36.672991 (MainThread): On master: COMMIT
2022-08-07 19:07:36.673447 (MainThread): Using postgres connection "master".
2022-08-07 19:07:36.673772 (MainThread): On master: COMMIT
2022-08-07 19:07:36.675985 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:07:36.677778 (MainThread): 19:07:36 | Concurrency: 1 threads (target='dev')
2022-08-07 19:07:36.678311 (MainThread): 19:07:36 | 
2022-08-07 19:07:36.701149 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:07:36.701889 (Thread-1): 19:07:36 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:07:36.703901 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:36.704324 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:07:36.704662 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:07:36.831184 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:07:36.835927 (Thread-1): finished collecting timing info
2022-08-07 19:07:36.940381 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:36.941061 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:07:36.953551 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 19:07:36.973379 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:36.984517 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:07:36.987356 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:07:37.076404 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:07:37.077794 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:37.078915 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:07:37.081112 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:07:37.081825 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:07:37.082661 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT *
    FROM source_camp
    INNER JOIN source_brief ON source_camp.campaign_id = source_brief.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:07:37.107759 (Thread-1): Postgres error: column "campaign_id" specified more than once

2022-08-07 19:07:37.113941 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 19:07:37.115937 (Thread-1): finished collecting timing info
2022-08-07 19:07:37.117612 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  column "campaign_id" specified more than once
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "campaign_id" specified more than once


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  column "campaign_id" specified more than once
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:07:37.127172 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'd69fb2cf-4c7c-40ef-9b78-76ce6f5ea51e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff293ba2d10>]}
2022-08-07 19:07:37.127997 (Thread-1): 19:07:37 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.42s]
2022-08-07 19:07:37.128464 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:07:37.227121 (MainThread): Using postgres connection "master".
2022-08-07 19:07:37.227824 (MainThread): On master: BEGIN
2022-08-07 19:07:37.229308 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:07:37.230024 (MainThread): On master: COMMIT
2022-08-07 19:07:37.230492 (MainThread): Using postgres connection "master".
2022-08-07 19:07:37.231413 (MainThread): On master: COMMIT
2022-08-07 19:07:37.232648 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:07:37.234496 (MainThread): 19:07:37 | 
2022-08-07 19:07:37.236589 (MainThread): 19:07:37 | Finished running 1 table model in 2.13s.
2022-08-07 19:07:37.237239 (MainThread): Connection 'master' was left open.
2022-08-07 19:07:37.239194 (MainThread): On master: Close
2022-08-07 19:07:37.246976 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:07:37.249324 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:07:37.307303 (MainThread): 
2022-08-07 19:07:37.307919 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 19:07:37.308307 (MainThread): 
2022-08-07 19:07:37.308657 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 19:07:37.308986 (MainThread):   column "campaign_id" specified more than once
2022-08-07 19:07:37.315243 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:07:37.316069 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 19:07:37.319776 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff293b3b7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff293b3ba90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff293b3b150>]}
2022-08-07 19:07:37.327503 (MainThread): Flushing usage events
2022-08-07 19:16:14.199324 (MainThread): Running with dbt=0.16.1
2022-08-07 19:16:14.753587 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:16:14.754605 (MainThread): Tracking: tracking
2022-08-07 19:16:14.795018 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9c9b0650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9cadb7d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9e424a50>]}
2022-08-07 19:16:14.972594 (MainThread): Partial parsing not enabled
2022-08-07 19:16:14.985167 (MainThread): Parsing macros/core.sql
2022-08-07 19:16:15.030338 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:16:15.110926 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:16:15.127831 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:16:15.416126 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:16:15.585979 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:16:15.654654 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:16:15.683948 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:16:15.829891 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:16:15.977280 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:16:16.047348 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:16:16.150455 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:16:16.199683 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:16:16.404811 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:16:16.410505 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:16:16.420995 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:16:16.431630 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:16:16.436513 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:16:16.444570 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:16:16.450020 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:16:16.494323 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:16:16.498957 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:16:16.508375 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:16:16.513866 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:16:16.520244 (MainThread): Parsing macros/relations.sql
2022-08-07 19:16:16.527032 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:16:16.605182 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:16:16.616591 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:16:16.708099 (MainThread): Partial parsing not enabled
2022-08-07 19:16:16.867395 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:16.867832 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:16:18.064774 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:16:19.622356 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:16:19.631705 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:16:19.632796 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:16:19.641490 (MainThread): 
2022-08-07 19:16:19.643105 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:16:19.643529 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:16:19.667826 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:16:19.668533 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:16:20.062708 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:16:20.063160 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:16:20.078231 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.01 seconds
2022-08-07 19:16:20.266348 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:16:20.266790 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:16:20.288858 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:16:20.289330 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:16:20.291640 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:16:20.292130 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:16:20.292415 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:16:20.313724 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.02 seconds
2022-08-07 19:16:20.330786 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:16:20.421187 (MainThread): Using postgres connection "master".
2022-08-07 19:16:20.421734 (MainThread): On master: BEGIN
2022-08-07 19:16:20.436505 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:16:20.437006 (MainThread): Using postgres connection "master".
2022-08-07 19:16:20.437295 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:16:20.464852 (MainThread): SQL status: SELECT 0 in 0.03 seconds
2022-08-07 19:16:20.468099 (MainThread): On master: ROLLBACK
2022-08-07 19:16:20.470318 (MainThread): Using postgres connection "master".
2022-08-07 19:16:20.470781 (MainThread): On master: BEGIN
2022-08-07 19:16:20.472358 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:16:20.473219 (MainThread): On master: COMMIT
2022-08-07 19:16:20.473630 (MainThread): Using postgres connection "master".
2022-08-07 19:16:20.474161 (MainThread): On master: COMMIT
2022-08-07 19:16:20.477704 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:16:20.478977 (MainThread): 19:16:20 | Concurrency: 1 threads (target='dev')
2022-08-07 19:16:20.479964 (MainThread): 19:16:20 | 
2022-08-07 19:16:20.518905 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:16:20.519592 (Thread-1): 19:16:20 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:16:20.521183 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:20.521653 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:16:20.521988 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:16:20.688479 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:16:20.689678 (Thread-1): finished collecting timing info
2022-08-07 19:16:20.860152 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:20.860658 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:16:20.873629 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 19:16:20.882605 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:20.883058 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:16:20.890557 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 19:16:21.024673 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:16:21.025996 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:21.033983 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:16:21.036374 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:16:21.036863 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:16:21.037203 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.campaign_id,
    c.types, 
    c.width, 
    c.height, 
    c.campaign_id, 
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:16:21.054670 (Thread-1): Postgres error: syntax error at or near "."
LINE 30:     b.campaign_name, 
              ^

2022-08-07 19:16:21.055277 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 19:16:21.056485 (Thread-1): finished collecting timing info
2022-08-07 19:16:21.058339 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near "."
  LINE 30:     b.campaign_name, 
                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.SyntaxError: syntax error at or near "."
LINE 30:     b.campaign_name, 
              ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  syntax error at or near "."
  LINE 30:     b.campaign_name, 
                ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:16:21.073872 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '83026917-1744-47db-a028-ff619eb419f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa8b7db750>]}
2022-08-07 19:16:21.074745 (Thread-1): 19:16:21 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.55s]
2022-08-07 19:16:21.080711 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:16:21.180139 (MainThread): Using postgres connection "master".
2022-08-07 19:16:21.180838 (MainThread): On master: BEGIN
2022-08-07 19:16:21.181753 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:16:21.182220 (MainThread): On master: COMMIT
2022-08-07 19:16:21.182509 (MainThread): Using postgres connection "master".
2022-08-07 19:16:21.182773 (MainThread): On master: COMMIT
2022-08-07 19:16:21.183852 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:16:21.184970 (MainThread): 19:16:21 | 
2022-08-07 19:16:21.185983 (MainThread): 19:16:21 | Finished running 1 table model in 1.54s.
2022-08-07 19:16:21.186780 (MainThread): Connection 'master' was left open.
2022-08-07 19:16:21.187141 (MainThread): On master: Close
2022-08-07 19:16:21.197943 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:16:21.198410 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:16:21.244498 (MainThread): 
2022-08-07 19:16:21.252927 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 19:16:21.254170 (MainThread): 
2022-08-07 19:16:21.255019 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 19:16:21.255779 (MainThread):   syntax error at or near "."
2022-08-07 19:16:21.256480 (MainThread):   LINE 30:     b.campaign_name, 
2022-08-07 19:16:21.257162 (MainThread):                 ^
2022-08-07 19:16:21.257892 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:16:21.258601 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 19:16:21.259796 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9ca72110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9a2626d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faa9c9b0650>]}
2022-08-07 19:16:21.260490 (MainThread): Flushing usage events
2022-08-07 19:21:11.067915 (MainThread): Running with dbt=0.16.1
2022-08-07 19:21:11.543306 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:21:11.544326 (MainThread): Tracking: tracking
2022-08-07 19:21:11.626198 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12a86feb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12a8727c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12a85e11d0>]}
2022-08-07 19:21:11.830456 (MainThread): Partial parsing not enabled
2022-08-07 19:21:11.846391 (MainThread): Parsing macros/core.sql
2022-08-07 19:21:11.904635 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:21:11.945320 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:21:11.955070 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:21:12.106053 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:21:12.187249 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:21:12.217620 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:21:12.226910 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:21:12.325826 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:21:12.415719 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:21:12.438879 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:21:12.467753 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:21:12.505056 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:21:12.704161 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:21:12.709032 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:21:12.725154 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:21:12.739238 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:21:12.760113 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:21:12.776389 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:21:12.787061 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:21:12.884198 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:21:12.897658 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:21:12.919117 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:21:12.924518 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:21:12.942655 (MainThread): Parsing macros/relations.sql
2022-08-07 19:21:12.950732 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:21:13.120113 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:21:13.141288 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:21:13.355402 (MainThread): Partial parsing not enabled
2022-08-07 19:21:13.662135 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:13.662646 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:21:15.090297 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:21:16.940669 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:21:16.959961 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:21:17.038405 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:21:17.052393 (MainThread): 
2022-08-07 19:21:17.053819 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:21:17.054407 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:21:17.078539 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:21:17.079315 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:21:17.631462 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:21:17.632195 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:21:17.664924 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-08-07 19:21:18.264304 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:21:18.268701 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:21:18.281369 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:21:18.281871 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:21:18.283502 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:21:18.283973 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:21:18.284239 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:21:18.296596 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.01 seconds
2022-08-07 19:21:18.314990 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:21:18.386093 (MainThread): Using postgres connection "master".
2022-08-07 19:21:18.386568 (MainThread): On master: BEGIN
2022-08-07 19:21:18.425534 (MainThread): SQL status: BEGIN in 0.04 seconds
2022-08-07 19:21:18.426035 (MainThread): Using postgres connection "master".
2022-08-07 19:21:18.426319 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:21:18.447335 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-07 19:21:18.450522 (MainThread): On master: ROLLBACK
2022-08-07 19:21:18.461783 (MainThread): Using postgres connection "master".
2022-08-07 19:21:18.462250 (MainThread): On master: BEGIN
2022-08-07 19:21:18.467790 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:21:18.468316 (MainThread): On master: COMMIT
2022-08-07 19:21:18.468620 (MainThread): Using postgres connection "master".
2022-08-07 19:21:18.468886 (MainThread): On master: COMMIT
2022-08-07 19:21:18.471389 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:21:18.477564 (MainThread): 19:21:18 | Concurrency: 1 threads (target='dev')
2022-08-07 19:21:18.478705 (MainThread): 19:21:18 | 
2022-08-07 19:21:18.567203 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:21:18.567855 (Thread-1): 19:21:18 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:21:18.570084 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:18.575544 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:21:18.575908 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:21:18.746460 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:21:18.747525 (Thread-1): finished collecting timing info
2022-08-07 19:21:18.965114 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:18.965668 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:21:18.967212 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:21:18.981566 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:18.982033 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:21:18.984650 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:21:19.124070 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:21:19.125314 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:19.125798 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:21:19.133650 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:21:19.134176 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:21:19.134462 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.campaign_id,
    c.types, 
    c.width, 
    c.height, 
    c.campaign_id, 
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:21:19.161605 (Thread-1): Postgres error: column "campaign_id" specified more than once

2022-08-07 19:21:19.162112 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-07 19:21:19.163238 (Thread-1): finished collecting timing info
2022-08-07 19:21:19.164842 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  column "campaign_id" specified more than once
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.DuplicateColumn: column "campaign_id" specified more than once


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  column "campaign_id" specified more than once
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:21:19.181990 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '86402498-beda-4750-a2d5-c2655d7cb24e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1297c47d10>]}
2022-08-07 19:21:19.182806 (Thread-1): 19:21:19 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.61s]
2022-08-07 19:21:19.183269 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:21:19.218110 (MainThread): Using postgres connection "master".
2022-08-07 19:21:19.218590 (MainThread): On master: BEGIN
2022-08-07 19:21:19.221747 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:21:19.222271 (MainThread): On master: COMMIT
2022-08-07 19:21:19.222562 (MainThread): Using postgres connection "master".
2022-08-07 19:21:19.222824 (MainThread): On master: COMMIT
2022-08-07 19:21:19.226419 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:21:19.227581 (MainThread): 19:21:19 | 
2022-08-07 19:21:19.242131 (MainThread): 19:21:19 | Finished running 1 table model in 2.17s.
2022-08-07 19:21:19.243255 (MainThread): Connection 'master' was left open.
2022-08-07 19:21:19.243627 (MainThread): On master: Close
2022-08-07 19:21:19.244118 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:21:19.244473 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:21:19.302404 (MainThread): 
2022-08-07 19:21:19.313725 (MainThread): Completed with 1 error and 0 warnings:
2022-08-07 19:21:19.314944 (MainThread): 
2022-08-07 19:21:19.315857 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-07 19:21:19.316623 (MainThread):   column "campaign_id" specified more than once
2022-08-07 19:21:19.317310 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-07 19:21:19.318136 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-07 19:21:19.319246 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1297c4d290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12973e3190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f12973e3e50>]}
2022-08-07 19:21:19.319987 (MainThread): Flushing usage events
2022-08-07 19:31:32.958528 (MainThread): Running with dbt=0.16.1
2022-08-07 19:31:33.231764 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:31:33.233019 (MainThread): Tracking: tracking
2022-08-07 19:31:33.262047 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2f17a78d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2f454c0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2f17c7510>]}
2022-08-07 19:31:33.419287 (MainThread): Partial parsing not enabled
2022-08-07 19:31:33.427329 (MainThread): Parsing macros/core.sql
2022-08-07 19:31:33.449145 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:31:33.485289 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:31:33.494113 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:31:33.635807 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:31:33.718740 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:31:33.747306 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:31:33.756199 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:31:33.880122 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:31:33.962210 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:31:33.985810 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:31:34.014712 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:31:34.053776 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:31:34.435361 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:31:34.441072 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:31:34.450520 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:31:34.461530 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:31:34.466222 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:31:34.474690 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:31:34.479993 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:31:34.527376 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:31:34.537898 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:31:34.552686 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:31:34.566565 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:31:34.575508 (MainThread): Parsing macros/relations.sql
2022-08-07 19:31:34.596366 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:31:34.740380 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:31:34.759941 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:31:34.954220 (MainThread): Partial parsing not enabled
2022-08-07 19:31:35.223723 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:35.229668 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:31:36.326543 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:31:37.831105 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:31:37.853723 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:31:37.858573 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:31:37.883743 (MainThread): 
2022-08-07 19:31:37.885142 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:31:37.890827 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:31:37.943777 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:31:37.945848 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:31:38.742391 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:31:38.743024 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:31:38.792889 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.05 seconds
2022-08-07 19:31:38.935667 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:31:38.936824 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:31:38.942866 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:31:38.943325 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:31:38.945530 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:31:38.946015 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:31:38.946292 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:31:38.954031 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.01 seconds
2022-08-07 19:31:38.962749 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:31:38.999232 (MainThread): Using postgres connection "master".
2022-08-07 19:31:38.999947 (MainThread): On master: BEGIN
2022-08-07 19:31:39.012075 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:31:39.012573 (MainThread): Using postgres connection "master".
2022-08-07 19:31:39.012857 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:31:39.027067 (MainThread): SQL status: SELECT 0 in 0.01 seconds
2022-08-07 19:31:39.030302 (MainThread): On master: ROLLBACK
2022-08-07 19:31:39.031271 (MainThread): Using postgres connection "master".
2022-08-07 19:31:39.031674 (MainThread): On master: BEGIN
2022-08-07 19:31:39.033872 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:31:39.034415 (MainThread): On master: COMMIT
2022-08-07 19:31:39.034770 (MainThread): Using postgres connection "master".
2022-08-07 19:31:39.035041 (MainThread): On master: COMMIT
2022-08-07 19:31:39.036316 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:31:39.037630 (MainThread): 19:31:39 | Concurrency: 1 threads (target='dev')
2022-08-07 19:31:39.038181 (MainThread): 19:31:39 | 
2022-08-07 19:31:39.061217 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:31:39.062025 (Thread-1): 19:31:39 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:31:39.064206 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:39.064617 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:31:39.064938 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:31:39.191740 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:31:39.206461 (Thread-1): finished collecting timing info
2022-08-07 19:31:39.392388 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:39.392866 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:31:39.401424 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 19:31:39.414515 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:39.414965 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:31:39.426076 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 19:31:39.563125 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:31:39.578804 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:39.579256 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:31:39.580337 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:31:39.580784 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:31:39.582968 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:32:41.614631 (Thread-1): SQL status: SELECT 422791 in 62.03 seconds
2022-08-07 19:32:41.668051 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:32:41.668610 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 19:32:41.676046 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-07 19:32:41.679787 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:32:41.684490 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:32:41.684886 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:32:41.735827 (Thread-1): SQL status: COMMIT in 0.05 seconds
2022-08-07 19:32:41.750890 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:32:41.757736 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:32:41.761980 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:32:41.790236 (Thread-1): finished collecting timing info
2022-08-07 19:32:41.792926 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '997bd5ee-e160-43eb-836c-d20bf225e532', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2eefc7210>]}
2022-08-07 19:32:41.793841 (Thread-1): 19:32:41 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 62.73s]
2022-08-07 19:32:41.797990 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:32:41.892614 (MainThread): Using postgres connection "master".
2022-08-07 19:32:41.893085 (MainThread): On master: BEGIN
2022-08-07 19:32:41.893936 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:32:41.894393 (MainThread): On master: COMMIT
2022-08-07 19:32:41.894673 (MainThread): Using postgres connection "master".
2022-08-07 19:32:41.894932 (MainThread): On master: COMMIT
2022-08-07 19:32:41.895762 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:32:41.896822 (MainThread): 19:32:41 | 
2022-08-07 19:32:41.897791 (MainThread): 19:32:41 | Finished running 1 table model in 64.01s.
2022-08-07 19:32:41.898176 (MainThread): Connection 'master' was left open.
2022-08-07 19:32:41.898451 (MainThread): On master: Close
2022-08-07 19:32:41.912127 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:32:41.912610 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:32:41.958451 (MainThread): 
2022-08-07 19:32:41.959469 (MainThread): Completed successfully
2022-08-07 19:32:41.960605 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 19:32:41.964821 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2ef0fb710>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2ef083fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd2eefe7390>]}
2022-08-07 19:32:41.965600 (MainThread): Flushing usage events
2022-08-07 19:33:06.146088 (MainThread): Running with dbt=0.16.1
2022-08-07 19:33:06.774990 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 19:33:06.775941 (MainThread): Tracking: tracking
2022-08-07 19:33:06.841870 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6ad771b50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6ad769650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb6ad854890>]}
2022-08-07 19:33:07.180516 (MainThread): Partial parsing not enabled
2022-08-07 19:33:07.208856 (MainThread): Parsing macros/core.sql
2022-08-07 19:33:07.285383 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:33:07.361747 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:33:07.372232 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:33:07.533114 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:33:07.629704 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:33:07.664268 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:33:07.673792 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:33:07.784517 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:33:07.842887 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:33:07.870932 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:33:07.899967 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:33:07.931935 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:33:08.147221 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:33:08.152368 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:33:08.164638 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:33:08.177643 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:33:08.182047 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:33:08.190054 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:33:08.195384 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:33:08.244664 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:33:08.253308 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:33:08.262906 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:33:08.269114 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:33:08.300663 (MainThread): Parsing macros/relations.sql
2022-08-07 19:33:08.319829 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:33:08.425644 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:33:08.436273 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:33:08.596323 (MainThread): Partial parsing not enabled
2022-08-07 19:33:08.920520 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:33:08.921004 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:33:10.486308 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:33:11.774116 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:33:11.792788 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:33:11.793916 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:33:11.802256 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 19:33:11.803207 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb69cfa39d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb69cfad150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb69cfad490>]}
2022-08-07 19:33:11.804043 (MainThread): Flushing usage events
2022-08-07 19:33:12.632689 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 19:44:09.603490 (MainThread): Running with dbt=0.16.1
2022-08-07 19:44:10.168617 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:44:10.182064 (MainThread): Tracking: tracking
2022-08-07 19:44:10.241960 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55faecb90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55fc989d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55fc98dd0>]}
2022-08-07 19:44:10.453531 (MainThread): Partial parsing not enabled
2022-08-07 19:44:10.463225 (MainThread): Parsing macros/core.sql
2022-08-07 19:44:10.515710 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:44:10.593793 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:44:10.607357 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:44:10.902584 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:44:11.112837 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:44:11.173096 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:44:11.193310 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:44:11.394836 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:44:11.495394 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:44:11.544582 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:44:11.592825 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:44:11.665128 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:44:11.995817 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:44:12.001815 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:44:12.011980 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:44:12.022797 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:44:12.031697 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:44:12.049774 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:44:12.059292 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:44:12.141315 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:44:12.154067 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:44:12.181645 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:44:12.187363 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:44:12.213976 (MainThread): Parsing macros/relations.sql
2022-08-07 19:44:12.226889 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:44:12.401349 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:44:12.422621 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:44:12.638906 (MainThread): Partial parsing not enabled
2022-08-07 19:44:12.946512 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:12.946964 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:44:14.606719 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:44:16.357961 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:44:16.368131 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:44:16.369722 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:44:16.378076 (MainThread): 
2022-08-07 19:44:16.379860 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:44:16.380283 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:44:16.416897 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:44:16.417853 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:44:16.822107 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:44:16.822720 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:44:16.916059 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.09 seconds
2022-08-07 19:44:17.096447 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:44:17.096881 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:44:17.118854 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:44:17.119314 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:44:17.121755 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:44:17.122223 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:44:17.122506 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:44:18.267883 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 1.15 seconds
2022-08-07 19:44:18.317154 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:44:18.419228 (MainThread): Using postgres connection "master".
2022-08-07 19:44:18.422646 (MainThread): On master: BEGIN
2022-08-07 19:44:18.450698 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-07 19:44:18.451710 (MainThread): Using postgres connection "master".
2022-08-07 19:44:18.452254 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:44:18.557504 (MainThread): SQL status: SELECT 0 in 0.10 seconds
2022-08-07 19:44:18.560634 (MainThread): On master: ROLLBACK
2022-08-07 19:44:18.562819 (MainThread): Using postgres connection "master".
2022-08-07 19:44:18.563270 (MainThread): On master: BEGIN
2022-08-07 19:44:18.564315 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:44:18.564776 (MainThread): On master: COMMIT
2022-08-07 19:44:18.565070 (MainThread): Using postgres connection "master".
2022-08-07 19:44:18.565332 (MainThread): On master: COMMIT
2022-08-07 19:44:18.566416 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:44:18.567565 (MainThread): 19:44:18 | Concurrency: 1 threads (target='dev')
2022-08-07 19:44:18.568535 (MainThread): 19:44:18 | 
2022-08-07 19:44:18.600346 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:44:18.601025 (Thread-1): 19:44:18 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:44:18.602559 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:18.602958 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:44:18.603292 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:44:18.678381 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:44:18.679486 (Thread-1): finished collecting timing info
2022-08-07 19:44:18.793169 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:18.793765 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:44:18.795799 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:44:18.806280 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:18.812020 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:44:18.813188 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:44:18.908256 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:44:18.909621 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:18.910068 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:44:18.911082 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:44:18.913800 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:44:18.914190 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:45:10.421718 (Thread-1): SQL status: SELECT 422791 in 51.51 seconds
2022-08-07 19:45:10.449307 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:45:10.453863 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 19:45:10.483252 (Thread-1): SQL status: ALTER TABLE in 0.03 seconds
2022-08-07 19:45:10.500034 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:45:10.500480 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 19:45:10.501820 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 19:45:10.504845 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:45:10.505288 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:45:10.505639 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:45:10.592899 (Thread-1): SQL status: COMMIT in 0.09 seconds
2022-08-07 19:45:10.600733 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:45:10.601218 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:45:11.244295 (Thread-1): SQL status: DROP TABLE in 0.64 seconds
2022-08-07 19:45:11.259766 (Thread-1): finished collecting timing info
2022-08-07 19:45:11.280724 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07cc65d9-0238-4445-bb74-e637880edc56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55d35d7d0>]}
2022-08-07 19:45:11.298328 (Thread-1): 19:45:11 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 52.68s]
2022-08-07 19:45:11.305791 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:45:11.382561 (MainThread): Using postgres connection "master".
2022-08-07 19:45:11.383015 (MainThread): On master: BEGIN
2022-08-07 19:45:11.386795 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:45:11.387316 (MainThread): On master: COMMIT
2022-08-07 19:45:11.387611 (MainThread): Using postgres connection "master".
2022-08-07 19:45:11.387907 (MainThread): On master: COMMIT
2022-08-07 19:45:11.396971 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-07 19:45:11.399910 (MainThread): 19:45:11 | 
2022-08-07 19:45:11.400534 (MainThread): 19:45:11 | Finished running 1 table model in 55.02s.
2022-08-07 19:45:11.400919 (MainThread): Connection 'master' was left open.
2022-08-07 19:45:11.401215 (MainThread): On master: Close
2022-08-07 19:45:11.404645 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:45:11.405099 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:45:11.466032 (MainThread): 
2022-08-07 19:45:11.467066 (MainThread): Completed successfully
2022-08-07 19:45:11.478177 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 19:45:11.479435 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55d365990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd54f183fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd55faecb90>]}
2022-08-07 19:45:11.480118 (MainThread): Flushing usage events
2022-08-07 19:45:36.718151 (MainThread): Running with dbt=0.16.1
2022-08-07 19:45:37.064621 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 19:45:37.065909 (MainThread): Tracking: tracking
2022-08-07 19:45:37.085232 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0481ef9950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0485924ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f048209f910>]}
2022-08-07 19:45:37.175787 (MainThread): Partial parsing not enabled
2022-08-07 19:45:37.183535 (MainThread): Parsing macros/core.sql
2022-08-07 19:45:37.223431 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:45:37.313852 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:45:37.335633 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:45:37.704386 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:45:37.903219 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:45:38.034849 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:45:38.068263 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:45:38.286800 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:45:38.416888 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:45:38.480706 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:45:38.584330 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:45:38.681667 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:45:39.159565 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:45:39.173038 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:45:39.201035 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:45:39.217770 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:45:39.237544 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:45:39.264561 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:45:39.275667 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:45:39.424917 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:45:39.437045 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:45:39.501011 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:45:39.515980 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:45:39.530456 (MainThread): Parsing macros/relations.sql
2022-08-07 19:45:39.541968 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:45:39.768096 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:45:39.806198 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:45:39.991055 (MainThread): Partial parsing not enabled
2022-08-07 19:45:40.163612 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:45:40.164059 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:45:42.003810 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:45:44.013672 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:45:44.023172 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:45:44.024285 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:45:44.032116 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 19:45:44.032869 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04717edad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04717f6590>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f04717f6350>]}
2022-08-07 19:45:44.033856 (MainThread): Flushing usage events
2022-08-07 19:45:44.917366 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 19:55:35.525274 (MainThread): Running with dbt=0.16.1
2022-08-07 19:55:36.313593 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 19:55:36.316416 (MainThread): Tracking: tracking
2022-08-07 19:55:36.356516 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2312b4f5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2312ce3890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2312c46b90>]}
2022-08-07 19:55:36.649852 (MainThread): Partial parsing not enabled
2022-08-07 19:55:36.673756 (MainThread): Parsing macros/core.sql
2022-08-07 19:55:36.754595 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:55:36.900310 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:55:36.947402 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:55:37.288505 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:55:37.740636 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:55:37.852806 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:55:37.883038 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:55:38.296695 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:55:38.542872 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:55:38.621032 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:55:38.751719 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:55:38.890388 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:55:39.697106 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:55:39.717666 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:55:39.759084 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:55:39.805961 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:55:39.826127 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:55:39.848988 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:55:39.865360 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:55:40.051173 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:55:40.056794 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:55:40.124983 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:55:40.140903 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:55:40.168662 (MainThread): Parsing macros/relations.sql
2022-08-07 19:55:40.205458 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:55:40.553312 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:55:40.614936 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:55:40.847212 (MainThread): Partial parsing not enabled
2022-08-07 19:55:41.118753 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:41.125703 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:55:42.000304 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:55:43.704566 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:55:43.719785 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:55:43.789948 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:55:43.803661 (MainThread): 
2022-08-07 19:55:43.805342 (MainThread): Acquiring new postgres connection "master".
2022-08-07 19:55:43.806030 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:55:43.862125 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 19:55:43.866637 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 19:55:44.319694 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 19:55:44.320337 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 19:55:44.551147 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.23 seconds
2022-08-07 19:55:44.733463 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 19:55:44.734110 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 19:55:44.743385 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:55:44.743838 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 19:55:44.747153 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:55:44.747671 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 19:55:44.747969 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 19:55:44.775006 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.03 seconds
2022-08-07 19:55:44.796692 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 19:55:44.891436 (MainThread): Using postgres connection "master".
2022-08-07 19:55:44.891902 (MainThread): On master: BEGIN
2022-08-07 19:55:44.962554 (MainThread): SQL status: BEGIN in 0.07 seconds
2022-08-07 19:55:44.963051 (MainThread): Using postgres connection "master".
2022-08-07 19:55:44.963338 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 19:55:45.057438 (MainThread): SQL status: SELECT 0 in 0.09 seconds
2022-08-07 19:55:45.060444 (MainThread): On master: ROLLBACK
2022-08-07 19:55:45.085913 (MainThread): Using postgres connection "master".
2022-08-07 19:55:45.086391 (MainThread): On master: BEGIN
2022-08-07 19:55:45.096976 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 19:55:45.097553 (MainThread): On master: COMMIT
2022-08-07 19:55:45.097893 (MainThread): Using postgres connection "master".
2022-08-07 19:55:45.098169 (MainThread): On master: COMMIT
2022-08-07 19:55:45.101104 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:55:45.102435 (MainThread): 19:55:45 | Concurrency: 1 threads (target='dev')
2022-08-07 19:55:45.103480 (MainThread): 19:55:45 | 
2022-08-07 19:55:45.152419 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 19:55:45.153101 (Thread-1): 19:55:45 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 19:55:45.154189 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:45.154824 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 19:55:45.155185 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 19:55:45.244509 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 19:55:45.318559 (Thread-1): finished collecting timing info
2022-08-07 19:55:45.528314 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:45.528818 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 19:55:45.530945 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:55:45.539351 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:45.539799 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:55:45.540999 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 19:55:45.681316 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 19:55:45.789275 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:45.790006 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 19:55:45.791098 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:55:45.791532 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:55:45.791813 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 19:57:42.696929 (Thread-1): SQL status: SELECT 845582 in 116.90 seconds
2022-08-07 19:57:42.764097 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:57:42.764562 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 19:57:44.097907 (Thread-1): SQL status: ALTER TABLE in 1.33 seconds
2022-08-07 19:57:44.124753 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:57:44.125211 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 19:57:44.126620 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 19:57:44.138591 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:57:44.139051 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:57:44.139329 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 19:57:44.578636 (Thread-1): SQL status: COMMIT in 0.44 seconds
2022-08-07 19:57:44.612138 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 19:57:44.630013 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 19:57:46.137323 (Thread-1): SQL status: DROP TABLE in 1.51 seconds
2022-08-07 19:57:46.156951 (Thread-1): finished collecting timing info
2022-08-07 19:57:46.164725 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '7641300f-7681-4459-9584-5884bf2cb2a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23103ab050>]}
2022-08-07 19:57:46.175405 (Thread-1): 19:57:46 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 845582 in 121.01s]
2022-08-07 19:57:46.176752 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 19:57:46.263338 (MainThread): Using postgres connection "master".
2022-08-07 19:57:46.263804 (MainThread): On master: BEGIN
2022-08-07 19:57:46.264643 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 19:57:46.265112 (MainThread): On master: COMMIT
2022-08-07 19:57:46.269605 (MainThread): Using postgres connection "master".
2022-08-07 19:57:46.270158 (MainThread): On master: COMMIT
2022-08-07 19:57:46.271137 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 19:57:46.272272 (MainThread): 19:57:46 | 
2022-08-07 19:57:46.272763 (MainThread): 19:57:46 | Finished running 1 table model in 122.47s.
2022-08-07 19:57:46.273110 (MainThread): Connection 'master' was left open.
2022-08-07 19:57:46.273390 (MainThread): On master: Close
2022-08-07 19:57:46.283412 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 19:57:46.283903 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 19:57:46.322408 (MainThread): 
2022-08-07 19:57:46.322929 (MainThread): Completed successfully
2022-08-07 19:57:46.323312 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 19:57:46.323871 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23103b8150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2312b4f5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f23021ca150>]}
2022-08-07 19:57:46.324540 (MainThread): Flushing usage events
2022-08-07 19:58:10.833702 (MainThread): Running with dbt=0.16.1
2022-08-07 19:58:11.431655 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 19:58:11.438499 (MainThread): Tracking: tracking
2022-08-07 19:58:11.467799 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efdb0a72950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efdb0afc690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efdb0a60410>]}
2022-08-07 19:58:11.675822 (MainThread): Partial parsing not enabled
2022-08-07 19:58:11.691455 (MainThread): Parsing macros/core.sql
2022-08-07 19:58:11.742207 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 19:58:11.834564 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 19:58:11.851156 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 19:58:12.182497 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 19:58:12.376721 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 19:58:12.463025 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 19:58:12.486474 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 19:58:12.690219 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 19:58:12.857040 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 19:58:12.914895 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 19:58:12.982013 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 19:58:13.054736 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 19:58:13.536929 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 19:58:13.550046 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 19:58:13.592859 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 19:58:13.636591 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 19:58:13.645325 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 19:58:13.661647 (MainThread): Parsing macros/etc/query.sql
2022-08-07 19:58:13.672859 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 19:58:13.760904 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 19:58:13.778720 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 19:58:13.796565 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 19:58:13.810948 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 19:58:13.826950 (MainThread): Parsing macros/relations.sql
2022-08-07 19:58:13.844121 (MainThread): Parsing macros/adapters.sql
2022-08-07 19:58:14.158378 (MainThread): Parsing macros/catalog.sql
2022-08-07 19:58:14.188485 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 19:58:14.474471 (MainThread): Partial parsing not enabled
2022-08-07 19:58:14.767091 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 19:58:14.767532 (MainThread): Opening a new connection, currently in state init
2022-08-07 19:58:16.168647 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 19:58:17.563127 (MainThread): scipy not found, skipping conversion test.
2022-08-07 19:58:17.572763 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 19:58:17.574318 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 19:58:17.583001 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 19:58:17.584518 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efda02ac450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efda02ac350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efda02ac910>]}
2022-08-07 19:58:17.585517 (MainThread): Flushing usage events
2022-08-07 19:58:19.123398 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 21:05:02.722997 (MainThread): Running with dbt=0.16.1
2022-08-07 21:05:03.139315 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 21:05:03.193446 (MainThread): Tracking: tracking
2022-08-07 21:05:03.246413 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b5c54e690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b5c632f10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b5c694cd0>]}
2022-08-07 21:05:03.368721 (MainThread): Partial parsing not enabled
2022-08-07 21:05:03.412052 (MainThread): Parsing macros/core.sql
2022-08-07 21:05:03.492227 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:05:03.552456 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:05:03.577469 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:05:03.713625 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:05:03.838377 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:05:03.886146 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:05:03.896068 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:05:04.031765 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:05:04.122812 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:05:04.171879 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:05:04.207612 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:05:04.257102 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:05:04.525671 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:05:04.544551 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:05:04.554927 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:05:04.574402 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:05:04.599034 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:05:04.627451 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:05:04.666118 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:05:04.783778 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:05:04.797782 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:05:04.822636 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:05:04.849568 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:05:04.906943 (MainThread): Parsing macros/relations.sql
2022-08-07 21:05:04.938522 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:05:05.032968 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:05:05.052930 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:05:05.137534 (MainThread): Partial parsing not enabled
2022-08-07 21:05:05.271352 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:05.271785 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:05:06.049636 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:05:07.076468 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:05:07.085361 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:05:07.086580 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:05:07.094958 (MainThread): 
2022-08-07 21:05:07.096807 (MainThread): Acquiring new postgres connection "master".
2022-08-07 21:05:07.097224 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:05:07.139931 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 21:05:07.140924 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 21:05:07.537025 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 21:05:07.537749 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 21:05:07.681638 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.14 seconds
2022-08-07 21:05:07.838253 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 21:05:07.839075 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 21:05:07.845687 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:05:07.846342 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 21:05:07.849050 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:05:07.850006 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:05:07.850533 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 21:05:07.951317 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.10 seconds
2022-08-07 21:05:07.963791 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 21:05:07.999563 (MainThread): Using postgres connection "master".
2022-08-07 21:05:08.000221 (MainThread): On master: BEGIN
2022-08-07 21:05:08.013129 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 21:05:08.014765 (MainThread): Using postgres connection "master".
2022-08-07 21:05:08.015366 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 21:05:08.159108 (MainThread): SQL status: SELECT 0 in 0.13 seconds
2022-08-07 21:05:08.162399 (MainThread): On master: ROLLBACK
2022-08-07 21:05:08.163560 (MainThread): Using postgres connection "master".
2022-08-07 21:05:08.164132 (MainThread): On master: BEGIN
2022-08-07 21:05:08.166437 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:05:08.167974 (MainThread): On master: COMMIT
2022-08-07 21:05:08.176259 (MainThread): Using postgres connection "master".
2022-08-07 21:05:08.176774 (MainThread): On master: COMMIT
2022-08-07 21:05:08.179809 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:05:08.180930 (MainThread): 21:05:08 | Concurrency: 1 threads (target='dev')
2022-08-07 21:05:08.182136 (MainThread): 21:05:08 | 
2022-08-07 21:05:08.222828 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 21:05:08.223680 (Thread-1): 21:05:08 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 21:05:08.225644 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:08.226218 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 21:05:08.226765 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 21:05:08.309511 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 21:05:08.336932 (Thread-1): finished collecting timing info
2022-08-07 21:05:08.427359 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:08.428048 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 21:05:08.429576 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 21:05:08.439006 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:08.439985 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:05:08.447913 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 21:05:08.545859 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 21:05:08.565356 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:08.566014 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 21:05:08.568238 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:05:08.569699 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:05:08.570346 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 21:06:06.005611 (Thread-1): SQL status: SELECT 422791 in 57.42 seconds
2022-08-07 21:06:06.095423 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:06:06.095903 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 21:06:06.203797 (Thread-1): SQL status: ALTER TABLE in 0.11 seconds
2022-08-07 21:06:06.224984 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:06:06.231943 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 21:06:06.237727 (Thread-1): SQL status: ALTER TABLE in 0.01 seconds
2022-08-07 21:06:06.240815 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:06:06.241242 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:06:06.241591 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:06:06.260210 (Thread-1): SQL status: COMMIT in 0.01 seconds
2022-08-07 21:06:06.278944 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:06:06.279429 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:06:07.204693 (Thread-1): SQL status: DROP TABLE in 0.92 seconds
2022-08-07 21:06:07.214769 (Thread-1): finished collecting timing info
2022-08-07 21:06:07.217860 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '95f0ef80-62cd-43da-ad67-ae4f123b1ca8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b59dceb50>]}
2022-08-07 21:06:07.249576 (Thread-1): 21:06:07 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 58.99s]
2022-08-07 21:06:07.250297 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 21:06:07.308554 (MainThread): Using postgres connection "master".
2022-08-07 21:06:07.309718 (MainThread): On master: BEGIN
2022-08-07 21:06:07.311250 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:06:07.311844 (MainThread): On master: COMMIT
2022-08-07 21:06:07.312116 (MainThread): Using postgres connection "master".
2022-08-07 21:06:07.312369 (MainThread): On master: COMMIT
2022-08-07 21:06:07.313288 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:06:07.314371 (MainThread): 21:06:07 | 
2022-08-07 21:06:07.314840 (MainThread): 21:06:07 | Finished running 1 table model in 60.22s.
2022-08-07 21:06:07.315170 (MainThread): Connection 'master' was left open.
2022-08-07 21:06:07.315488 (MainThread): On master: Close
2022-08-07 21:06:07.315996 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 21:06:07.316335 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 21:06:07.352023 (MainThread): 
2022-08-07 21:06:07.352608 (MainThread): Completed successfully
2022-08-07 21:06:07.352993 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 21:06:07.353887 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b5025f250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b4be6a4d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9b4bbd93d0>]}
2022-08-07 21:06:07.355341 (MainThread): Flushing usage events
2022-08-07 21:06:31.841928 (MainThread): Running with dbt=0.16.1
2022-08-07 21:06:32.181187 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 21:06:32.182472 (MainThread): Tracking: tracking
2022-08-07 21:06:32.231881 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f249ef1d650>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f249ef85390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f249f0c6a90>]}
2022-08-07 21:06:32.318018 (MainThread): Partial parsing not enabled
2022-08-07 21:06:32.352243 (MainThread): Parsing macros/core.sql
2022-08-07 21:06:32.385538 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:06:32.437988 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:06:32.462457 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:06:32.596926 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:06:32.712339 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:06:32.760770 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:06:32.770904 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:06:32.895130 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:06:32.975116 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:06:33.023501 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:06:33.059509 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:06:33.108857 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:06:33.443783 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:06:33.462782 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:06:33.473109 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:06:33.514682 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:06:33.537305 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:06:33.571162 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:06:33.602893 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:06:33.713240 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:06:33.727392 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:06:33.763181 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:06:33.878572 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:06:33.936053 (MainThread): Parsing macros/relations.sql
2022-08-07 21:06:34.023053 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:06:34.117161 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:06:34.149788 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:06:34.299917 (MainThread): Partial parsing not enabled
2022-08-07 21:06:34.464717 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:06:34.465342 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:06:35.246209 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:06:36.701309 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:06:36.718977 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:06:37.015200 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:06:37.023829 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 21:06:37.024866 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f248e838c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f248e8418d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f248e841810>]}
2022-08-07 21:06:37.025988 (MainThread): Flushing usage events
2022-08-07 21:06:37.869800 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 21:14:27.684140 (MainThread): Running with dbt=0.16.1
2022-08-07 21:14:28.394264 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 21:14:28.441092 (MainThread): Tracking: tracking
2022-08-07 21:14:28.593654 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff950bbe150>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff950bb31d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff950bb3290>]}
2022-08-07 21:14:28.817598 (MainThread): Partial parsing not enabled
2022-08-07 21:14:28.869912 (MainThread): Parsing macros/core.sql
2022-08-07 21:14:28.985903 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:14:29.155127 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:14:29.202229 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:14:29.463379 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:14:29.563363 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:14:29.621953 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:14:29.631885 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:14:29.813952 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:14:30.091952 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:14:30.255152 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:14:30.366190 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:14:30.455978 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:14:30.749563 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:14:30.768333 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:14:30.786046 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:14:30.809067 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:14:30.831764 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:14:30.862631 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:14:30.880916 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:14:30.944037 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:14:30.978335 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:14:31.035473 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:14:31.061987 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:14:31.119581 (MainThread): Parsing macros/relations.sql
2022-08-07 21:14:31.150988 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:14:31.346377 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:14:31.400432 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:14:31.707971 (MainThread): Partial parsing not enabled
2022-08-07 21:14:32.032935 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:32.041762 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:14:34.067964 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:14:36.720068 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:14:36.737269 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:14:36.738608 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:14:36.749107 (MainThread): 
2022-08-07 21:14:36.757380 (MainThread): Acquiring new postgres connection "master".
2022-08-07 21:14:36.764561 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:14:36.836227 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 21:14:36.836973 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 21:14:37.905370 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 21:14:37.922118 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 21:14:38.078985 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.16 seconds
2022-08-07 21:14:38.482304 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 21:14:38.482766 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 21:14:38.500218 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:14:38.500690 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 21:14:38.509729 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-07 21:14:38.510210 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:14:38.510493 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 21:14:38.706510 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.20 seconds
2022-08-07 21:14:38.734156 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 21:14:38.878999 (MainThread): Using postgres connection "master".
2022-08-07 21:14:38.879484 (MainThread): On master: BEGIN
2022-08-07 21:14:38.908658 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-07 21:14:38.909148 (MainThread): Using postgres connection "master".
2022-08-07 21:14:38.909728 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 21:14:39.137682 (MainThread): SQL status: SELECT 0 in 0.23 seconds
2022-08-07 21:14:39.150700 (MainThread): On master: ROLLBACK
2022-08-07 21:14:39.151696 (MainThread): Using postgres connection "master".
2022-08-07 21:14:39.152112 (MainThread): On master: BEGIN
2022-08-07 21:14:39.158824 (MainThread): SQL status: BEGIN in 0.01 seconds
2022-08-07 21:14:39.159355 (MainThread): On master: COMMIT
2022-08-07 21:14:39.159681 (MainThread): Using postgres connection "master".
2022-08-07 21:14:39.159956 (MainThread): On master: COMMIT
2022-08-07 21:14:39.174035 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-07 21:14:39.175199 (MainThread): 21:14:39 | Concurrency: 1 threads (target='dev')
2022-08-07 21:14:39.175717 (MainThread): 21:14:39 | 
2022-08-07 21:14:39.279730 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 21:14:39.280504 (Thread-1): 21:14:39 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 21:14:39.281590 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:39.281997 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 21:14:39.282321 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 21:14:39.606008 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 21:14:39.607120 (Thread-1): finished collecting timing info
2022-08-07 21:14:40.012911 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:40.034065 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 21:14:40.048774 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 21:14:40.086955 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:40.087408 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:14:40.094881 (Thread-1): SQL status: DROP TABLE in 0.01 seconds
2022-08-07 21:14:40.334661 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 21:14:40.335888 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:40.347657 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 21:14:40.350380 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:14:40.351934 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:14:40.352420 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 21:15:35.604315 (Thread-1): SQL status: SELECT 422791 in 55.25 seconds
2022-08-07 21:15:35.726791 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:15:35.727589 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 21:15:35.780669 (Thread-1): SQL status: ALTER TABLE in 0.05 seconds
2022-08-07 21:15:35.796445 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:15:35.797244 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 21:15:35.799661 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 21:15:35.808049 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:15:35.808534 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:15:35.808819 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:15:35.836453 (Thread-1): SQL status: COMMIT in 0.03 seconds
2022-08-07 21:15:35.843514 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:15:35.850030 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:15:36.264685 (Thread-1): SQL status: DROP TABLE in 0.41 seconds
2022-08-07 21:15:36.278712 (Thread-1): finished collecting timing info
2022-08-07 21:15:36.284465 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '867adff7-5431-478b-9a94-256af96f7eab', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff9448a2150>]}
2022-08-07 21:15:36.304463 (Thread-1): 21:15:36 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 57.00s]
2022-08-07 21:15:36.305205 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 21:15:36.342668 (MainThread): Using postgres connection "master".
2022-08-07 21:15:36.343183 (MainThread): On master: BEGIN
2022-08-07 21:15:36.347014 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:15:36.347526 (MainThread): On master: COMMIT
2022-08-07 21:15:36.347817 (MainThread): Using postgres connection "master".
2022-08-07 21:15:36.348081 (MainThread): On master: COMMIT
2022-08-07 21:15:36.349714 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:15:36.351231 (MainThread): 21:15:36 | 
2022-08-07 21:15:36.352438 (MainThread): 21:15:36 | Finished running 1 table model in 59.59s.
2022-08-07 21:15:36.353598 (MainThread): Connection 'master' was left open.
2022-08-07 21:15:36.354219 (MainThread): On master: Close
2022-08-07 21:15:36.361240 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 21:15:36.362003 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 21:15:36.416385 (MainThread): 
2022-08-07 21:15:36.417446 (MainThread): Completed successfully
2022-08-07 21:15:36.418371 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 21:15:36.419551 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff93fa28110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff950bb31d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff94e4843d0>]}
2022-08-07 21:15:36.420256 (MainThread): Flushing usage events
2022-08-07 21:16:14.288371 (MainThread): Running with dbt=0.16.1
2022-08-07 21:16:14.893698 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 21:16:14.894766 (MainThread): Tracking: tracking
2022-08-07 21:16:14.955458 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a193bbb50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a19497a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a193b0450>]}
2022-08-07 21:16:15.058285 (MainThread): Partial parsing not enabled
2022-08-07 21:16:15.231523 (MainThread): Parsing macros/core.sql
2022-08-07 21:16:15.275646 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:16:15.361839 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:16:15.597346 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:16:15.793151 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:16:15.902963 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:16:16.028197 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:16:16.051626 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:16:16.207504 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:16:16.287094 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:16:16.358244 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:16:16.449253 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:16:16.598634 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:16:17.055838 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:16:17.108412 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:16:17.127864 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:16:17.159921 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:16:17.182557 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:16:17.236057 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:16:17.270501 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:16:17.391645 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:16:17.439194 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:16:17.486155 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:16:17.523890 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:16:17.559129 (MainThread): Parsing macros/relations.sql
2022-08-07 21:16:17.605838 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:16:17.728719 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:16:17.783104 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:16:17.892775 (MainThread): Partial parsing not enabled
2022-08-07 21:16:18.131122 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:16:18.131747 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:16:19.233876 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:16:20.720007 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:16:20.732010 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:16:20.733343 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:16:20.748447 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 21:16:20.758780 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a08cad8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a08cb86d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9a08cb85d0>]}
2022-08-07 21:16:20.759521 (MainThread): Flushing usage events
2022-08-07 21:16:21.770017 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 21:50:35.049186 (MainThread): Running with dbt=0.16.1
2022-08-07 21:50:38.701533 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 21:50:38.764019 (MainThread): Tracking: tracking
2022-08-07 21:50:38.978564 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f599cf4a810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f599d03b450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f599d03b210>]}
2022-08-07 21:50:39.344063 (MainThread): Partial parsing not enabled
2022-08-07 21:50:39.442832 (MainThread): Parsing macros/core.sql
2022-08-07 21:50:39.742588 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:50:39.806049 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:50:39.854119 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:50:40.239787 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:50:40.435939 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:50:40.483699 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:50:40.500866 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:50:40.640483 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:50:40.853719 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:50:40.924530 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:50:40.993495 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:50:41.076361 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:50:41.489061 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:50:41.508051 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:50:41.525895 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:50:41.548827 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:50:41.571699 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:50:41.602346 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:50:41.614910 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:50:41.725194 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:50:41.739335 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:50:41.775244 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:50:41.801745 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:50:41.859331 (MainThread): Parsing macros/relations.sql
2022-08-07 21:50:41.890713 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:50:42.040408 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:50:42.073960 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:50:42.389232 (MainThread): Partial parsing not enabled
2022-08-07 21:50:42.709903 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:42.710531 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:50:43.799719 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:50:45.499613 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:50:45.524063 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:50:45.525364 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:50:45.540862 (MainThread): 
2022-08-07 21:50:45.548450 (MainThread): Acquiring new postgres connection "master".
2022-08-07 21:50:45.550708 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:50:45.622486 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 21:50:45.625110 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 21:50:46.303903 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 21:50:46.304576 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 21:50:46.351383 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.05 seconds
2022-08-07 21:50:46.565578 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 21:50:46.566498 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 21:50:46.574049 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:50:46.574703 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 21:50:46.576613 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:50:46.577960 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:50:46.578847 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 21:50:47.550770 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.97 seconds
2022-08-07 21:50:47.574402 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 21:50:47.654690 (MainThread): Using postgres connection "master".
2022-08-07 21:50:47.657570 (MainThread): On master: BEGIN
2022-08-07 21:50:47.686702 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-07 21:50:47.687201 (MainThread): Using postgres connection "master".
2022-08-07 21:50:47.687482 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 21:50:48.144875 (MainThread): SQL status: SELECT 0 in 0.46 seconds
2022-08-07 21:50:48.148061 (MainThread): On master: ROLLBACK
2022-08-07 21:50:48.149160 (MainThread): Using postgres connection "master".
2022-08-07 21:50:48.149784 (MainThread): On master: BEGIN
2022-08-07 21:50:48.153608 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:50:48.154345 (MainThread): On master: COMMIT
2022-08-07 21:50:48.166619 (MainThread): Using postgres connection "master".
2022-08-07 21:50:48.167061 (MainThread): On master: COMMIT
2022-08-07 21:50:48.170145 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:50:48.171780 (MainThread): 21:50:48 | Concurrency: 1 threads (target='dev')
2022-08-07 21:50:48.177945 (MainThread): 21:50:48 | 
2022-08-07 21:50:48.260776 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 21:50:48.261473 (Thread-1): 21:50:48 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 21:50:48.262467 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:48.262829 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 21:50:48.263144 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 21:50:48.398570 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 21:50:48.399637 (Thread-1): finished collecting timing info
2022-08-07 21:50:48.511823 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:48.512405 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 21:50:48.513758 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 21:50:48.525938 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:48.526384 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:50:48.530448 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 21:50:48.621441 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 21:50:48.622644 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:48.623092 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 21:50:48.634311 (Thread-1): SQL status: BEGIN in 0.01 seconds
2022-08-07 21:50:48.635568 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:48.636075 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 21:50:53.905690 (MainThread): Running with dbt=0.16.1
2022-08-07 21:50:54.299606 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 21:50:54.301185 (MainThread): Tracking: tracking
2022-08-07 21:50:54.425081 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfa1596bd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfa173ecd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfa53d7910>]}
2022-08-07 21:50:54.513857 (MainThread): Partial parsing not enabled
2022-08-07 21:50:54.522550 (MainThread): Parsing macros/core.sql
2022-08-07 21:50:54.545111 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:50:54.581379 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:50:54.590014 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:50:54.738422 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:50:54.844856 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:50:54.873921 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:50:54.883073 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:50:55.034187 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:50:55.130970 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:50:55.204883 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:50:55.238949 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:50:55.282646 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:50:55.463521 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:50:55.468868 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:50:55.478820 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:50:55.489349 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:50:55.493999 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:50:55.502114 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:50:55.507533 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:50:55.548806 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:50:55.553343 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:50:55.562934 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:50:55.568441 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:50:55.574857 (MainThread): Parsing macros/relations.sql
2022-08-07 21:50:55.581939 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:50:55.652684 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:50:55.665220 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:50:55.766410 (MainThread): Partial parsing not enabled
2022-08-07 21:50:55.948368 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:50:55.948994 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:50:56.883317 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:50:59.189633 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:50:59.323977 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:50:59.325361 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:50:59.340969 (MainThread): 
2022-08-07 21:50:59.350200 (MainThread): Acquiring new postgres connection "master".
2022-08-07 21:50:59.350874 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:50:59.426584 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-07 21:50:59.429986 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-07 21:50:59.886993 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-07 21:50:59.887474 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-07 21:51:00.268623 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.38 seconds
2022-08-07 21:51:00.502429 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-07 21:51:00.505283 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-07 21:51:00.511010 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:51:00.511460 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-07 21:51:00.521868 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.01 seconds
2022-08-07 21:51:00.522353 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-07 21:51:00.522623 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-07 21:51:00.546758 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.02 seconds
2022-08-07 21:51:00.573757 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-07 21:51:00.671706 (MainThread): Using postgres connection "master".
2022-08-07 21:51:00.675876 (MainThread): On master: BEGIN
2022-08-07 21:51:00.729359 (MainThread): SQL status: BEGIN in 0.05 seconds
2022-08-07 21:51:00.730050 (MainThread): Using postgres connection "master".
2022-08-07 21:51:00.730470 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-07 21:51:00.807962 (MainThread): SQL status: SELECT 0 in 0.08 seconds
2022-08-07 21:51:00.811264 (MainThread): On master: ROLLBACK
2022-08-07 21:51:00.812948 (MainThread): Using postgres connection "master".
2022-08-07 21:51:00.813606 (MainThread): On master: BEGIN
2022-08-07 21:51:00.819583 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:51:00.820185 (MainThread): On master: COMMIT
2022-08-07 21:51:00.821073 (MainThread): Using postgres connection "master".
2022-08-07 21:51:00.825757 (MainThread): On master: COMMIT
2022-08-07 21:51:00.827873 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:51:00.828950 (MainThread): 21:51:00 | Concurrency: 1 threads (target='dev')
2022-08-07 21:51:00.838303 (MainThread): 21:51:00 | 
2022-08-07 21:51:00.865223 (Thread-1): Began running node model.dbt_.trans_join
2022-08-07 21:51:00.866149 (Thread-1): 21:51:00 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-07 21:51:00.867321 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:00.868292 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-07 21:51:00.868868 (Thread-1): Compiling model.dbt_.trans_join
2022-08-07 21:51:00.945589 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-07 21:51:00.961236 (Thread-1): finished collecting timing info
2022-08-07 21:51:01.046541 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:01.047233 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-07 21:51:01.048647 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 21:51:01.058544 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:01.059170 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:51:01.060252 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-07 21:51:01.130258 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-07 21:51:01.131674 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:01.132255 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-07 21:51:01.133253 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:51:01.133923 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:01.134354 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-07 21:51:34.209862 (Thread-1): SQL status: SELECT 422791 in 45.57 seconds
2022-08-07 21:51:34.243572 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:34.244193 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 21:51:34.246250 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 21:51:34.255402 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:34.255845 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 21:51:34.257266 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 21:51:34.260611 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:51:34.261163 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:34.261496 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:51:34.445966 (Thread-1): SQL status: COMMIT in 0.18 seconds
2022-08-07 21:51:34.452796 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:51:34.453516 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:51:34.861122 (Thread-1): SQL status: DROP TABLE in 0.41 seconds
2022-08-07 21:51:34.876877 (Thread-1): finished collecting timing info
2022-08-07 21:51:34.882596 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '713f6574-ca90-4bdc-9e4f-c30ecf2d2216', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f599a7be790>]}
2022-08-07 21:51:34.883560 (Thread-1): 21:51:34 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 46.62s]
2022-08-07 21:51:34.884602 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 21:51:34.957041 (MainThread): Using postgres connection "master".
2022-08-07 21:51:34.958818 (MainThread): On master: BEGIN
2022-08-07 21:51:34.960222 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:51:34.960864 (MainThread): On master: COMMIT
2022-08-07 21:51:34.961283 (MainThread): Using postgres connection "master".
2022-08-07 21:51:34.962198 (MainThread): On master: COMMIT
2022-08-07 21:51:34.965724 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:51:34.967052 (MainThread): 21:51:34 | 
2022-08-07 21:51:34.968091 (MainThread): 21:51:34 | Finished running 1 table model in 49.42s.
2022-08-07 21:51:34.968903 (MainThread): Connection 'master' was left open.
2022-08-07 21:51:34.970354 (MainThread): On master: Close
2022-08-07 21:51:34.973952 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 21:51:34.974577 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 21:51:35.001527 (MainThread): 
2022-08-07 21:51:35.002223 (MainThread): Completed successfully
2022-08-07 21:51:35.002763 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 21:51:35.003480 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f598d6e7950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f599cf46050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f598c5c0690>]}
2022-08-07 21:51:35.006003 (MainThread): Flushing usage events
2022-08-07 21:52:58.620279 (Thread-1): SQL status: SELECT 422791 in 117.49 seconds
2022-08-07 21:52:58.777526 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:52:58.778042 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-07 21:52:59.818374 (Thread-1): SQL status: ALTER TABLE in 1.04 seconds
2022-08-07 21:52:59.833205 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:52:59.841855 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-07 21:52:59.843656 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-07 21:52:59.846724 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:52:59.847156 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:52:59.847423 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-07 21:53:00.015811 (Thread-1): SQL status: COMMIT in 0.17 seconds
2022-08-07 21:53:00.031901 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-07 21:53:00.032372 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-07 21:53:01.844085 (Thread-1): SQL status: DROP TABLE in 1.81 seconds
2022-08-07 21:53:01.853753 (Thread-1): finished collecting timing info
2022-08-07 21:53:01.856397 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c4567f6e-655d-418c-b44d-139274990339', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfa161ead0>]}
2022-08-07 21:53:01.884593 (Thread-1): 21:53:01 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 120.99s]
2022-08-07 21:53:01.885884 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-07 21:53:02.018459 (MainThread): Using postgres connection "master".
2022-08-07 21:53:02.018941 (MainThread): On master: BEGIN
2022-08-07 21:53:02.019753 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-07 21:53:02.020282 (MainThread): On master: COMMIT
2022-08-07 21:53:02.020558 (MainThread): Using postgres connection "master".
2022-08-07 21:53:02.020807 (MainThread): On master: COMMIT
2022-08-07 21:53:02.021781 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-07 21:53:02.022917 (MainThread): 21:53:02 | 
2022-08-07 21:53:02.023902 (MainThread): 21:53:02 | Finished running 1 table model in 122.67s.
2022-08-07 21:53:02.024693 (MainThread): Connection 'master' was left open.
2022-08-07 21:53:02.025044 (MainThread): On master: Close
2022-08-07 21:53:02.026952 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-07 21:53:02.027665 (MainThread): On model.dbt_.trans_join: Close
2022-08-07 21:53:02.043210 (MainThread): 
2022-08-07 21:53:02.044328 (MainThread): Completed successfully
2022-08-07 21:53:02.045374 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-07 21:53:02.046990 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf9040e5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf903f26d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcfa173ecd0>]}
2022-08-07 21:53:02.047952 (MainThread): Flushing usage events
2022-08-07 21:53:22.726469 (MainThread): Running with dbt=0.16.1
2022-08-07 21:53:23.188147 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 21:53:23.189324 (MainThread): Tracking: tracking
2022-08-07 21:53:23.327359 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa85310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa99110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57bfa99210>]}
2022-08-07 21:53:23.592196 (MainThread): Partial parsing not enabled
2022-08-07 21:53:23.681211 (MainThread): Parsing macros/core.sql
2022-08-07 21:53:23.781533 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:53:23.910895 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:53:23.941755 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:53:24.159212 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:53:24.318577 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:53:24.366657 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:53:24.375862 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:53:24.534460 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:53:24.670342 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:53:24.740868 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:53:24.865254 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:53:24.948001 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:53:25.238685 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:53:25.257733 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:53:25.278130 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:53:25.342865 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:53:25.377749 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:53:25.418653 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:53:25.446357 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:53:25.574662 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:53:25.589335 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:53:25.614083 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:53:25.640402 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:53:25.697757 (MainThread): Parsing macros/relations.sql
2022-08-07 21:53:25.851147 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:53:25.978114 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:53:26.043426 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:53:26.206363 (MainThread): Partial parsing not enabled
2022-08-07 21:53:26.468544 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:53:26.475888 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:53:27.572973 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:53:28.267706 (MainThread): Running with dbt=0.16.1
2022-08-07 21:53:28.874138 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-07 21:53:28.875102 (MainThread): Tracking: tracking
2022-08-07 21:53:28.938315 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99ac5345d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99ac534690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f99ac5f4410>]}
2022-08-07 21:53:29.094861 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:53:29.105673 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:53:29.108360 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:53:29.117084 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 21:53:29.120509 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57af377a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57af380690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f57af380310>]}
2022-08-07 21:53:29.121592 (MainThread): Flushing usage events
2022-08-07 21:53:29.171890 (MainThread): Partial parsing not enabled
2022-08-07 21:53:29.183120 (MainThread): Parsing macros/core.sql
2022-08-07 21:53:29.241006 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-07 21:53:29.362257 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-07 21:53:29.375648 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-07 21:53:29.636373 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-07 21:53:29.807730 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-07 21:53:29.841613 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-07 21:53:29.859446 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-07 21:53:30.043993 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-07 21:53:30.152575 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-07 21:53:30.176882 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-07 21:53:30.219561 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-07 21:53:30.222928 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-07 21:53:30.272643 (MainThread): Parsing macros/adapters/common.sql
2022-08-07 21:53:31.018903 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-07 21:53:31.034071 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-07 21:53:31.074969 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-07 21:53:31.107063 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-07 21:53:31.126126 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-07 21:53:31.141886 (MainThread): Parsing macros/etc/query.sql
2022-08-07 21:53:31.151309 (MainThread): Parsing macros/etc/datetime.sql
2022-08-07 21:53:31.252775 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-07 21:53:31.264349 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-07 21:53:31.280734 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-07 21:53:31.293164 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-07 21:53:31.325359 (MainThread): Parsing macros/relations.sql
2022-08-07 21:53:31.348214 (MainThread): Parsing macros/adapters.sql
2022-08-07 21:53:31.580399 (MainThread): Parsing macros/catalog.sql
2022-08-07 21:53:31.613094 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-07 21:53:31.807241 (MainThread): Partial parsing not enabled
2022-08-07 21:53:32.138644 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-07 21:53:32.149742 (MainThread): Opening a new connection, currently in state init
2022-08-07 21:53:33.711407 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-07 21:53:35.288298 (MainThread): scipy not found, skipping conversion test.
2022-08-07 21:53:35.298923 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-07 21:53:35.300259 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-07 21:53:35.337041 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-07 21:53:35.340333 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f999be14850>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f999be14950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f999edd60d0>]}
2022-08-07 21:53:35.349163 (MainThread): Flushing usage events
2022-08-07 21:53:36.333527 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-08 00:41:13.906425 (MainThread): Running with dbt=0.16.1
2022-08-08 00:41:14.663377 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-08 00:41:14.695903 (MainThread): Tracking: tracking
2022-08-08 00:41:14.748652 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb78a910890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb78a9f8510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb78a9f8e10>]}
2022-08-08 00:41:14.960256 (MainThread): Partial parsing not enabled
2022-08-08 00:41:15.002439 (MainThread): Parsing macros/core.sql
2022-08-08 00:41:15.057733 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-08 00:41:15.267335 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-08 00:41:15.312525 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-08 00:41:15.838022 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-08 00:41:16.545544 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-08 00:41:16.720158 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-08 00:41:16.737296 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-08 00:41:17.236671 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-08 00:41:17.456469 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-08 00:41:17.538187 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-08 00:41:17.618608 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-08 00:41:17.701484 (MainThread): Parsing macros/adapters/common.sql
2022-08-08 00:41:18.135998 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-08 00:41:18.154994 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-08 00:41:18.172042 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-08 00:41:18.221458 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-08 00:41:18.285049 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-08 00:41:18.315995 (MainThread): Parsing macros/etc/query.sql
2022-08-08 00:41:18.450545 (MainThread): Parsing macros/etc/datetime.sql
2022-08-08 00:41:18.616500 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-08 00:41:18.696244 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-08 00:41:18.744116 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-08 00:41:18.797617 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-08 00:41:18.872409 (MainThread): Parsing macros/relations.sql
2022-08-08 00:41:18.903768 (MainThread): Parsing macros/adapters.sql
2022-08-08 00:41:19.097755 (MainThread): Parsing macros/catalog.sql
2022-08-08 00:41:19.129370 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-08 00:41:19.349352 (MainThread): Partial parsing not enabled
2022-08-08 00:41:19.677210 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:19.680174 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:41:20.936705 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-08 00:41:22.568676 (MainThread): scipy not found, skipping conversion test.
2022-08-08 00:41:22.590311 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-08 00:41:22.591535 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-08 00:41:22.609955 (MainThread): 
2022-08-08 00:41:22.611151 (MainThread): Acquiring new postgres connection "master".
2022-08-08 00:41:22.611548 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:41:22.686829 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-08 00:41:22.697330 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-08 00:41:23.733850 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-08 00:41:23.734333 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-08 00:41:23.782105 (ThreadPoolExecutor-0_0): SQL status: SELECT 4 in 0.05 seconds
2022-08-08 00:41:23.798337 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "create_staging_staging".
2022-08-08 00:41:23.798769 (ThreadPoolExecutor-0_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-08 00:41:23.799026 (ThreadPoolExecutor-0_0): Creating schema "staging"."staging".
2022-08-08 00:41:23.995438 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-08 00:41:23.996064 (ThreadPoolExecutor-0_0): On create_staging_staging: BEGIN
2022-08-08 00:41:23.999676 (ThreadPoolExecutor-0_0): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:41:24.000227 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-08 00:41:24.000515 (ThreadPoolExecutor-0_0): On create_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "create_staging_staging"} */
create schema if not exists "staging"
2022-08-08 00:41:24.002939 (ThreadPoolExecutor-0_0): SQL status: CREATE SCHEMA in 0.00 seconds
2022-08-08 00:41:24.006710 (ThreadPoolExecutor-0_0): On create_staging_staging: COMMIT
2022-08-08 00:41:24.008267 (ThreadPoolExecutor-0_0): Using postgres connection "create_staging_staging".
2022-08-08 00:41:24.008660 (ThreadPoolExecutor-0_0): On create_staging_staging: COMMIT
2022-08-08 00:41:24.047556 (ThreadPoolExecutor-0_0): SQL status: COMMIT in 0.04 seconds
2022-08-08 00:41:24.088948 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-08 00:41:24.089390 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly create_staging_staging).
2022-08-08 00:41:24.103770 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:41:24.104564 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-08 00:41:24.107870 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:41:24.108671 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:41:24.109054 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-08 00:41:24.125783 (ThreadPoolExecutor-1_0): SQL status: SELECT 0 in 0.02 seconds
2022-08-08 00:41:24.132406 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-08 00:41:24.220094 (MainThread): Using postgres connection "master".
2022-08-08 00:41:24.222164 (MainThread): On master: BEGIN
2022-08-08 00:41:24.244140 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-08 00:41:24.244741 (MainThread): Using postgres connection "master".
2022-08-08 00:41:24.245033 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-08 00:41:24.290476 (MainThread): SQL status: SELECT 0 in 0.05 seconds
2022-08-08 00:41:24.302905 (MainThread): On master: ROLLBACK
2022-08-08 00:41:24.315399 (MainThread): Using postgres connection "master".
2022-08-08 00:41:24.315853 (MainThread): On master: BEGIN
2022-08-08 00:41:24.317000 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:41:24.317603 (MainThread): On master: COMMIT
2022-08-08 00:41:24.317906 (MainThread): Using postgres connection "master".
2022-08-08 00:41:24.319576 (MainThread): On master: COMMIT
2022-08-08 00:41:24.325983 (MainThread): SQL status: COMMIT in 0.01 seconds
2022-08-08 00:41:24.327305 (MainThread): 00:41:24 | Concurrency: 1 threads (target='dev')
2022-08-08 00:41:24.331938 (MainThread): 00:41:24 | 
2022-08-08 00:41:24.443105 (Thread-1): Began running node model.dbt_.trans_join
2022-08-08 00:41:24.443765 (Thread-1): 00:41:24 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-08 00:41:24.444731 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:24.445092 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-08 00:41:24.445452 (Thread-1): Compiling model.dbt_.trans_join
2022-08-08 00:41:24.653605 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-08 00:41:24.688349 (Thread-1): finished collecting timing info
2022-08-08 00:41:24.939003 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:24.939513 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-08 00:41:24.940710 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:41:24.976143 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:24.976610 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-08 00:41:24.980889 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:41:25.126689 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-08 00:41:25.149898 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:25.150361 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-08 00:41:25.151438 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:41:25.151935 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:41:25.152220 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-08 00:41:25.154581 (Thread-1): Postgres error: relation "staging.source_campaign" does not exist
LINE 9:     select * from staging.staging.source_campaign
                          ^

2022-08-08 00:41:25.155076 (Thread-1): On model.dbt_.trans_join: ROLLBACK
2022-08-08 00:41:25.161880 (Thread-1): finished collecting timing info
2022-08-08 00:41:25.163584 (Thread-1): Database Error in model trans_join (models/traffic_models/trans_join.sql)
  relation "staging.source_campaign" does not exist
  LINE 9:     select * from staging.staging.source_campaign
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 46, in exception_handler
    yield
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 74, in add_query
    cursor.execute(sql, bindings)
psycopg2.errors.UndefinedTable: relation "staging.source_campaign" does not exist
LINE 9:     select * from staging.staging.source_campaign
                          ^


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 223, in safe_run
    result = self.compile_and_execute(manifest, ctx)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 166, in compile_and_execute
    result = self.run(ctx.node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 268, in run
    return self.execute(compiled_node, manifest)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/node_runners.py", line 450, in execute
    result = MacroGenerator(materialization_macro, context)()
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 61, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 231, in __call__
    return self.call_macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/clients/jinja.py", line 161, in call_macro
    return macro(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 570, in __call__
    return self._invoke(arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/asyncsupport.py", line 110, in _invoke
    return original_invoke(self, arguments, autoescape)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 574, in _invoke
    rv = self._func(*arguments)
  File "<template>", line 40, in macro
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/sandbox.py", line 440, in call
    return __context.call(__obj, *args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/jinja2/runtime.py", line 262, in call
    return __obj(*args, **kwargs)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/base/impl.py", line 220, in execute
    fetch=fetch
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 116, in execute
    _, cursor = self.add_query(sql, auto_begin)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/sql/connections.py", line 82, in add_query
    return connection, cursor
  File "/usr/local/lib/python3.7/contextlib.py", line 130, in __exit__
    self.gen.throw(type, value, traceback)
  File "/usr/local/airflow/.local/lib/python3.7/site-packages/dbt/adapters/postgres/connections.py", line 58, in exception_handler
    raise dbt.exceptions.DatabaseException(str(e).strip()) from e
dbt.exceptions.DatabaseException: Database Error in model trans_join (models/traffic_models/trans_join.sql)
  relation "staging.source_campaign" does not exist
  LINE 9:     select * from staging.staging.source_campaign
                            ^
  compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-08 00:41:25.242422 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c270ba4a-250b-42ef-8227-8588249a66e5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb788141810>]}
2022-08-08 00:41:25.243247 (Thread-1): 00:41:25 | 1 of 1 ERROR creating table model staging.trans_join................. [ERROR in 0.80s]
2022-08-08 00:41:25.243726 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-08 00:41:25.325501 (MainThread): Using postgres connection "master".
2022-08-08 00:41:25.325986 (MainThread): On master: BEGIN
2022-08-08 00:41:25.327997 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:41:25.328602 (MainThread): On master: COMMIT
2022-08-08 00:41:25.328902 (MainThread): Using postgres connection "master".
2022-08-08 00:41:25.329176 (MainThread): On master: COMMIT
2022-08-08 00:41:25.338531 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-08 00:41:25.339726 (MainThread): 00:41:25 | 
2022-08-08 00:41:25.346280 (MainThread): 00:41:25 | Finished running 1 table model in 2.73s.
2022-08-08 00:41:25.347129 (MainThread): Connection 'master' was left open.
2022-08-08 00:41:25.347582 (MainThread): On master: Close
2022-08-08 00:41:25.348103 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-08 00:41:25.348581 (MainThread): On model.dbt_.trans_join: Close
2022-08-08 00:41:25.401112 (MainThread): 
2022-08-08 00:41:25.406189 (MainThread): Completed with 1 error and 0 warnings:
2022-08-08 00:41:25.406728 (MainThread): 
2022-08-08 00:41:25.407106 (MainThread): Database Error in model trans_join (models/traffic_models/trans_join.sql)
2022-08-08 00:41:25.407661 (MainThread):   relation "staging.source_campaign" does not exist
2022-08-08 00:41:25.408041 (MainThread):   LINE 9:     select * from staging.staging.source_campaign
2022-08-08 00:41:25.408356 (MainThread):                             ^
2022-08-08 00:41:25.408665 (MainThread):   compiled SQL at dbt_/target/run/dbt_/traffic_models/trans_join.sql
2022-08-08 00:41:25.408988 (MainThread): 
Done. PASS=0 WARN=0 ERROR=1 SKIP=0 TOTAL=1
2022-08-08 00:41:25.414385 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb779fa6110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb779780950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb78816c110>]}
2022-08-08 00:41:25.415034 (MainThread): Flushing usage events
2022-08-08 00:43:39.304137 (MainThread): Running with dbt=0.16.1
2022-08-08 00:43:39.954766 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-08 00:43:39.956031 (MainThread): Tracking: tracking
2022-08-08 00:43:40.018503 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18a4348610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18a44e6ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18a4344b90>]}
2022-08-08 00:43:40.173021 (MainThread): Partial parsing not enabled
2022-08-08 00:43:40.198914 (MainThread): Parsing macros/core.sql
2022-08-08 00:43:40.242861 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-08 00:43:40.323806 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-08 00:43:40.336697 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-08 00:43:40.491470 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-08 00:43:40.572376 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-08 00:43:40.602761 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-08 00:43:40.612085 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-08 00:43:40.707341 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-08 00:43:40.764442 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-08 00:43:40.787969 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-08 00:43:40.816766 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-08 00:43:40.847310 (MainThread): Parsing macros/adapters/common.sql
2022-08-08 00:43:41.030432 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-08 00:43:41.035754 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-08 00:43:41.046820 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-08 00:43:41.087196 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-08 00:43:41.091766 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-08 00:43:41.100654 (MainThread): Parsing macros/etc/query.sql
2022-08-08 00:43:41.106301 (MainThread): Parsing macros/etc/datetime.sql
2022-08-08 00:43:41.156307 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-08 00:43:41.161756 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-08 00:43:41.171446 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-08 00:43:41.177539 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-08 00:43:41.183906 (MainThread): Parsing macros/relations.sql
2022-08-08 00:43:41.191674 (MainThread): Parsing macros/adapters.sql
2022-08-08 00:43:41.291885 (MainThread): Parsing macros/catalog.sql
2022-08-08 00:43:41.302683 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-08 00:43:41.400068 (MainThread): Partial parsing not enabled
2022-08-08 00:43:41.631384 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:41.635046 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:43:42.784697 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-08 00:43:44.273617 (MainThread): scipy not found, skipping conversion test.
2022-08-08 00:43:44.295284 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-08 00:43:44.296401 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-08 00:43:44.317678 (MainThread): 
2022-08-08 00:43:44.319498 (MainThread): Acquiring new postgres connection "master".
2022-08-08 00:43:44.320376 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:43:44.383286 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-08 00:43:44.383989 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-08 00:43:45.036565 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-08 00:43:45.038307 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-08 00:43:45.068626 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.03 seconds
2022-08-08 00:43:45.230186 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-08 00:43:45.230841 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-08 00:43:45.236799 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:43:45.237262 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-08 00:43:45.239251 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:43:45.239732 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:43:45.240005 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-08 00:43:45.262208 (ThreadPoolExecutor-1_0): SQL status: SELECT 2 in 0.02 seconds
2022-08-08 00:43:45.289553 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-08 00:43:45.390686 (MainThread): Using postgres connection "master".
2022-08-08 00:43:45.391232 (MainThread): On master: BEGIN
2022-08-08 00:43:45.414419 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-08 00:43:45.414915 (MainThread): Using postgres connection "master".
2022-08-08 00:43:45.415230 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-08 00:43:45.432364 (MainThread): SQL status: SELECT 0 in 0.02 seconds
2022-08-08 00:43:45.435920 (MainThread): On master: ROLLBACK
2022-08-08 00:43:45.436911 (MainThread): Using postgres connection "master".
2022-08-08 00:43:45.437318 (MainThread): On master: BEGIN
2022-08-08 00:43:45.440047 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:43:45.441801 (MainThread): On master: COMMIT
2022-08-08 00:43:45.442206 (MainThread): Using postgres connection "master".
2022-08-08 00:43:45.442478 (MainThread): On master: COMMIT
2022-08-08 00:43:45.443178 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-08 00:43:45.444227 (MainThread): 00:43:45 | Concurrency: 1 threads (target='dev')
2022-08-08 00:43:45.445183 (MainThread): 00:43:45 | 
2022-08-08 00:43:45.465513 (Thread-1): Began running node model.dbt_.trans_join
2022-08-08 00:43:45.466403 (Thread-1): 00:43:45 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-08 00:43:45.467864 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:45.468267 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-08 00:43:45.468589 (Thread-1): Compiling model.dbt_.trans_join
2022-08-08 00:43:45.600312 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-08 00:43:45.601715 (Thread-1): finished collecting timing info
2022-08-08 00:43:45.704860 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:45.705674 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-08 00:43:45.708677 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:43:45.717748 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:45.718415 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-08 00:43:45.719537 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:43:45.799896 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-08 00:43:45.801469 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:45.802087 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-08 00:43:45.803125 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:43:45.803729 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:43:45.817672 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-08 00:45:10.954357 (Thread-1): SQL status: SELECT 422791 in 85.14 seconds
2022-08-08 00:45:10.993131 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:45:10.995632 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-08 00:45:11.053654 (Thread-1): SQL status: ALTER TABLE in 0.06 seconds
2022-08-08 00:45:11.057791 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-08 00:45:11.058442 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:45:11.058889 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-08 00:45:11.149335 (Thread-1): SQL status: COMMIT in 0.09 seconds
2022-08-08 00:45:11.157278 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:45:11.157981 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-08 00:45:11.159208 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:45:11.184969 (Thread-1): finished collecting timing info
2022-08-08 00:45:11.195859 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '70d022f8-ae21-4fae-aefe-4ab846d11bff', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18a4445c10>]}
2022-08-08 00:45:11.196951 (Thread-1): 00:45:11 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 85.73s]
2022-08-08 00:45:11.197755 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-08 00:45:11.249155 (MainThread): Using postgres connection "master".
2022-08-08 00:45:11.249950 (MainThread): On master: BEGIN
2022-08-08 00:45:11.270574 (MainThread): SQL status: BEGIN in 0.02 seconds
2022-08-08 00:45:11.272439 (MainThread): On master: COMMIT
2022-08-08 00:45:11.272826 (MainThread): Using postgres connection "master".
2022-08-08 00:45:11.273104 (MainThread): On master: COMMIT
2022-08-08 00:45:11.274190 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-08 00:45:11.275357 (MainThread): 00:45:11 | 
2022-08-08 00:45:11.276347 (MainThread): 00:45:11 | Finished running 1 table model in 86.96s.
2022-08-08 00:45:11.277123 (MainThread): Connection 'master' was left open.
2022-08-08 00:45:11.277530 (MainThread): On master: Close
2022-08-08 00:45:11.278575 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-08 00:45:11.279016 (MainThread): On model.dbt_.trans_join: Close
2022-08-08 00:45:11.302580 (MainThread): 
2022-08-08 00:45:11.303335 (MainThread): Completed successfully
2022-08-08 00:45:11.303910 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-08 00:45:11.304668 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18939c0390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18939c0410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18939e95d0>]}
2022-08-08 00:45:11.305932 (MainThread): Flushing usage events
2022-08-08 00:45:32.509212 (MainThread): Running with dbt=0.16.1
2022-08-08 00:45:32.859792 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-08 00:45:32.860736 (MainThread): Tracking: tracking
2022-08-08 00:45:32.878757 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29bcb4ad50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29bcc3f690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29bcc3f7d0>]}
2022-08-08 00:45:32.980079 (MainThread): Partial parsing not enabled
2022-08-08 00:45:32.986640 (MainThread): Parsing macros/core.sql
2022-08-08 00:45:33.009805 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-08 00:45:33.060811 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-08 00:45:33.069999 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-08 00:45:33.239275 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-08 00:45:33.324065 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-08 00:45:33.353743 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-08 00:45:33.363003 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-08 00:45:33.500053 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-08 00:45:33.557239 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-08 00:45:33.581884 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-08 00:45:33.612107 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-08 00:45:33.643127 (MainThread): Parsing macros/adapters/common.sql
2022-08-08 00:45:33.838856 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-08 00:45:33.843965 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-08 00:45:33.853634 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-08 00:45:33.864094 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-08 00:45:33.868600 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-08 00:45:33.876545 (MainThread): Parsing macros/etc/query.sql
2022-08-08 00:45:33.882093 (MainThread): Parsing macros/etc/datetime.sql
2022-08-08 00:45:33.923548 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-08 00:45:33.928346 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-08 00:45:33.941894 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-08 00:45:33.948037 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-08 00:45:33.957740 (MainThread): Parsing macros/relations.sql
2022-08-08 00:45:33.964919 (MainThread): Parsing macros/adapters.sql
2022-08-08 00:45:34.036573 (MainThread): Parsing macros/catalog.sql
2022-08-08 00:45:34.047166 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-08 00:45:34.135350 (MainThread): Partial parsing not enabled
2022-08-08 00:45:34.277726 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:45:34.278169 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:45:35.243683 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-08 00:45:36.712027 (MainThread): scipy not found, skipping conversion test.
2022-08-08 00:45:36.727444 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-08 00:45:36.728815 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-08 00:45:36.741259 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-08 00:45:36.743647 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ac455810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ac455310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f29ac455150>]}
2022-08-08 00:45:36.744563 (MainThread): Flushing usage events
2022-08-08 00:45:37.762339 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
2022-08-08 00:51:43.161763 (MainThread): Running with dbt=0.16.1
2022-08-08 00:51:44.394470 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, exclude=None, full_refresh=False, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='run', single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-08 00:51:44.499162 (MainThread): Tracking: tracking
2022-08-08 00:51:44.569068 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f563fb989d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f563fbb6950>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f563fbb1bd0>]}
2022-08-08 00:51:44.840578 (MainThread): Partial parsing not enabled
2022-08-08 00:51:44.906037 (MainThread): Parsing macros/core.sql
2022-08-08 00:51:44.968003 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-08 00:51:45.058809 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-08 00:51:45.076117 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-08 00:51:45.373105 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-08 00:51:45.552446 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-08 00:51:45.588751 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-08 00:51:45.599683 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-08 00:51:45.727002 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-08 00:51:45.807375 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-08 00:51:45.838021 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-08 00:51:45.871554 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-08 00:51:45.907655 (MainThread): Parsing macros/adapters/common.sql
2022-08-08 00:51:46.123917 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-08 00:51:46.129390 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-08 00:51:46.141539 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-08 00:51:46.155744 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-08 00:51:46.160453 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-08 00:51:46.170894 (MainThread): Parsing macros/etc/query.sql
2022-08-08 00:51:46.176484 (MainThread): Parsing macros/etc/datetime.sql
2022-08-08 00:51:46.226329 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-08 00:51:46.234504 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-08 00:51:46.244311 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-08 00:51:46.251983 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-08 00:51:46.297556 (MainThread): Parsing macros/relations.sql
2022-08-08 00:51:46.306732 (MainThread): Parsing macros/adapters.sql
2022-08-08 00:51:46.399589 (MainThread): Parsing macros/catalog.sql
2022-08-08 00:51:46.410359 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-08 00:51:46.520003 (MainThread): Partial parsing not enabled
2022-08-08 00:51:46.734670 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:46.735114 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:51:47.614810 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-08 00:51:48.693088 (MainThread): scipy not found, skipping conversion test.
2022-08-08 00:51:48.703897 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-08 00:51:48.705382 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-08 00:51:48.714875 (MainThread): 
2022-08-08 00:51:48.716455 (MainThread): Acquiring new postgres connection "master".
2022-08-08 00:51:48.717054 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:51:48.743345 (ThreadPoolExecutor-0_0): Acquiring new postgres connection "list_staging".
2022-08-08 00:51:48.747627 (ThreadPoolExecutor-0_0): Opening a new connection, currently in state init
2022-08-08 00:51:49.168134 (ThreadPoolExecutor-0_0): Using postgres connection "list_staging".
2022-08-08 00:51:49.168794 (ThreadPoolExecutor-0_0): On list_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging"} */

    select distinct nspname from pg_namespace
  
2022-08-08 00:51:49.239354 (ThreadPoolExecutor-0_0): SQL status: SELECT 5 in 0.07 seconds
2022-08-08 00:51:49.412271 (ThreadPoolExecutor-1_0): Acquiring new postgres connection "list_staging_staging".
2022-08-08 00:51:49.412736 (ThreadPoolExecutor-1_0): Re-using an available connection from the pool (formerly list_staging).
2022-08-08 00:51:49.435628 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:51:49.436098 (ThreadPoolExecutor-1_0): On list_staging_staging: BEGIN
2022-08-08 00:51:49.439163 (ThreadPoolExecutor-1_0): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:51:49.442757 (ThreadPoolExecutor-1_0): Using postgres connection "list_staging_staging".
2022-08-08 00:51:49.447901 (ThreadPoolExecutor-1_0): On list_staging_staging: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "list_staging_staging"} */
select
      'staging' as database,
      tablename as name,
      schemaname as schema,
      'table' as type
    from pg_tables
    where schemaname ilike 'staging'
    union all
    select
      'staging' as database,
      viewname as name,
      schemaname as schema,
      'view' as type
    from pg_views
    where schemaname ilike 'staging'
  
2022-08-08 00:51:49.619099 (ThreadPoolExecutor-1_0): SQL status: SELECT 3 in 0.17 seconds
2022-08-08 00:51:49.633270 (ThreadPoolExecutor-1_0): On list_staging_staging: ROLLBACK
2022-08-08 00:51:49.673361 (MainThread): Using postgres connection "master".
2022-08-08 00:51:49.674072 (MainThread): On master: BEGIN
2022-08-08 00:51:49.701719 (MainThread): SQL status: BEGIN in 0.03 seconds
2022-08-08 00:51:49.702371 (MainThread): Using postgres connection "master".
2022-08-08 00:51:49.702818 (MainThread): On master: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "connection_name": "master"} */
with relation as (
        select
            pg_rewrite.ev_class as class,
            pg_rewrite.oid as id
        from pg_rewrite
    ),
    class as (
        select
            oid as id,
            relname as name,
            relnamespace as schema,
            relkind as kind
        from pg_class
    ),
    dependency as (
        select
            pg_depend.objid as id,
            pg_depend.refobjid as ref
        from pg_depend
    ),
    schema as (
        select
            pg_namespace.oid as id,
            pg_namespace.nspname as name
        from pg_namespace
        where nspname != 'information_schema' and nspname not like 'pg\_%'
    ),
    referenced as (
        select
            relation.id AS id,
            referenced_class.name ,
            referenced_class.schema ,
            referenced_class.kind
        from relation
        join class as referenced_class on relation.class=referenced_class.id
        where referenced_class.kind in ('r', 'v')
    ),
    relationships as (
        select
            referenced.name as referenced_name,
            referenced.schema as referenced_schema_id,
            dependent_class.name as dependent_name,
            dependent_class.schema as dependent_schema_id,
            referenced.kind as kind
        from referenced
        join dependency on referenced.id=dependency.id
        join class as dependent_class on dependency.ref=dependent_class.id
        where
            (referenced.name != dependent_class.name or
             referenced.schema != dependent_class.schema)
    )

    select
        referenced_schema.name as referenced_schema,
        relationships.referenced_name as referenced_name,
        dependent_schema.name as dependent_schema,
        relationships.dependent_name as dependent_name
    from relationships
    join schema as dependent_schema on relationships.dependent_schema_id=dependent_schema.id
    join schema as referenced_schema on relationships.referenced_schema_id=referenced_schema.id
    group by referenced_schema, referenced_name, dependent_schema, dependent_name
    order by referenced_schema, referenced_name, dependent_schema, dependent_name;
2022-08-08 00:51:49.847078 (MainThread): SQL status: SELECT 0 in 0.14 seconds
2022-08-08 00:51:49.850553 (MainThread): On master: ROLLBACK
2022-08-08 00:51:49.851671 (MainThread): Using postgres connection "master".
2022-08-08 00:51:49.852249 (MainThread): On master: BEGIN
2022-08-08 00:51:49.854798 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:51:49.855477 (MainThread): On master: COMMIT
2022-08-08 00:51:49.855952 (MainThread): Using postgres connection "master".
2022-08-08 00:51:49.857097 (MainThread): On master: COMMIT
2022-08-08 00:51:49.858894 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-08 00:51:49.866720 (MainThread): 00:51:49 | Concurrency: 1 threads (target='dev')
2022-08-08 00:51:49.867220 (MainThread): 00:51:49 | 
2022-08-08 00:51:49.915125 (Thread-1): Began running node model.dbt_.trans_join
2022-08-08 00:51:49.915990 (Thread-1): 00:51:49 | 1 of 1 START table model staging.trans_join.......................... [RUN]
2022-08-08 00:51:49.917151 (Thread-1): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:49.917771 (Thread-1): Re-using an available connection from the pool (formerly list_staging_staging).
2022-08-08 00:51:49.918838 (Thread-1): Compiling model.dbt_.trans_join
2022-08-08 00:51:50.011859 (Thread-1): Writing injected SQL for node "model.dbt_.trans_join"
2022-08-08 00:51:50.014721 (Thread-1): finished collecting timing info
2022-08-08 00:51:50.114735 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:50.115531 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_tmp" cascade
2022-08-08 00:51:50.117664 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:51:50.128434 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:50.129282 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-08 00:51:50.130847 (Thread-1): SQL status: DROP TABLE in 0.00 seconds
2022-08-08 00:51:50.212954 (Thread-1): Writing runtime SQL for node "model.dbt_.trans_join"
2022-08-08 00:51:50.214491 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:50.215187 (Thread-1): On model.dbt_.trans_join: BEGIN
2022-08-08 00:51:50.219329 (Thread-1): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:51:50.231922 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:51:50.232386 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */


  create  table "staging"."staging"."trans_join__dbt_tmp"
  as (
    

with source_camp as (
    select * from staging.staging.source_campaign
),
source_brief as (
    select * from staging.staging.source_briefing
),
joins as (
    SELECT 
    c.id,
    c.campaign_id,
    c.types, 
    c.width, 
    c.height,  
    c.creative_id, 
    c.auction_id,
    c.browser_ts, 
    c.game_key,
    c.geo_country, 
    c.site_name, 
    c.platform_os,
    c.device_type, 
    c.browser,
    b.campaign_name, 
    b.Submission_Date, 
    b.Descriptions,
    b.Campaign_Objectives, 
    b.KPIs, 
    b.Placement, 
    b.StartDate, 
    b.EndDate,
    b.Serving_Location, 
    b.Black_white_audience,
    b.Delivery_Requirements, 
    b.Cost_Centre,
    b.Currency, 
    b.Buy_Rate_CPE, 
    b.Volume_Agreed, 
    b.Gross_Cost_or_Budget,
    b.Agency_Fee, 
    b.Percentages, 
    b.Flat_Fee, 
    b.Net_Cost
    FROM source_camp as c
    INNER JOIN source_brief as b ON c.campaign_id = b.campaign_id
),

final as (
    select * from joins
)

select * from final
  );
2022-08-08 00:52:35.191740 (Thread-1): SQL status: SELECT 422791 in 44.96 seconds
2022-08-08 00:52:35.241866 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:52:35.242499 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join" rename to "trans_join__dbt_backup"
2022-08-08 00:52:35.265755 (Thread-1): SQL status: ALTER TABLE in 0.02 seconds
2022-08-08 00:52:35.274671 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:52:35.275329 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
alter table "staging"."staging"."trans_join__dbt_tmp" rename to "trans_join"
2022-08-08 00:52:35.276785 (Thread-1): SQL status: ALTER TABLE in 0.00 seconds
2022-08-08 00:52:35.280028 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-08 00:52:35.282293 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:52:35.282767 (Thread-1): On model.dbt_.trans_join: COMMIT
2022-08-08 00:52:35.398709 (Thread-1): SQL status: COMMIT in 0.12 seconds
2022-08-08 00:52:35.406672 (Thread-1): Using postgres connection "model.dbt_.trans_join".
2022-08-08 00:52:35.407337 (Thread-1): On model.dbt_.trans_join: /* {"app": "dbt", "dbt_version": "0.16.1", "profile_name": "transformer", "target_name": "dev", "node_id": "model.dbt_.trans_join"} */
drop table if exists "staging"."staging"."trans_join__dbt_backup" cascade
2022-08-08 00:52:35.673173 (Thread-1): SQL status: DROP TABLE in 0.27 seconds
2022-08-08 00:52:35.685021 (Thread-1): finished collecting timing info
2022-08-08 00:52:35.687646 (Thread-1): Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '00e78042-07ec-43a5-bbe6-e4fad3f16f11', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5630e77c50>]}
2022-08-08 00:52:35.688488 (Thread-1): 00:52:35 | 1 of 1 OK created table model staging.trans_join..................... [SELECT 422791 in 45.77s]
2022-08-08 00:52:35.688945 (Thread-1): Finished running node model.dbt_.trans_join
2022-08-08 00:52:35.787785 (MainThread): Using postgres connection "master".
2022-08-08 00:52:35.788548 (MainThread): On master: BEGIN
2022-08-08 00:52:35.790010 (MainThread): SQL status: BEGIN in 0.00 seconds
2022-08-08 00:52:35.790797 (MainThread): On master: COMMIT
2022-08-08 00:52:35.791332 (MainThread): Using postgres connection "master".
2022-08-08 00:52:35.791840 (MainThread): On master: COMMIT
2022-08-08 00:52:35.793260 (MainThread): SQL status: COMMIT in 0.00 seconds
2022-08-08 00:52:35.794891 (MainThread): 00:52:35 | 
2022-08-08 00:52:35.796288 (MainThread): 00:52:35 | Finished running 1 table model in 47.08s.
2022-08-08 00:52:35.799090 (MainThread): Connection 'master' was left open.
2022-08-08 00:52:35.799765 (MainThread): On master: Close
2022-08-08 00:52:35.800557 (MainThread): Connection 'model.dbt_.trans_join' was left open.
2022-08-08 00:52:35.801143 (MainThread): On model.dbt_.trans_join: Close
2022-08-08 00:52:35.816960 (MainThread): 
2022-08-08 00:52:35.818284 (MainThread): Completed successfully
2022-08-08 00:52:35.819326 (MainThread): 
Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1
2022-08-08 00:52:35.820673 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f562f0a1310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f563d2aead0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f562f06e110>]}
2022-08-08 00:52:35.821696 (MainThread): Flushing usage events
2022-08-08 00:53:02.462602 (MainThread): Running with dbt=0.16.1
2022-08-08 00:53:02.815011 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.test.TestTask'>, data=False, debug=False, exclude=None, log_cache_events=False, log_format='default', models=None, partial_parse=None, profile=None, profiles_dir='/usr/local/airflow/dbt_/.dbt', project_dir=None, record_timing_info=None, rpc_method='test', schema=False, single_threaded=False, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, vars='{}', version_check=True, warn_error=False, which='test', write_json=True)
2022-08-08 00:53:02.826298 (MainThread): Tracking: tracking
2022-08-08 00:53:02.867240 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce4780f5d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce4787a510>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce4780bcd0>]}
2022-08-08 00:53:03.236886 (MainThread): Partial parsing not enabled
2022-08-08 00:53:03.259036 (MainThread): Parsing macros/core.sql
2022-08-08 00:53:03.367537 (MainThread): Parsing macros/materializations/helpers.sql
2022-08-08 00:53:03.506780 (MainThread): Parsing macros/materializations/snapshot/snapshot_merge.sql
2022-08-08 00:53:03.541022 (MainThread): Parsing macros/materializations/snapshot/snapshot.sql
2022-08-08 00:53:04.182778 (MainThread): Parsing macros/materializations/snapshot/strategies.sql
2022-08-08 00:53:04.411888 (MainThread): Parsing macros/materializations/incremental/incremental.sql
2022-08-08 00:53:04.509846 (MainThread): Parsing macros/materializations/incremental/helpers.sql
2022-08-08 00:53:04.532797 (MainThread): Parsing macros/materializations/seed/seed.sql
2022-08-08 00:53:04.702069 (MainThread): Parsing macros/materializations/common/merge.sql
2022-08-08 00:53:04.788394 (MainThread): Parsing macros/materializations/view/create_or_replace_view.sql
2022-08-08 00:53:04.848808 (MainThread): Parsing macros/materializations/view/view.sql
2022-08-08 00:53:04.917992 (MainThread): Parsing macros/materializations/table/table.sql
2022-08-08 00:53:04.992262 (MainThread): Parsing macros/adapters/common.sql
2022-08-08 00:53:05.216545 (MainThread): Parsing macros/etc/get_custom_database.sql
2022-08-08 00:53:05.221363 (MainThread): Parsing macros/etc/get_custom_schema.sql
2022-08-08 00:53:05.230861 (MainThread): Parsing macros/etc/get_relation_comment.sql
2022-08-08 00:53:05.241909 (MainThread): Parsing macros/etc/get_custom_alias.sql
2022-08-08 00:53:05.246818 (MainThread): Parsing macros/etc/is_incremental.sql
2022-08-08 00:53:05.255045 (MainThread): Parsing macros/etc/query.sql
2022-08-08 00:53:05.260397 (MainThread): Parsing macros/etc/datetime.sql
2022-08-08 00:53:05.309173 (MainThread): Parsing macros/schema_tests/not_null.sql
2022-08-08 00:53:05.317622 (MainThread): Parsing macros/schema_tests/accepted_values.sql
2022-08-08 00:53:05.328308 (MainThread): Parsing macros/schema_tests/unique.sql
2022-08-08 00:53:05.348585 (MainThread): Parsing macros/schema_tests/relationships.sql
2022-08-08 00:53:05.365673 (MainThread): Parsing macros/relations.sql
2022-08-08 00:53:05.380912 (MainThread): Parsing macros/adapters.sql
2022-08-08 00:53:05.532159 (MainThread): Parsing macros/catalog.sql
2022-08-08 00:53:05.556653 (MainThread): Parsing macros/materializations/snapshot_merge.sql
2022-08-08 00:53:05.738675 (MainThread): Partial parsing not enabled
2022-08-08 00:53:06.149255 (MainThread): Acquiring new postgres connection "model.dbt_.trans_join".
2022-08-08 00:53:06.150071 (MainThread): Opening a new connection, currently in state init
2022-08-08 00:53:07.741592 (MainThread): WARNING: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dbt_.example

2022-08-08 00:53:09.144025 (MainThread): scipy not found, skipping conversion test.
2022-08-08 00:53:09.159038 (MainThread): write_gpickle is deprecated and will be removed in 3.0.Use ``pickle.dump(G, path, protocol)``
2022-08-08 00:53:09.162356 (MainThread): Found 1 model, 0 tests, 0 snapshots, 0 analyses, 127 macros, 0 operations, 0 seed files, 2 sources
2022-08-08 00:53:09.174702 (MainThread): WARNING: Nothing to do. Try checking your model configs and model specification args
2022-08-08 00:53:09.175462 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce37104a90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce3710d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fce3710d590>]}
2022-08-08 00:53:09.176120 (MainThread): Flushing usage events
2022-08-08 00:53:10.106464 (MainThread): Connection 'model.dbt_.trans_join' was properly closed.
